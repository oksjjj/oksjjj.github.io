---
layout: post
title: "[1. Introduction] An Introduction to Variational Autoencoders"
date: 2025-10-22 20:00:00 +0900
categories:
  - "Books"
  - "An Introduction to Variational Autoencoders"
tags: []
---

# 1. Introduction

## 1.1 동기(Motivation)

---

머신러닝은 주로 생성(generative)모델링과 판별(discriminative) 모델링로 구분된다.  

판별 모델링에서는 관측된 데이터가 주어졌을 때 예측기(predictor)를 학습하는 것이 목표인 반면,  
생성 모델링에서는 모든 변수들에 대한 결합 분포를 학습하는 것과 같이 보다 일반적인 문제를 다루는 것이 목표이다.  

생성 모델은 현실 세계에서 데이터가 어떻게 생성되는지를 모사(simulate)한다.  

‘모델링(modeling)’이라는 개념은 거의 모든 과학 분야에서  
"생성 과정을 밝히기 위해 이론을 가정하고 관측을 통해 이를 검증하는 과정"으로 이해된다.  

예를 들어, 기상학자가 날씨를 모델링할 때는 날씨의 근본적인 물리 법칙을 표현하기 위해 매우 복잡한 편미분방정식을 사용한다.  
또한 천문학자가 은하의 형성을 모델링할 때는 별들 간의 상호작용을 지배하는 물리 법칙을 운동 방정식에 담는다.  

이와 같은 원리는 생물학자, 화학자, 경제학자 등 다른 과학자들에게도 동일하게 적용된다.  

즉, 과학에서의 모델링은 사실상 거의 언제나 생성 모델링인 것이다.

---

생성 모델링이 매력적인 이유는 많다.  

첫째, 우리는 물리 법칙과 제약 조건을 생성 과정에 포함시킬 수 있으며,  
우리가 알지 못하거나 관심 없는 세부 사항들, 즉 성가신 변수들은 노이즈로 취급된다.  

이러한 방식으로 얻어진 모델들은 일반적으로 매우 직관적이고 해석 가능하며,  
이를 관측값과 대조함으로써 우리는 세상이 어떻게 작동하는지에 대한 우리의 이론을 확인하거나 기각할 수 있다.

---

데이터의 생성 과정을 이해하려고 하는 또 다른 이유는  
그 과정이 자연스럽게 세상의 인과(causal) 관계를 표현하기 때문이다.  

인과 관계는 단순한 상관관계(correlations)보다 새로운 상황에 훨씬 더 잘 일반화된다는 큰 이점을 가진다.  

예를 들어, 우리가 지진의 생성 과정을 이해하게 되면, 그 지식을 캘리포니아나 칠레에서 모두 활용할 수 있다.

---

생성 모델을 판별기로 전환하려면 베이즈 규칙(Bayes rule)을 사용해야 한다.  

예를 들어, 우리가 A 유형의 지진과 B 유형의 지진에 대한 각각의 생성 모델을 가지고 있다면,  
이 두 모델 중 어느 쪽이 주어진 데이터를 더 잘 설명하는지를 비교함으로써,  
지진 A가 발생했을 확률과 지진 B가 발생했을 확률을 계산할 수 있다.  

그러나 베이즈 규칙을 적용하는 과정은 계산적으로 매우 비용이 많이 드는 경우가 많다.

---

판별적 방법에서는 미래 예측을 수행하려는 방향과 동일한 방향으로의 사상(map)을 직접 학습한다.  
이는 생성 모델의 방향과는 반대이다.  

예를 들어, 세상에서 이미지가 생성되는 과정을 생각해보면,  
먼저 객체를 인식(identify)하고, 그 다음 객체를 3차원(3D)으로 생성(generate)한 뒤,  
이를 픽셀 격자(pixel grid)에 투영(project)한다고 볼 수 있다.  

반면, 판별 모델은 이러한 픽셀 값들을 직접 입력으로 받아, 레이블(labels)로 매핑한다.  

생성 모델은 데이터로부터 효율적으로 학습할 수 있지만,  
순수한 판별 모델에 비해 데이터에 대해 더 강한 가정(stronger assumptions)을 세우는 경향이 있다.  

그 결과, 모델이 잘못되었을 경우, 비대칭적 편향(asymptotic bias)이 더 커지는 경우가 종종 발생한다 (Banerjee, 2007).  

따라서 모델이 잘못된 경우 — 그리고 실제로 대부분 어느 정도는 잘못되어 있다! —  
단순히 판별만을 학습하는 것이 목적이며, 충분히 많은 데이터가 주어진 환경에 있다면,  
순수한 판별 모델이 판별 과제에서 더 적은 오류를 낳는 경향을 보인다.  

그럼에도 불구하고, 데이터의 양에 따라,  
생성 과정을 연구하는 것이 판별기 — 예를 들어 분류기(classifier) — 의 학습에 도움이 될 수 있다.  

예를 들어, 레이블이 있는 데이터는 적고, 레이블이 없는 데이터는 훨씬 많은 경우,  
즉 준지도 학습(semi-supervised learning) 환경에서는,  
데이터의 생성 모델을 이용하여 분류성능을 향상시킬 수 있다 (Kingma et al., 2014; Sønderby et al., 2016a).

---

생성 모델링은 보다 일반적으로 유용할 수 있다.  

이를 보조(auxiliary) 작업으로 생각할 수도 있다.  

예를 들어, 가까운 미래를 예측하는 것은 세상을 이해하기 위한 유용한 추상적 표현을 형성하게 하며,  
이러한 표현은 이후 여러 예측 과제들에서 활용될 수 있다. 

데이터의 변화 요인(factors of variation) 가운데  
분리되어 있으며(disentangled), 의미론적으로 타당하고(semantically meaningful),  
통계적으로 독립적이며(statistically independent), 인과적인(causal) 요소들을 찾는 이 탐구는  
일반적으로 비지도 표현 학습(unsupervised representation learning)으로 알려져 있으며,  
이를 위해 변분 오토인코더(variational autoencoder, VAE)가 널리 활용되어 왔다.  

또 다른 관점에서는 이를 일종의 암묵적 정규화(implicit regularization)로 볼 수 있다.  
즉, 표현이 데이터 생성 과정에서 의미 있게 작동하도록 강제함으로써,  
입력에서 표현으로 가는 역방향 과정이 일정한 구조 안으로 유도되도록 만드는 것이다.  

>오토인코더 구조에서 생성 모델은 표현으로부터 입력을 생성하므로,  
>입력에서 표현으로 가는 인코딩 과정은 생성 과정의 역방향으로 볼 수 있다.

이러한 보조 작업, 즉 세상을 예측하는 작업은 세계를 추상적인 수준에서 더 잘 이해하기 위해 사용되며,  
결국 후속 예측(downstream prediction)을 더 잘 수행할 수 있도록 돕는다.

---

변분 오토인코더(VAE)는 서로 연결되어 있지만, 각각 독립적으로 매개변수화된 두 개의 모델로 볼 수 있다.  

즉, 인코더(encoder) 또는 인식 모델(recognition model)과  
디코더(decoder) 또는 생성 모델(generative model)이다.  

이 두 모델은 서로를 보완하며 협력한다.  

인식 모델은 잠재 확률 변수(latent random variables)에 대한  
사후분포(posterior)의 근사값을 생성 모델에 제공한다.  

> 인식 모델은 입력 변수 x를 받아 잠재 확률 변수 z를 생성한다.  
> 즉, 데이터 x를 관측한 이후의 z에 대한 확률 분포를 추정하므로,  
> 이는 z에 대한 사후분포(posterior)로 볼 수 있다.

생성 모델은 이를 이용하여 기대-최대화(expectation–maximization, EM) 학습의  
한 반복(iteration) 내에서 자신의 매개변수를 갱신한다.  

> 인식 모델이 제공한 사후분포 근사값 $q_\phi(z \mid x)$ 은  
> EM 알고리즘의 E단계(expectation step)에 해당하며,  
> 생성 모델은 이를 바탕으로 M단계(maximization step)에서  
> 데이터의 우도(likelihood)를 최대화한다.

반대로, 생성 모델은, 인식 모델이 데이터의 의미 있는 표현(예를 들어 클래스 레이블)을 학습할 수 있도록  
일종의 학습 틀(scaffolding) 역할을 한다.  

즉, 인식 모델은 베이즈 규칙에 따라 생성 모델의 근사적 역함수로 작동한다.

---

일반적인 변분 추론(Variational Inference, VI)과 비교했을 때,  
변분 오토인코더(VAE) 프레임워크의 한 가지 장점은  
인식 모델(또는 추론 모델)이 입력 변수의 (확률적) 함수로 정의된다는 점이다.  

> 기존 VI에서는 데이터 샘플마다 각각 다른 분포를 따로 학습해야 하지만,  
> VAE에서는 하나의 인식 모델이 모든 데이터에 대해 공통으로 작동하므로  
> 훨씬 효율적으로 추론을 수행할 수 있다.

이는 VI에서 각 데이터 샘플마다 별도의 변분 분포를 두는 방식과 대조된다.  
그러한 접근은 데이터셋이 커질수록 비효율적이다.  

VAE의 인식 모델은 하나의 파라미터 집합으로 입력 변수와 잠재 변수 간의 관계를 학습하며,  
이러한 방식을 암묵적 추론(amortized inference)이라고 부른다.  

이 인식 모델은 임의의 복잡한 형태를 가질 수 있지만,  
구조상 입력에서 잠재 변수로의 단일 feedforward 연산만으로 수행되므로 비교적 빠르게 계산할 수 있다.  

그러나 그 대가로, 이러한 샘플링 과정은  
학습에 필요한 그래디언트에 샘플링 노이즈(sampling noise)를 유발한다.  

VAE 프레임워크의 가장 큰 공헌 중 하나는 이러한 분산(variance)을 줄이기 위해  
재매개변수화 기법(reparameterization trick)으로 알려진 간단한 그래디언트 계산 재구성 절차를 도입했다는 점이다.  
이를 통해 그래디언트의 분산을 효과적으로 줄일 수 있다.

---

변분 오토인코더(VAE)는 헬름홀츠 머신(Helmholtz Machine, Dayan et al., 1995)에서 영감을 받았다.  
헬름홀츠 머신은 아마도 최초로 인식 모델(recognition model)을 도입한 모델이었다.  

그러나 그 학습 방식인 wake-sleep 알고리즘은 비효율적이었으며, 단일 목적 함수를 최적화하는 방식이 아니었다.  

> 헬름홀츠 머신은 생성 모델과 인식 모델을 번갈아 학습했지만,  
> VAE는 하나의 명확한 목적 함수(ELBO)를 중심으로  
> 두 모델을 함께 최적화한다는 점에서 다르다.

반면, VAE의 학습 규칙은  
최대우도(maximum likelihood) 목표를 단일 근사 형태로 유도하여  
보다 일관되고 효율적인 학습이 가능하도록 한다.  

---

변분 오토인코더(VAE)는 그래픽(graphical) 모델과 딥러닝을 결합한 구조이다.  

생성 모델은 $p(x \mid z)p(z)$과 같은 형태의 베이즈 네트워크(Bayesian network)로 표현된다:  
  
또는 다층 잠재 변수가 존재하는 경우,  
$p(x \mid z_L)p(z_L \mid z_{L-1}) \dots p(z_1 \mid z_0)$ 와 같은 계층적(hierarchical) 구조를 가진다.  

마찬가지로 인식 모델(추론 모델)도 $q(z \mid x)$ 형태의 조건부 베이즈 네트워크이거나,  
$q(z_0 \mid z_1) \dots q(z_L \mid x)$ 와 같은 계층 구조로 표현될 수 있다.  

이때 각 조건부 분포 안에는 $z \mid x \sim f(x, \epsilon)$ 과 같이 복잡한 딥러닝 신경망이 내포되어 있을 수 있다.   
여기에서 $f$ 는 신경망 기반의 매핑 함수이며, $\epsilon$ 은 노이즈이다.  

VAE의 학습 알고리즘은  
고전적인 변분 기대-최대화(variational expectation–maximization)를 기반으로 하지만,  
재매개변수화 기법(reparameterization trick)을 통해  
그 안에 포함된 여러 신경망 계층을 역전파(backpropagation)로 학습한다.  

