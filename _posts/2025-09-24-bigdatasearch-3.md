---
layout: post
title: "[3주차] 빅데이터와 정보검색"
date: 2025-09-24 08:00:00 +0900
categories: ["빅데이터와 정보검색"]
tags: []
---

> 출처: 빅데이터와 정보검색 – 황영숙 교수님, 고려대학교 (2025)

## 랭킹 기반 검색 (Ranked Retrieval)
  
  
- **Boolean Query의 장점과 한계**  
  - 불리언 질의: 문서가 질의와 일치하거나, 그렇지 않거나 둘 중 하나  

- **장점**  
  - 자신의 요구사항과 문서 집합을 정확하게 이해하는 **전문가**에게 유용  
  - 애플리케이션이 수천 개의 결과를 쉽게 처리할 수 있어 **응용 프로그램**에 적합  

- **한계**  
  - 대부분의 일반 사용자에게는 적합하지 않음  
  - 대다수 사용자는 불리언 질의를 작성하는 데 어려움을 느끼거나, 작성하는 것을 번거로워함  
  - 대부분의 사용자는 수천 개의 결과 목록을 일일이 확인하는 것을 원하지 않음  
  - 결과 수의 불균형: 결과의 수가 너무 적거나(0개) 혹은 너무 많은(수천 개) 극단적 상황 초래  
  → 적절한 수의 결과를 얻기 위해서는 상당한 수준의 질의 작성 기술이 필요  
   (AND: 종종 너무 적은 결과, OR: 종종 너무 많은 결과)  

---

### 보충 설명

#### 1. **Boolean Query의 특성**  
- 불리언 검색은 `AND`, `OR`, `NOT` 연산자를 이용해 문서를 필터링하는 방식이다.  
- 따라서 결과는 “포함” 또는 “배제”라는 이분법적 결과만 나오며, 문서 간의 **관련성 정도**를 반영하지 못한다.  

#### 2. **전문가에게 유용한 이유**  
- 도메인 지식이 있는 전문가라면 자신이 원하는 조건을 정확히 정의할 수 있다.  
- 예를 들어 법률, 특허, 의료 분야에서는 **정밀 검색**이 필요하므로 Boolean Query가 강점을 가진다.  

#### 3. **일반 사용자에게 불편한 이유**  
- 대부분의 사용자는 복잡한 연산자를 활용해 질의를 구성하기 어렵다.  
- 또한, 원하는 결과를 얻으려면 수차례 질의를 수정해야 하는데, 이는 비효율적이다.  

#### 4. **결과 수의 불균형 문제**  
- `AND` 연산을 많이 사용하면 검색 결과가 거의 없거나 전혀 나오지 않을 수 있다.  
- 반대로 `OR` 연산을 많이 사용하면 결과가 수천~수만 건 이상으로 불어나, 사용자가 소화하기 힘들다.  

#### 5. **Ranked Retrieval의 필요성**  
- 위 문제를 해결하기 위해 단순한 포함 여부가 아니라 **문서의 관련성 점수**를 계산하고,  
  그 점수를 기준으로 결과를 **랭킹(순위화)** 하는 방식이 필요하다.  
- 이를 통해 사용자는 상위 몇 개 문서만 확인해도 원하는 정보를 얻을 수 있다.  

---

## 랭킹 기반 검색 (Ranked Retrieval)

**랭킹 기반 검색 시스템**  

- 질의에 대한 **상위(top) 문서들에 순위**를 매겨 반환  

- 자유 텍스트 질의 (Free Text Queries)  
  - 연산자나 표현식 대신, 사용자가 **자연어**로 하나 또는 그 이상의 단어를 입력하는 방식  

- 랭킹 기반 검색의 장점: 대량 결과 처리  
  - 사실상 결과 집합의 크기는 중요하지 않아, **대량의 검색 결과**가 나와도 문제 없음  
  - 상위 k개(약 10개)의 결과만 사용자에게 제시  
  ← **랭킹 알고리즘이 효과적으로 작동한다는 전제 하에 유효**

---

## 랭킹 기반 검색의 기본

- **문서 순위화의 필요성?**  
  - 검색 사용자에게 가장 유용할 가능성이 높은 문서들을 **순서대로** 제시  

- **어떻게 문서를 순위화 하나?**  
  - 각 문서에 대해 **0과 1 사이의 점수**를 부여  
  - 점수: 문서와 질의가 얼마나 잘 일치하는지를 측정한 결과  

---

> - 문서에 부여되는 **0과 1 사이의 점수**를 **관련성(Relevance)** 이라고 부른다.  
> - 이 값은 질의(Query)와 문서(Document)가 얼마나 잘 일치하는지를 수치적으로 표현한 것이다.  

---

## 유사도 측정 방법

### Jaccard coefficient

  **개요**
  - 두 집합 A, B의 중복의 정도를 측정
  - jaccard(A, B) = |A ∩ B| / |A ∪ B|
  - jaccard(A, A) = 1
  - jaccard(A, B) = 0 if A ∩ B = 0

  **한계점**
  - 단어 빈도(term frequency)를 고려하지 않음
  - 문서 집합 내에서 **드문 단어**가 자주 등장하는 단어보다 더 많은 정보를 담고 있지만, 이러한 정보를 고려하지 않음
  - 문서 길이에 대한 정규화 방식이 필요

---

>**보충설명**
>
>1. **가장 단순한 랭킹**  
>   - Jaccard coefficient는 단순히 집합 간 교집합과 합집합의 비율을 이용하여 유사도를 측정한다.  
>   - 따라서 구현이 간단하고 이해하기 쉽다는 장점이 있다.  
>
>2. **문서의 크기가 반영되는 장점**  
>   - 교집합과 합집합 크기를 비교하기 때문에, 문서가 클수록 포함하는 단어가 많아 합집합의 크기에 반영된다.  
>   - 즉, 문서의 크기가 어느 정도 유사도에 반영되는 효과가 있다.  
>
>3. **단어 빈도가 표현되지 못하는 단점**  
>   - Jaccard는 단어의 **출현 여부만** 고려한다.  
>   - 예를 들어, 단어가 문서에 1번 등장하든 100번 등장하든 똑같이 "존재(1)"로 처리되므로, 단어 빈도(frequency)를 반영하지 못한다.  
>
>4. **쿼리와 문서 간 유사도 계산 방식**  
>   - 쿼리와 문서를 각각 집합으로 보고, 두 집합 간 교집합의 크기를 합집합의 크기로 나눈다.  
>   - 즉, `유사도 = |쿼리 ∩ 문서| / |쿼리 ∪ 문서|`로 정의된다.  
>
>5. **벡터 표현 방식**  
>   - 쿼리와 문서는 **vocabulary 크기 N만큼의 벡터**로 표현된다.  
>   - 각 term이 출현하면 1, 출현하지 않으면 0으로 표시한다 → 예: `01001...`  
>   - vocabulary가 크기 N이면, 각 term은 0 ~ N-1 범위의 인덱스를 가진다.  
>   - 따라서 교집합과 합집합을 벡터 연산으로 계산할 수 있고, 이후 "1"의 개수를 세어 유사도를 구한다.  
>
>6. **명시된 한계점에 대한 보충설명**  
>   - **단어 빈도(term frequency)를 고려하지 않음**: 단순히 등장 여부만 체크하므로, 중요한 단어의 반복적인 등장 정보를 무시하게 된다.  
>   - **드문 단어와 자주 등장하는 단어 구분 불가**: 예를 들어, "the"와 같은 흔한 단어와 "인공지능" 같은 드문 단어를 동일하게 취급한다.  
>   - **문서 길이 정규화 필요**: 문서 길이가 길수록 단어 수가 많아 합집합이 커지고, 유사도가 작게 계산될 수 있다. 따라서 길이에 따른 보정이 필요하다.  


---

## Bag of Words Model

- Dictionary와 Term의 문서별 출현빈도 테이블  

Count vector(dictionary size N)

| term       | Antony and Cleopatra | Julius Caesar | The Tempest | Hamlet | Othello | Macbeth |
|------------|-----------------------|---------------|-------------|--------|---------|---------|
| **Antony**     | 157                   | 73            | 0           | 0      | 0       | 0       |
| **Brutus**     | 4                     | 157           | 0           | 1      | 0       | 0       |
| **Caesar**     | 232                   | 227           | 0           | 2      | 1       | 1       |
| **Calpurnia**  | 0                     | 10            | 0           | 0      | 0       | 0       |
| **Cleopatra**  | 57                    | 0             | 0           | 0      | 0       | 0       |
| **mercy**      | 2                     | 0             | 3           | 5      | 5       | 1       |
| **worser**     | 2                     | 0             | 1           | 1      | 1       | 0       |

- Bag of Words model : 출현 단어들의 순서관계를 고려하지 않음

---

### 보충 설명

#### 1. **Bag of Words라는 말의 유래**  
- "Bag"은 단어들의 **순서와 구조를 무시하고, 단지 단어가 몇 번 등장했는지만 기록**한다는 의미에서 유래되었다.  
- 즉, 문서를 하나의 "가방(Bag)"에 담긴 단어들의 집합으로 보고, 이 가방 속에는 단어가 몇 개 들어있는지만 중요하지, 어떤 순서로 들어있는지는 고려하지 않는다.  
- 예를 들어, "AI is the future"와 "the future is AI"는 같은 Bag of Words 표현을 가진다.  

#### 2. **위 테이블에 대한 설명**  
- 테이블은 특정 용어(term)가 여러 문서에서 몇 번 등장했는지를 보여주는 **출현 빈도(frequency) 매트릭스**이다.  
- 행(Row)은 단어(term), 열(Column)은 문서를 나타낸다.  
- 각 셀 값은 해당 단어가 해당 문서에 등장한 횟수(TF, Term Frequency)이다.  
- 예: **"Caesar"** 는 **Antony and Cleopatra 문서** 에서 232번, **Julius Caesar 문서** 에서 227번 등장한다.  
- 이러한 빈도 행렬은 이후 **Count Vector**나 **TF-IDF 벡터**로 변환되어 문서 간 유사도 계산이나 랭킹에 활용된다.  

---

## Tf-idf

- **Tf(term frequency)**
  - 문서 d에 t가 나타나는 횟수: $tf_{t,d}$
  - Log frequency weight  

    $$
    w_{t,d} =
    \begin{cases}
    1 + \log tf_{t,d}, & \text{if } tf_{t,d} > 0 \\
    0, & \text{otherwise}
    \end{cases}
    $$

- **Idf**
  - T의 문서 빈도(t를 포함하는 문서의 수): $df_t$
  - T의 정보성 측정: Inverse document frequency  

    $$
    idf_t = \log \left(\frac{N}{df_t}\right)
    $$

- **score**  

  $$
  score = \sum_{t \in q \cap d} (1 + \log tf_{t,d})
  $$

| term      | $df_t$    | $idf_t$ |
|-----------|-----------|---------|
| calpurnia | 1         | 6       |
| animal    | 100       | 4       |
| sunday    | 1,000     | 3       |
| fly       | 10,000    | 2       |
| under     | 100,000   | 1       |
| the       | 1,000,000 | 0       |

---

### 보충 설명

#### 1. **log의 역할**  
- 단어의 빈도 $tf_{t,d}$가 너무 커질 경우, 해당 단어가 과도하게 중요하게 평가되는 것을 방지한다.  
- 로그 함수를 적용하면 빈도의 증가 효과가 점점 완만해져, 자주 등장하는 단어라 하더라도 영향력이 **포화(saturation)**되어 균형 잡힌 가중치를 부여할 수 있다.  

#### 2. **$1+$가 log 바깥쪽에 있는 이유**  
- **표준 TF–IDF**에서는 $w_{t,d} = \log(1+tf_{t,d})$를 사용한다.  
  - $tf_{t,d}=1$일 때 → $w_{t,d} = \log 2$ (밑 10 기준 약 0.301, 밑 e 기준 약 0.693)  
  - 즉, 한 번 등장한 단어라도 가중치는 **0과 1 사이의 작은 값**으로 부여된다.  
- **슬라이드 변형**에서는 $w_{t,d} = 1 + \log(tf_{t,d})$를 사용한다.  
  - $tf_{t,d}=1$일 때 → $w_{t,d} = 1 + \log 1 = 1$  
  - 즉, 한 번이라도 등장한 단어는 가중치가 **확실히 1 이상**으로 보장된다.  
- 이런 방식은 **Bag of Words(BOW) 기반 단순 가중치 모델**에서 가끔 쓰이는 변형으로,  
  문서 내 최소 등장 단어라도 “존재한다”는 의미를 더 분명하게 반영하려는 목적이 있다.  

---

## Tf-idf

- **Idf**
  - 질의 랭킹에의 영향: 단일어 질의에는 영향을 미치지 않으나, 2개 이상의 용어로 구성된 질의에는 영향을 미침  
  - 예) 질의 *“carpricious perspn”* 에서 *“carpricious”* 가 *“person”* 보다 최종 문서 랭킹에 더 큰 비중

- **정보검색의 가중치 부여방식**
  - $w_{t,d} = \log(1+tf_{t,d}) \times \log_{10}\left(\frac{N}{df_t}\right)$  
    - 문서 내에서 해당 용어가 출현하는 횟수가 많을수록 값이 커지고  
    - 전체 문서 집합에서 해당 용어가 희귀할수록 값이 커진다.
  - $Score(q,d) = \sum_{t \in q \cap d} tf \cdot idf_{t,d}$

---

>**보충설명**
>
>1. **왜 idf는 단일어 질의에 영향이 없나?**  
>   - 단일어 질의 $q=\{t\}$ 의 점수는  
>     $$
>     Score(d,q)=tf_{t,d}\cdot idf_t
>     $$  
>     이때 $idf_t$는 **모든 문서에 동일하게 곱해지는 상수**이므로, 문서 간 **순위는 $tf_{t,d}$** 만으로 결정된다.  
>   - 반면 다중어 질의 $q=\{t_1,t_2,\dots\}$ 에서는  
>     $$
>     Score(d,q)=\sum_i tf_{t_i,d}\,idf_{t_i}
>     $$  
>     각 용어의 $idf$가 서로 달라 **희귀한 단어(큰 $idf$)** 가 점수에 더 크게 기여하여 **순위가 달라진다**.
>
>2. **$1+$를 $\log$ 안에 넣는 이유**  
>   - $tf_{t,d}=0$일 때 $\log(tf_{t,d})$는 **정의되지 않음**.  
>   - 이를 피하기 위해  
>     $$
>     w_{t,d}=\log\!\big(1+tf_{t,d}\big)
>     $$  
>     로 정의하면 $tf_{t,d}=0 \Rightarrow w_{t,d}=\log 1=0$ 이 되어 자연스럽게 0 가중치가 되고,  
>     $tf$가 커질수록 증가폭이 완만해져 **포화(saturation)** 효과도 함께 얻을 수 있다.

---

## Tf-idf

- **Count**

Count vector (dictionary size V):

| term       | Antony and Cleopatra | **Julius Caesar** | The Tempest | Hamlet | Othello | Macbeth |
|------------|-----------------------|------------------|-------------|--------|---------|---------|
| Antony     | 157                   | **73**           | 0           | 0      | 0       | 0       |
| Brutus     | 4                     | **157**          | 0           | 1      | 0       | 0       |
| Caesar     | 232                   | **227**          | 0           | 2      | 1       | 1       |
| Calpurnia  | 0                     | **10**           | 0           | 0      | 0       | 0       |
| Cleopatra  | 57                    | **0**            | 0           | 0      | 0       | 0       |
| mercy      | 2                     | **0**            | 3           | 5      | 5       | 1       |
| worser     | 2                     | **0**            | 1           | 1      | 1       | 0       |

---

- **WeightMatrix**
  - tf-idf weights

Weight Vector (dictionary size V):

| term       | Antony and Cleopatra | **Julius Caesar** | The Tempest | Hamlet | Othello | Macbeth |
|------------|-----------------------|------------------|-------------|--------|---------|---------|
| Antony     | 5.25                  | **3.18**         | 0           | 0      | 0       | 0.35    |
| Brutus     | 1.21                  | **6.1**          | 0           | 1      | 0       | 0       |
| Caesar     | 8.59                  | **2.54**         | 0           | 1.51   | 0.25    | 0       |
| Calpurnia  | 0                     | **1.54**         | 0           | 0      | 0       | 0       |
| Cleopatra  | 2.85                  | **0**            | 0           | 0      | 0       | 0       |
| mercy      | 1.51                  | **0**            | 1.9         | 0.12   | 5.25    | 0.88    |
| worser     | 1.37                  | **0**            | 0.11        | 4.15   | 0.25    | 1.95    |

---

>**Macbeth 열의 오타**  
>   - `Antony` 행과 `worser` 행의 Macbeth 열 값이 각각 **0.35**, **1.95**로 표기되어 있지만, 위의 **Count vector**에서 이 두 위치의 `tf` 값이 0이므로 실제 `tf-idf` 가중치는 0이어야 한다.  
>   - 따라서 이는 슬라이드 상의 **오타**이다.

---

## Vector Space Model

1. **Document Vectors**  
   - $|V|$-dimensional vector space  
   - 고차원 벡터, but 희소벡터  

2. **Query Vectors**  
   - 질의도 문서와 동일하게 **벡터 공간**에 벡터로 표현  
   - 벡터 공간에서 질의와의 **근접성(proximity)**에 따라 문서의 순위를 매김  

   - **근접성(Proximity)?**  
     - 벡터들의 **유사성**과 동일  
     - 벡터들 간의 **거리**에 반비례  

3. **유클리드 거리와 벡터간 각도**  
   - 질의 q와 문서 d2 간 유클리드 거리는 상당히 크지만, 각도는 아주 작음  
   - **문서의 순위는 질의와의 각도에 따라 결정**됨  
     - 각도가 작을수록 유사도가 높음  

<img src="/assets/img/bigdatasearch/3/image_1.png" alt="image" width="400px">

---

>**보충설명**
>
>1. **벡터 공간의 의미**  
>   - 문서와 질의를 같은 차원의 벡터로 표현하면, 이들을 동일한 공간에서 비교할 수 있다.  
>   - 차원 수는 전체 **vocabulary(용어집)** 크기와 동일하다. 예를 들어 단어가 10,000개라면 각 문서와 질의는 10,000차원 벡터로 표현된다.  
>
>2. **왜 희소 벡터인가?**  
>   - 실제로 한 문서가 모든 단어를 포함하는 경우는 거의 없다.  
>   - 대부분의 차원 값은 0이고, 특정 단어들에 대해서만 값이 존재하므로 **희소(sparse)** 벡터가 된다.  
>
>3. **근접성(Proximity)과 유사성(Similarity)**  
>   - 벡터 간의 유사성은 보통 **각도(코사인 유사도)**로 측정한다.  
>   - 유클리드 거리만 보면 질의와 멀리 떨어진 것처럼 보일 수 있으나, **각도는 방향성을 반영하기 때문에** 실제 의미적 유사성을 더 잘 잡아낼 수 있다.  
>
>4. **예시 설명 (장표 그림)**  
>   - 질의 $q$와 문서 $d2$ 사이:  
>     - 유클리드 거리는 멀지만, 각도가 매우 작다 → 실제로는 높은 유사도를 가진다.  
>   - 질의 $q$와 문서 $d1$, $d3$:  
>     - 각도가 더 크기 때문에 $d2$보다 유사도가 낮게 평가된다.  
>
>5. **정리**  
>   - **문서의 순위는 질의와의 거리보다는 각도에 의해 결정**된다.  
>   - 따라서, **벡터 간 코사인 유사도(cosine similarity)**가 랭킹 기반 검색에서 표준적으로 사용된다.  

## 코사인 유사도

- **코사인(cosine)**
  - 질의와 문서 사이의 **각도**를 기준으로 내림차순으로 문서의 순위를 매김  
  - $\cos(\text{질의}, \text{문서})$ 값을 기준으로 오름차순으로 문서의 순위  
  - 코사인 함수는 $[0^\circ, 180^\circ]$ 구간에서 **단조 감소 함수**
  
<img src="/assets/img/bigdatasearch/3/image_2.png" alt="image" width="540px">

---

- **코사인 유사도**
  - 벡터 크기 정규화  
    $$
    \| \vec{x} \|_2 = \sqrt{\sum_i x_i^2}
    $$
  - 정의  
    $$
    \cos(\vec{q}, \vec{d}) = 
    \frac{\vec{q} \cdot \vec{d}}{\|\vec{q}\|\|\vec{d}\|}
    = \frac{\sum_{i=1}^{|V|} q_i d_i}{\sqrt{\sum_{i=1}^{|V|} q_i^2} \sqrt{\sum_{i=1}^{|V|} d_i^2}}
    $$

  - 벡터 크기 정규화 코사인 유사도  
    $$
    \cos(\vec{q}, \vec{d}) = \vec{q} \cdot \vec{d} = \sum_{i=1}^{|V|} q_i d_i
    $$

<img src="/assets/img/bigdatasearch/3/image_3.png" alt="image" width="400px">

- **$q_i$, $d_i$의 의미**
  - $q_i$: **질의(query)에서 용어 $i$의 가중치**  
  - $d_i$: **문서(document)에서 용어 $i$의 가중치**
  
---

## 코사인 유사도

**Term frequencies**  

| term       | SaS  | PaP | WH  |
|------------|------|-----|-----|
| affection  | 115  | 58  | 20  |
| jealous    | 10   | 7   | 11  |
| gossip     | 2    | 0   | 6   |
| wuthering  | 0    | 0   | 38  |

---

**Log frequency weighting**  

| term       | SaS  | PaP | WH  |
|------------|------|-----|-----|
| affection  | 3.06 | 2.76| 2.30|
| jealous    | 2.00 | 1.85| 2.04|
| gossip     | 1.30 | 0   | 1.78|
| wuthering  | 0    | 0   | 2.58|

- $dot(SaS, PaP) \approx 12.1$  
- $dot(SaS, WH) \approx 13.4$  
- $dot(PaP, WH) \approx 10.1$  

---

**After length normalization**  

| term       | SaS   | PaP   | WH   |
|------------|-------|-------|------|
| affection  | 0.789 | 0.832 | 0.524|
| jealous    | 0.515 | 0.555 | 0.465|
| gossip     | 0.335 | 0     | 0.405|
| wuthering  | 0     | 0     | 0.588|

- $cos(SaS, PaP) \approx 0.94$  
- $cos(SaS, WH) \approx 0.79$  
- $cos(PaP, WH) \approx 0.69$  

---

>**보충 설명**
>
>1. **Dot Product 계산**  
>   두 벡터 $\mathbf{a}, \mathbf{b}$의 내적(dot product)은 다음과 같이 계산된다.  
>   $$
>   \mathbf{a} \cdot \mathbf{b} = \sum_i a_i b_i
>   $$  
>   예를 들어 $dot(SaS, PaP)$의 경우:  
>   $$
>   3.06 \times 2.76 + 2.00 \times 1.85 + 1.30 \times 0 + 0 \times 0 \approx 12.1
>   $$
>
>2. **Cosine Similarity 계산**  
>   코사인 유사도는 두 벡터의 내적을 두 벡터 크기의 곱으로 나눈 값이다.  
>   $$
>   cos(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\|\|\mathbf{b}\|}
>   $$  
>   여기서 $\|\mathbf{a}\| = \sqrt{\sum_i a_i^2}$ 는 벡터의 크기이다.  
>
>   예를 들어 $cos(SaS, PaP)$:  
>   - $dot(SaS, PaP) \approx 12.1$  
>   - $\|SaS\| = \sqrt{3.06^2 + 2.00^2 + 1.30^2 + 0^2} \approx 3.86$  
>   - $\|PaP\| = \sqrt{2.76^2 + 1.85^2 + 0^2 + 0^2} \approx 3.22$  
>   - 따라서,  
>   $$
>   cos(SaS, PaP) \approx \frac{12.1}{3.86 \times 3.22} \approx 0.94
>   $$
   
---

## Tf-idf 변형들

**Term frequency (TF)**  
- **n (natural)**: $tf_{t,d}$  
- **l (logarithm)**: $1 + \log(tf_{t,d})$  
- **a (augmented)**: $0.5 + \dfrac{0.5 \times tf_{t,d}}{\max_t(tf_{t,d})}$  
- **b (boolean)**:  
  $$
  \begin{cases} 
  1 & \text{if } tf_{t,d} > 0 \\
  0 & \text{otherwise}
  \end{cases}
  $$  
- **L (log ave)**: $\dfrac{1 + \log(tf_{t,d})}{1 + \log(\text{ave}_{t \in d}(tf_{t,d}))}$  

---

**Document frequency (DF)**  
- **n (no)**: $1$  
- **t (idf)**: $\log \dfrac{N}{df_t}$  
- **p (prob idf)**: $\max\{0, \log \dfrac{N - df_t}{df_t}\}$  

---

**Normalization**  
- **n (none)**: $1$  
- **c (cosine)**: $\dfrac{1}{\sqrt{w_1^2 + w_2^2 + \dots + w_M^2}}$  
- **u (pivoted unique)**: $\dfrac{1}{u}$  
- **b (byte size)**: $\dfrac{1}{CharLength^\alpha}, \ \alpha < 1$

---

>**보충설명**
>
>1. **TF 변형**
>   - **n (natural)**: 원시 단순 빈도. 단어가 등장한 횟수를 그대로 사용한다.  
>   - **l (logarithm)**: 단어가 지나치게 많이 등장하는 경우 그 영향이 너무 커지는 문제를 완화한다. 로그 함수를 취해 포화 효과를 준다.  
>   - **a (augmented)**: 문서 길이 차이를 보정하기 위한 방법. 최대 빈도로 정규화하여 상대적 비율을 강조한다.  
>   - **b (boolean)**: 단어가 등장했는지 여부만 고려한다. 단순하지만 단어의 빈도 정보가 사라지는 단점이 있다.  
>   - **L (log ave)**: 각 문서의 평균 빈도로 나누어 문서마다 등장 빈도의 편차를 완화한다.  
>
>2. **DF 변형**
>   - **n (no)**: DF를 고려하지 않는다. 모든 단어를 동일한 중요도로 간주한다.  
>   - **t (idf)**: 가장 널리 쓰이는 방식. 드문 단어일수록 가중치를 크게 주어 정보성을 반영한다.  
>   - **p (prob idf)**: 확률적 IDF. 단어가 등장하지 않은 문서와 등장한 문서를 비교하여, 극단적으로 빈도가 높은 단어에 대한 가중치를 더 보수적으로 조정한다.  
>
>3. **Normalization**
>   - **n (none)**: 정규화를 하지 않는다. 단어가 많이 등장하는 긴 문서가 유리해진다.  
>   - **c (cosine)**: 벡터 크기를 1로 정규화하여 문서 길이 차이를 제거한다. 코사인 유사도 계산과 함께 쓰이는 표준 방식이다.  
>   - **u (pivoted unique)**: 문서 내 고유 단어 수에 따라 보정한다. 문서가 길수록 다양한 단어가 포함된다는 점을 고려한 기법이다.  
>   - **b (byte size)**: 문서의 바이트 크기에 따라 정규화한다. 길이가 큰 문서에서 생기는 편향을 줄이기 위한 방법이다.  
   
---

## Vector Space Model

**코사인 유사도를 활용한 문서 랭킹 과정**

- **벡터 생성**
  - 질의(Query)를 가중치가 부여된 **tf-idf 벡터**로 표현: $tf(q, t), idf(t)$  
  - 각 문서(Document)를 가중치가 부여된 **tf-idf 벡터**로 표현: $tf(d, t), idf(t)$  

- **유사도 계산 및 순위화**
  - 질의 벡터와 각 문서 벡터 간의 **코사인 유사도 점수**를 계산  
  - 점수를 기준으로 질의에 대한 문서들의 순위 결정  

- **결과 반환**
  - 가장 높은 순위를 가진 **상위 K개의 문서**(예: $K=10$)를 사용자에게 반환  

---

## Probabilistic IR

- **불리언 검색의 문제**  
  - “대박” 아니면 “쪽박”  
  - 쿼리에 넣은 키워드의 수와 AND, OR 연산 표현의 기술 요구  
    - AND 연산이 늘어나면 결과의 수는 줄어듦  
    - OR 연산이 늘어나면 결과의 수는 급격히 증가  

➡ 따라서 **좋은 문서를 랭킹하는 Soft한 방법**이 필요  


- **Why Probabilistic IR**

<img src="/assets/img/bigdatasearch/3/image_4.png" alt="image" width="600px">

> - **Hard 방식**: 문서가 질의 조건을 만족하면 1, 만족하지 않으면 0으로 처리한다.  
>   - 예: Boolean Retrieval (AND, OR 연산으로만 결과 결정)  
>   - 결과가 극단적으로 나오며, “관련 있음/없음”만 구분할 수 있다.  
>   
> - **Soft 방식**: 문서와 질의 간의 관련성을 **0과 1 사이의 연속적인 값**으로 평가한다.  
>   - 예: Probabilistic IR, Vector Space Model  
>   - 문서가 질의와 **얼마나 유사한지, 관련성이 얼마나 강한지**를 수치화하여 랭킹 가능하다.  

---

## Probabilistic IR

**고전적 확률 기반 정보 검색 모델**
- **Probability Ranking Principle (PRP)**: 확률 순위 결정 원칙  
- **이진 독립 모델 (≈ Naïve Bayes 텍스트 분류 모델)**  
  - *Independence*: terms occur in documents independently (단순화를 위해 용어 간 독립 가정)  
- (Okapi) **BM25**  

**확률론적 방법**
- 정보 검색 분야에서 오래되었지만 현재에도 많이 사용되는 방법  
- **문서와 정보 요구 간의 관련성 확률**에 따라 순위를 매기는 방식  

$$
p(R=1|x) = \frac{p(x|R=1)p(R=1)}{p(x)}
$$

$$
p(R=0|x) = \frac{p(x|R=0)p(R=0)}{p(x)}
$$

$$
p(R=0|x) + p(R=1|x) = 1
$$

- 여기서:
  - \(x\): document  
  - \(R\): query에 대한 문서의 적합성 (relevance)  
    - \(R=1\): 적합  
    - \(R=0\): 부적합  

---

> - 용어가 문서에 **독립적으로 등장한다(independently)**고 가정하는 이유는 계산을 단순화하기 위함이다.  
> - 독립 가정을 하면 각 단어의 확률을 곱셈으로 단순 결합할 수 있어 모델이 간단해진다.  
> - 하지만 실제로는 단어들이 서로 **상관관계**를 가진다. 예를 들어 “뉴”와 “욕”은 함께 등장할 확률이 높다.  
> - 만약 독립성을 가정하지 않으면, 단어들 간의 **공동 확률분포(joint probability distribution)**를 모델링해야 하고, 이는 차원이 급격히 커져 계산이 복잡해진다.  

---

## Probabilistic IR

- **The Probability Ranking Principle (PRP)**  

---

**영문 원문**  
> “If a reference retrieval system’s response to each request is a ranking of the documents in the collection in order of decreasing probability of relevance to the user who submitted the request, where the probabilities are estimated as accurately as possible on the basis of whatever data have been made available to the system for this purpose, the overall effectiveness of the system to its user will be the best that is obtainable on the basis of those data.”  

---

**번역본**  
> “만약 어떤 참고 검색 시스템이 각 요청에 대해 응답을 생성하는 방식이, 요청을 제출한 사용자에게 관련될 확률이 감소하는 순서대로 문서 집합 내 문서들을 나열하는 것이라면, 그리고 이러한 확률들이 해당 목적을 위해 시스템에 제공된 모든 데이터를 가능한 한 정확하게 기반으로 추정된다면, 그 시스템의 전반적인 효과성은 해당 데이터에 기초하여 얻을 수 있는 최선의 것이 될 것이다.”  

---

- [1960s/1970s] S. Robertson, W.S. Cooper, M.E. Maron;  
  van Rijsbergen (1979:113); Manning & Schütze (1999:538)  

- **검색 시스템이 각 문서의 관련성 확률을 정확히 추정하여 내림차순으로 순위화할 때 최적의 검색 성능을 달성할 수 있다는 이론적 기반을 제시**  

---

## Probabilistic IR

- 확률의 기초 – 베이즈의 정리  

$$
p(A,B) = p(A \cap B) = p(A|B)p(B) = p(B|A)p(A)
$$  

$$
p(A|B) = \frac{p(B|A)p(A)}{p(B)}
       = \frac{p(B|A)p(A)}{\sum_{X=A,\bar{A}} p(B|X)p(X)}
$$  

- Odds: 어떤 사건이 발생할 확률과 발생하지 않을 확률의 비율  

$$
O(A) = \frac{p(A)}{1-p(A)}
$$  

---

>**보충설명**
>
>1. **베이즈 정리 (Bayes' Rule)**  
>   - 조건부 확률을 연결하는 기본 공식이다.  
>   - 어떤 사건 $A$의 사후확률 $p(A|B)$를, 사전확률 $p(A)$와 가능도 $p(B|A)$로 표현한다.  
>   - 식:  
>     $$
>     p(A|B) = \frac{p(B|A)p(A)}{p(B)}
>     $$  
>
>   - **사전확률 (Prior, $p(A)$)**  
>     : 사건 $A$가 일어나리라는 믿음(확률)에 대해, 관찰된 정보 $B$를 보기 전에 우리가 가진 **사전 지식**을 의미한다.  
>     예: “어떤 문서가 관련 문서일 확률이 전체적으로 10% 정도 된다.”  
>
>   - **가능도 (Likelihood, $p(B|A)$)**  
>     : 사건 $A$가 참일 때, 관측된 증거 $B$가 나타날 **가능성**을 의미한다.  
>     예: “문서가 관련 문서라면, 특정 단어 $B$가 등장할 확률은 0.8이다.” > 
>   - **사후확률 (Posterior, $p(A|B)$)**  
>     : 관측된 증거 $B$를 반영한 후, 사건 $A$가 참일 **최종 확률**이다. 
>     즉, 사전확률을 가능도로 갱신(update)하여 얻는다.  
>     예: “이 단어가 등장했다면, 문서가 관련 문서일 확률은 40%로 갱신된다.”  
>
>   - **요약**:  
>     - 사전확률은 **사건에 대한 선입견**  
>     - 가능도는 **데이터가 사건과 얼마나 잘 맞는지**  
>     - 사후확률은 **데이터까지 반영한 새로운 믿음**  
>
>2. **Odds (승산)**  
>   - 사건이 발생할 확률과 발생하지 않을 확률의 비율이다.  
>   - 식:  
>     $$
>     O(A) = \frac{p(A)}{1-p(A)}
>     $$  
>   - 단순히 확률 $p(A)$를 보는 것이 아니라, 사건 발생 가능성이 “얼마나 더 큰지”를 상대적으로 측정한다.  
>   - **Logistic Regression**에서 핵심적으로 사용된다.

---

## Probabilistic IR

- 각 term이 관련성에 어떻게 기여하는지 추정  
- 문서의 관련성 확률을 찾기 위해 결합  
- 확률이 감소하는 순서로 문서를 정렬  

- 정리(Theorem): PRP를 사용하는 것은 최적이다.  
  이는 1/0 손실 하에서 베이즈 위험(Bayes risk)을 최소화한다는 의미이다.  

  - 1/0 손실 (1/0 loss): 이진 손실 함수로, 결과가 '올바른 경우(0)'와 '잘못된 경우(1)' 두 가지로만 구분된다.  
  검색에서는 관련 있는 문서를 놓치거나, 관련 없는 문서를 제시할 때 손실이 발생한다고 본다.  

  - 베이즈 위험 (Bayes risk): 어떤 결정 규칙(문서 순위 결정)을 사용했을 때 예상되는 평균 손실을 의미한다.  
  PRP를 따르면 이 평균 손실이 가장 낮아진다는 것을 뜻한다.  

  - 가장 관련성이 높은 문서들을 우선적으로 보여주는 것이 이론적으로 가장 좋은 결과를 보장한다는 정리로,  
  확률에 기반해 순위를 매기는 방식이 사용자의 정보 요구를 가장 효과적으로 충족시킨다는 뜻이다.  

---

>**보충설명**
>
>1. **1/0 손실 (1/0 loss)**  
>   - **의미**: 결과가 두 가지 경우만 있는 단순한 손실 함수.  
>     - 정답을 맞히면 손실이 0  
>     - 틀리면 손실이 1  
>   - **정보 검색에서의 해석**:  
>     - 관련 있는 문서를 보여주지 못하거나  
>     - 관련 없는 문서를 보여줄 때  
>     → 이를 “오류”로 보고 손실 1이 발생한다고 간주한다.  
>   - 따라서 1/0 손실은 **관련성 판단의 정확도만을 고려**하고, 다른 비용(부분적 오류 등)은 고려하지 않는다.  
>
>2. **베이즈 위험 (Bayes risk)**  
>   - **의미**: 어떤 결정 규칙(예: 문서 순위 결정 방법)을 사용했을 때 기대되는 **평균 손실**.  
>   - **공식 표현**:  
>     $$
>     R(\delta) = \mathbb{E}[L(\theta, \delta(X))]
>     $$  
>     - $\delta$: **결정 규칙(Decision rule)**  
>       → 관찰된 데이터 $X$를 보고 어떤 행동을 취할지(문서를 어떻게 순위화할지) 정하는 규칙.  
>       예: "관련성 확률이 높은 문서부터 순위화한다."  
>     - $L$: **손실 함수(Loss function)**  
>       → 실제 상태 $\theta$와 선택한 행동 $\delta(X)$가 얼마나 차이가 나는지를 수치화한 함수.  
>       예: 문서가 실제로 관련 있는데 보여주지 않으면 1, 관련 없는데 보여주면 1, 나머지는 0.  
>   - **정보 검색에서의 해석**:  
>     - PRP(Probability Ranking Principle)를 따르면  
>       관련 문서를 놓치거나 불필요한 문서를 보여줄 확률이 최소화된다.  
>     - 즉, 평균적으로 발생하는 손실이 최소가 되며, 이는 이론적으로 최적임을 의미한다.  
>
>3. **종합**  
>   - **결정 규칙**: 데이터를 보고 행동을 정하는 방식 (문서를 어떤 기준으로 정렬할지).  
>   - **손실 함수**: 그 결정이 실제와 얼마나 어긋났는지를 수치화하는 도구. 
>   - **베이즈 위험**: 손실 함수와 결정 규칙을 종합해, 기대되는 평균 손실을 계산한 값.  
>   - 따라서 PRP는 손실을 최소화하는 최적의 결정 규칙이 된다.  

---

## Binary Independence Model

- 쿼리: 이진 단어 출현 벡터  

- 주어진 쿼리 $q$  
  • 각 문서 $d$에 대해 $p(R|q,d)$ 계산  
  • $x$는 문서 $d$를 표현하는 이진 단어 출현 벡터일 때 $p(R|q,x)$로 대체  

- Odds와 **베이즈 룰**을 이용하여 정리  
  먼저, 베이즈 룰에 따라  
  $$
  p(R|q,\vec{x}) = \frac{p(R|q) \, p(\vec{x}|R,q)}{p(\vec{x}|q)}
  $$  

  따라서 odds는  
  $$
  O(R|q,\vec{x}) 
   = \frac{p(R=1|q,\vec{x})}{p(R=0|q,\vec{x})}
   = \frac{\frac{p(R=1|q)\,p(\vec{x}|R=1,q)}{p(\vec{x}|q)}}
          {\frac{p(R=0|q)\,p(\vec{x}|R=0,q)}{p(\vec{x}|q)}}
  $$  

  분모 $p(\vec{x}|q)$는 약분되어  
  $$
  O(R|q,\vec{x}) = \frac{p(R=1|q)}{p(R=0|q)} \cdot \frac{p(\vec{x}|R=1,q)}{p(\vec{x}|R=0,q)}
  $$  

  - $\frac{p(R=1|q)}{p(R=0|q)}$: **상수(constant)**  
  - $\frac{p(\vec{x}|R=1,q)}{p(\vec{x}|R=0,q)}$: **추정해야 할 값(estimable)**  

- Independence 가정 적용  
  $$
  \frac{p(\vec{x}|R=1,q)}{p(\vec{x}|R=0,q)}
   = \prod_{i=1}^n \frac{p(x_i|R=1,q)}{p(x_i|R=0,q)}
  $$  

  따라서  
  $$
  O(R|q,\vec{x}) = O(R|q) \cdot \prod_{i=1}^n \frac{p(x_i|R=1,q)}{p(x_i|R=0,q)}
  $$  

---

>**보충설명**
>
>1. **쿼리의 이진 단어 출현 벡터와 독립성 가정**  
>   - 쿼리나 문서를 벡터로 표현할 때, 각 단어가 등장했으면 1, 등장하지 않았으면 0로 표시한다.  
>     예: 단어 집합이 {apple, banana, cat}이고 문서에 "apple"과 "cat"이 등장하면 벡터는 $[1,0,1]$.  
>   - **독립성 가정(Independence)**: 각 단어의 출현 여부가 서로 독립이라고 본다.  
>     이 가정 덕분에 $p(\vec{x}\mid R,q)=\prod_i p(x_i\mid R,q)$처럼 곱으로 분해되어 계산이 단순해진다.  
>
>2. **베이즈 룰과 $q$가 조건으로 상시 적용된 경우**  
>   - 일반 베이즈:  
>     $$
>     p(R\mid \vec{x})=\frac{p(R)\,p(\vec{x}\mid R)}{p(\vec{x})}
>     $$  
>   - $q$가 항상 조건으로 주어지면, **모든 확률항에 $q$가 붙는다**:  
>     $$
>     p(R\mid q,\vec{x})=\frac{p(R\mid q)\,p(\vec{x}\mid R,q)}{p(\vec{x}\mid q)}
>     $$  
>
>3. **$R$이 1인지 0인지 어떻게 아는가?**  
>   - 정보검색(IR)에서는 **라벨된 학습 데이터**(평가 세트, 사용자 피드백 등)로 문서–쿼리 쌍에 $R\in\{0,1\}$을 부여한다.  
>   - **MNIST 비유**: 클래스 $Y\in\{0,\dots,9\}$이고, 예를 들어 $Y=3$으로 라벨된 **다양한 이미지들**로부터  
>     픽셀 벡터 $\vec{x}$의 분포를 **추정**한다.  
>     - Bernoulli Naive Bayes: 각 픽셀의 on/off에 대해 $p(x_j=1\mid Y=3)$을 추정.  
>     - Gaussian Naive Bayes: 각 픽셀 강도의 평균·분산으로 $p(x_j\mid Y=3)$을 추정.  
>   - 이와 **동일한 방식**으로 IR에서도 $R=1$(관련)과 $R=0$(비관련)으로 라벨된 데이터로부터  
>     $p(x_i\mid R=1,q)$, $p(x_i\mid R=0,q)$ 등을 **데이터로 추정**하여 새로운 문서의 관련성 확률을 예측한다.  

---

## Binary Independence Model

- 기본 식  
  $$
  O(R|q,\vec{x}) = O(R|q) \cdot \prod_{i=1}^n \frac{p(x_i \mid R=1,q)}{p(x_i \mid R=0,q)}
  $$  

- $x_i$가 0 또는 1일 때 분리하여 곱으로 표현 가능  
  $$
  O(R|q,\vec{x}) 
   = O(R|q) \cdot 
     \prod_{x_i=1} \frac{p(x_i=1 \mid R=1,q)}{p(x_i=1 \mid R=0,q)} \cdot
     \prod_{x_i=0} \frac{p(x_i=0 \mid R=1,q)}{p(x_i=0 \mid R=0,q)}
  $$  

- 정의  
  - $p_i = p(x_i=1 \mid R=1,q)$  
  - $r_i = p(x_i=1 \mid R=0,q)$  

- 모든 term이 쿼리에 나타나지 않는 경우($q_i=0$이라고 가정)  
  $$
  p_i = r_i
  $$  

- 최종 정리식  
  $$
  O(R|q,\vec{x})
   = O(R|q) \cdot 
     \prod_{x_i=1,\, q_i=1} \frac{p_i}{r_i} \cdot
     \prod_{x_i=0,\, q_i=1} \frac{1-p_i}{1-r_i}
  $$   

| document 상태 | relevant (R=1) | not relevant (R=0) |
|---------------|----------------|---------------------|
| term present ($x_i=1$) | $p_i$ | $r_i$ |
| term absent ($x_i=0$)  | $1-p_i$ | $1-r_i$ |

---

>**보충설명**
>
>1. **$x_i$가 1인 경우와 0인 경우로 분리하는 이유**  
>   - 원래 식은 모든 term을 곱하는 형태:  
>     $$
>     O(R|q,\vec{x}) = O(R|q)\cdot \prod_{i=1}^n \frac{p(x_i \mid R=1,q)}{p(x_i \mid R=0,q)}
>     $$  
>   - 여기서 각 항은 $x_i=1$일 때의 비율 또는 $x_i=0$일 때의 비율 중 하나를 택한다.  
>   - 따라서 곱을 $x_i=1$인 경우와 $x_i=0$인 경우로 분리할 수 있고,  
>     $$
>     O(R|q,\vec{x}) 
     = O(R|q)\cdot 
       \prod_{x_i=1} \frac{p(x_i=1 \mid R=1,q)}{p(x_i=1 \mid R=0,q)}
       \cdot
       \prod_{x_i=0} \frac{p(x_i=0 \mid R=1,q)}{p(x_i=0 \mid R=0,q)}
>     $$  
>   - 두 곱셈 항을 합치면 원래 $n$개의 항과 동일하다.  
>
>2. **모든 term이 쿼리에 나타나지 않는 경우 $q_i=0$이라고 가정하는 이유**  
>   - 쿼리에 없는 term은 관련성 판정에 영향을 주지 않아야 한다.  
>   - 이를 위해 $q_i=0$일 때 $p_i=r_i$로 두면,  
>     - $x_i=1$일 때: $\tfrac{p_i}{r_i} = 1$  
>     - $x_i=0$일 때: $\tfrac{1-p_i}{1-r_i} = 1$  
>   - 즉, 곱셈 항에서 $q_i=0$인 경우는 모두 $1$로 대체된다.  
>
>   - 따라서 원래 식  
>     $$
>     O(R|q,\vec{x}) 
     = O(R|q)\cdot 
       \prod_{x_i=1} \frac{p_i}{r_i}
       \cdot
       \prod_{x_i=0} \frac{1-p_i}{1-r_i}
>     $$  
>
>     가 **쿼리에 포함된 term($q_i=1$)**만 남는 식으로 단순화된다:  
>     $$
>     O(R|q,\vec{x}) 
     = O(R|q)\cdot 
       \prod_{x_i=1,\, q_i=1} \frac{p_i}{r_i}
       \cdot
       \prod_{x_i=0,\, q_i=1} \frac{1-p_i}{1-r_i}
>     $$  
>
>   - 즉, 쿼리에 없는 term들은 odds 계산에 아무 기여도 하지 않게 되어, 최종 식이 간단해진다.  

---

## Binary Independence Model

- 1단계: 쿼리 term($q_i=1$) 기준으로 odds 표현  
  $$
  O(R|q,\vec{x})
   = O(R|q) \cdot 
     \underbrace{\prod_{x_i=1,\, q_i=1} \frac{p_i}{r_i}}_{\text{All matching terms}}
     \cdot 
     \underbrace{\prod_{x_i=0,\, q_i=1} \frac{1-p_i}{1-r_i}}_{\text{Non-matching query terms}}
  $$  

---

- 2단계: 트릭을 이용한 전개  

  위 식에서 **All matching terms** 부분을 다음과 같이 변형한다.  
  $$
  \frac{p_i}{r_i}
  = \frac{p_i}{r_i} \cdot \frac{1-r_i}{1-p_i} \cdot \frac{1-p_i}{1-r_i}
  $$  

  여기서 $\frac{1-r_i}{1-p_i} \cdot \frac{1-p_i}{1-r_i} = 1$ 이므로,  
  새로운 항을 곱해도 식의 값은 변하지 않는다.  

  따라서  
  $$
  \prod_{x_i=1,\, q_i=1} \frac{p_i}{r_i}
  = \prod_{x_i=1,\, q_i=1} \frac{p_i}{r_i} \cdot \prod_{x_i=1,\, q_i=1} \frac{1-r_i}{1-p_i} \cdot \prod_{x_i=1,\, q_i=1} \frac{1-p_i}{1-r_i}
  $$  

  이를 전체 식에 대입하면,  

  $$
  O(R|q,\vec{x})
   = O(R|q) \cdot 
     \underbrace{\prod_{x_i=1,\, q_i=1} \frac{p_i}{r_i}}_{\text{원래 All matching terms}}
     \cdot 
     \underbrace{\prod_{x_i=1,\, q_i=1} \frac{1-r_i}{1-p_i}}_{\text{보정 항}}
     \cdot 
     \underbrace{\prod_{x_i=1,\, q_i=1} \frac{1-p_i}{1-r_i} \cdot \prod_{x_i=0,\, q_i=1} \frac{1-p_i}{1-r_i}}_{\text{Non-matching query terms와 합쳐짐}}
  $$  

---

- 3단계: 최종 단순화  

  이제 $x_i=1, q_i=1$인 경우와 $x_i=0, q_i=1$인 경우의 항들을 묶어 정리하면,  

  $$
  O(R|q,\vec{x})
   = O(R|q) \cdot 
     \underbrace{\prod_{x_i=q_i=1} \frac{p_i (1-r_i)}{r_i (1-p_i)}}_{\text{All matching terms}}
     \cdot 
     \underbrace{\prod_{q_i=1} \frac{1-p_i}{1-r_i}}_{\text{All query terms}}
  $$  

- **정리**  
  - 2단계의 트릭: $\frac{1-r_i}{1-p_i} \cdot \frac{1-p_i}{1-r_i} = 1$을 곱해 넣어 항을 분리.  
  - 이렇게 해서 All matching terms와 Non-matching query terms가 정리되어,  
    최종적으로 쿼리 term 전체에 대한 odds 공식을 얻는다.  

---

## Binary Independence Model

- 1단계: 쿼리 term($q_i=1$) 기준으로 odds 표현  

$$
O(R|q,\vec{x})
 = O(R|q) \cdot 
   \prod_{x_i = q_i = 1} \frac{p_i(1-r_i)}{r_i(1-p_i)} 
   \cdot \prod_{q_i=1} \frac{1-p_i}{1-r_i}
$$  

- $O(R|q)$는 **쿼리마다 고정되는 값**으로, 순위 계산에서는 상수 역할을 한다.  
- 첫 번째 곱($\prod_{x_i = q_i = 1}$)은 **쿼리와 문서 모두에 등장하는 단어(매칭 term)**들의 기여도를 의미한다.  
- 두 번째 곱($\prod_{q_i=1}$)은 **쿼리에는 포함되지만 문서에는 등장하지 않는 단어(비매칭 term)**들의 기여도를 의미한다.  

---

- 2단계: Retrieval Status Value(RSV) 정의  

위 식에서 랭킹에 필요한 부분만 로그를 취해 정의하면,  

$$
RSV = \log \prod_{x_i = q_i = 1} \frac{p_i(1-r_i)}{r_i(1-p_i)}
    = \sum_{x_i = q_i = 1} \log \frac{p_i(1-r_i)}{r_i(1-p_i)}
$$  

- **RSV(Retrieval Status Value)**는 문서가 쿼리와 얼마나 관련 있는지를 점수로 나타낸다.  
- 각 매칭 term마다 $\frac{p_i(1-r_i)}{r_i(1-p_i)}$라는 비율을 계산하여 로그 합으로 표현한다.  
- 이 값이 클수록 해당 문서가 쿼리와 더 관련성이 크다고 판단된다.  

---

## BM25 모델

- **BM25 (Okapi BM25) ?**  
  - 확률적 정보검색 모델의 대표적 알고리즘  
  - TF-IDF 모델을 개선한 확률적 정보 검색 랭킹 함수  
    - TF-IDF의 한계점을 보완하여 **문서의 길이와 단어 빈도수**를 더욱 정교하게 반영  
    - 문서와 질의(쿼리) 간의 관련성을 점수화하여 검색 결과를 순위 매기는 데 사용  
  - **Best Matching 25**의 약자로, 25는 이 모델의 25번째 버전이라는 의미  
  - 현재 대부분의 검색엔진에서 기본 알고리즘으로 사용  

- **이론적 기반**  
  - 확률 순위 원리 (Probability Ranking Principle)  

- **BIM (Binary Independent Model)**  
  - Term Frequency를 고려하지 않음  
  - 문서의 길이를 고려하지 않음  
  - Term 간의 Independence를 가정  

- **BM25 모델의 개선점**  
  - **Term Frequency 고려**  
  - **문서의 길이 정규화 적용**  
  - **포화 함수** 사용으로 과도한 TF 억제  

---

>**보충설명**
>
>- **왜 BM25가 중요한가?**  
>  - 기존 TF-IDF는 단순히 단어 빈도와 역문서 빈도를 곱하는 방식이라, 문서 길이가 길거나 특정 단어가 반복되는 경우 공정성이 떨어지는 문제가 있었다.  
>  - BM25는 이를 보완하여 **현실적인 검색 시나리오에서 더 합리적인 점수 계산**을 제공한다.  
>
>- **BIM과의 차이**  
>  - Binary Independent Model(BIM)은 단어가 문서에 등장했는지(0/1)만 반영하기 때문에, 실제로 몇 번 등장했는지에 따른 **빈도 정보(TF)**를 활용하지 못했다.  
>  - 또한 문서 길이를 고려하지 않아, 긴 문서가 짧은 문서보다 불리하거나 유리해지는 왜곡이 발생했다.  
>  - **Independence 가정의 문제:** BIM은 각 단어(term)가 서로 독립적으로 등장한다고 가정하지만, 실제 언어 데이터에서는 단어들이 종종 **강한 상관관계(예: "machine"과 "learning")**를 가진다. 이 가정은 모델을 단순화하지만 현실을 잘 반영하지 못하는 한계가 있다.  
>
>- **BM25의 개선 아이디어**  
>  - **TF 고려:** 단어가 여러 번 등장할수록 문서가 더 관련 있을 가능성이 크다고 가정.  
>  - **문서 길이 정규화:** 긴 문서는 자연스럽게 단어가 더 많이 등장할 수 있으므로, 이를 보정하여 짧은 문서와 공정하게 비교.  
>  - **포화 함수 적용:** 단어가 지나치게 많이 등장하더라도 관련성이 무한히 커지지 않도록 **증가율을 점차 둔화(saturation)**시킴.  
>
>- **이론적 기반과 실제 활용**  
>  - BM25는 **확률 순위 원리(PRP)**에 기반하여, “각 문서의 관련성 확률을 추정해 내림차순으로 정렬하면 최적의 검색 성능을 달성할 수 있다”는 이론을 실용적으로 구현한 것이다.  
>  - 이 때문에 대부분의 검색엔진(예: Elasticsearch, Lucene, Solr 등)에서 기본 검색 알고리즘으로 채택되어 있다.  
>
>- **Okapi의 의미**  
>  - BM25에서 **Okapi**는 런던 시티 대학의 **Okapi 정보검색 시스템**에서 유래된 이름이다.  
>  - 1980~1990년대에 개발된 Okapi 시스템은 **실험적인 문헌 검색 시스템**으로, BM25를 포함한 다양한 정보검색 모델을 테스트하고 검증하는 데 사용되었다.  
>  - 따라서 "Okapi BM25"라는 이름은 BM25가 Okapi 시스템에서 개발·검증되었음을 반영한다.  

---

## BM25 모델

**BM25의 주요 개선점**

- **TF 포화 (TF Saturation)**  
  - TF-IDF는 단어 빈도수(TF)가 높을수록 점수가 선형적으로 증가  
  - 한 문서에 특정 단어가 아무리 많이 나와도 문서의 관련성 점수가 무한히 커지는 것은 비합리적  
  - BM25는 TF가 일정 수준 이상이 되면 증가율을 둔화시키는 **포화 개념**을 도입  
  - 문서에 단어가 너무 많이 등장하는 경우를 제어하여, 특정 키워드를 반복적으로 사용해 검색 결과를 조작하는 것을 방지하는 효과  

---

- **문서 길이 정규화 (Document Length Normalization)**  
  - TF-IDF는 문서의 길이가 길수록 TF 값이 높아져 짧은 문서보다 높은 점수를 받는 경향  
  - **BM25는 문서의 길이를 평균 문서 길이와 비교하여 점수를 정규화**  
    - 평균보다 긴 문서는 페널티를 주고, 짧은 문서는 상대적으로 높은 점수 → **공정성 확보**  

---

## BM25 모델

**BM25의 주요 개선점**  

- TF 포화(TF Saturation)
  - TF-IDF는 단어 빈도수(TF)가 높을수록 점수가 선형적으로 증가
  - 한 문서에 특정 단어가 아무리 많이 나와도 문서의 관련성 점수가 무한히 커지는 것은 비합리적
  - BM25는 TF가 일정 수준 이상이 되면 증가율을 둔화시키는 '포화' 개념을 도입
  - 문서에 단어가 너무 많이 등장하는 경우를 제어하여, 특정 키워드를 반복적으로 사용해 검색 결과를 조작하는 것을 방지하는 효과

- 문서 길이 정규화(Document Length Normalization)
  - TF-IDF는 문서의 길이가 길수록 TF 값이 높아져 짧은 문서보다 높은 점수를 받는 경향
  - BM25는 문서의 길이를 평균 문서 길이와 비교하여 점수를 정규화
    - 평균보다 긴 문서는 패널티를 주고, 짧은 문서는 상대적으로 높은 점수 → 공정성 확보

---

## BM25 모델

- **Tf 포화(tf saturation)**  

$$
\frac{tf}{k_1 + tf}
$$  

- **k1은 문서 내 단어 빈도에 얼마나 가중치를 둘 것인지를 결정하는 중요한 요소**  
  - **높은 k1 값**: tf가 증가할수록 점수도 계속해서 거의 선형적으로 증가.  
    - 단어가 많이 나올수록 문서의 관련성이 계속해서 커진다고 판단  
  - **낮은 k1 값**: tf가 어느 정도 증가하면 점수 증가율이 급격히 둔화.  
    - 일정 횟수 이상 등장하는 단어는 더 이상 점수에 큰 영향을 주지 않는다고 판단  
    
<img src="/assets/img/bigdatasearch/3/image_5.png" alt="image" width="600px">

---

>**보충설명**
>
>**왜 k1이 중요한가?**  
>  - k1은 문서 내 단어 빈도(tf)에 얼마나 가중치를 줄 것인지 결정하는 핵심 파라미터이다.  
>  - k1 값이 크면 tf가 증가할수록 점수가 선형적으로 계속 증가한다.  
>  - k1 값이 작으면 tf가 조금만 커져도 점수가 빠르게 포화되어 이후 영향이 줄어든다.  
>
>**높은 k1 값의 경우 (선형적 증가 경향)**  
>  - 예: k1 = 10  
>  - tf=1 → 1/(10+1)=0.09  
>  - tf=5 → 5/(10+5)=0.33  
>  - tf=10 → 10/(10+10)=0.50  
>  - tf=50 → 50/(10+50)=0.83  
>  → tf가 커질수록 꾸준히 증가하며 선형적인 패턴에 가까움.  
>
> **낮은 k1 값의 경우 (빠른 포화)**  
>  - 예: k1 = 1  
>  - tf=1 → 1/(1+1)=0.50  
>  - tf=5 → 5/(1+5)=0.83  
>  - tf=10 → 10/(1+10)=0.91  
>  - tf=50 → 50/(1+50)=0.98  
>  → tf=5 정도만 되어도 점수가 거의 포화 상태에 도달하여 이후 증가율이 급격히 둔화됨.  
>
>**의의**  
>  - 높은 k1은 단어 빈도가 많을수록 문서의 관련성이 계속 커진다고 보는 모델에 적합하다.  
>  - 낮은 k1은 특정 단어가 일정 횟수 이상 등장하면 더 이상 큰 의미가 없다고 보는 모델에 적합하다.  
>  - 이를 통해 검색 결과에서 특정 키워드를 반복적으로 삽입해 점수를 인위적으로 높이는 행위를 방지할 수 있다.  

---

## BM25 모델

**문서 길이의 정규화**  

- 문서 길이가 길어지면 tf의 값이 커지는 경향이 있음  

  $$
  dl = \sum_{i \in V} tf_i
  $$

- *avdl*: 문서 집합 내 **평균 문서 길이**  
- 문서 길이 정규화 요소  

  $$
  B = \left( (1-b) + b \cdot \frac{dl}{avdl} \right), \quad 0 \leq b \leq 1
  $$

  - $b = 1$: full document length normalization  
    → 문서 길이를 **완전히 보정**  
  - $b = 0$: no document length normalization  
    → 문서 길이에 대해 **보정하지 않음**  

<img src="/assets/img/bigdatasearch/3/image_6.png" alt="image" width="600px">

---  
>**보충설명**
>
> **차트 해석**  
>  - 그래프는 문서 길이(dl)가 평균 문서 길이(avdl)에 비해 짧거나 긴 경우, BM25 점수가 어떻게 변하는지를 보여준다.  
>  - **dl = avdl * 0.1 (빨간선):** 문서 길이가 평균보다 매우 짧을 때, 같은 tf 값이라도 점수가 상대적으로 높게 나온다.  
>  - **dl = avdl (검은선):** 문서 길이가 평균일 때, 기준선처럼 점수가 증가한다.  
>  - **dl = avdl * 10 (파란선):** 문서 길이가 평균보다 훨씬 길 경우, 같은 tf 값이라도 점수가 낮아진다.  
>
> **의미**  
>  - 긴 문서는 자연스럽게 단어가 많이 포함될 수 있으므로, 이를 그대로 두면 불공정하다.  
>  - 따라서 BM25는 **문서 길이 정규화(document length normalization)**를 통해 긴 문서에는 불이익을, 짧은 문서에는 상대적 이익을 주어 **공정한 점수 비교**가 가능하게 한다.  
>
> **b 파라미터의 역할**  
>  - $b=1$일 때: 문서 길이에 대한 보정을 **완전히 적용**하여, 길이가 긴 문서에 강한 패널티를 부여.  
>  - $b=0$일 때: 문서 길이를 전혀 고려하지 않음.  
>  - 실제로는 $0 < b < 1$의 값을 사용하여, **적절히 균형 잡힌 보정**을 수행한다.  
  
---

## BM25 모델

- **BM25의 핵심 수식과 구성 요소**

$$
RSV^{BM25} = \sum_{i \in q} \log \frac{N}{df_i} \cdot 
\frac{(k_1+1)tf_i}{k_1 \left( (1-b) + b \frac{dl}{avdl} \right) + tf_i}
$$

- 질의에 포함된 모든 단어에 대해 계산  
- idf가 높은 희귀 단어일수록 최종 점수에 더 큰 영향을 줌  

---

- **N:** 총 문서의 수  
- **dfᵢ:** 쿼리 q가 출현한 문서의 수  
- **tfᵢ:** 쿼리 q가 출현한 빈도  
- **dl:** 문서 d의 길이  
- **avdl:** 전체 문서 집합의 평균 길이  
- **k₁:** TF 포화도를 조절하는 하이퍼파라미터  
  - k₁이 작을수록 TF 포화가 빨리 일어남 (일반적으로 1.2 ~ 2.0 사이의 값)  
- **b:** 문서 길이 정규화 정도를 조절하는 하이퍼파라미터  
  - b가 1에 가까울수록 문서 길이에 대한 패널티가 커짐 (일반적으로 0.75)  

---

>**보충설명**
>
> **BM25 수식의 부분별 의미**  
>  BM25는 세 가지 핵심 요소(IDF, TF 포화, 문서 길이 정규화)를 결합해 현실적인 검색 순위를 산정한다.  
>
> **IDF (Inverse Document Frequency)**  
>  $$
>  \log \frac{N}{df_i}
>  $$  
>  - 전체 문서 수 $N$ 대비 단어 $i$가 등장한 문서 수 $df_i$로 계산된다. 
>  - 희귀한 단어일수록 값이 커져 **검색에서 중요한 단어**로 반영된다.  
>
> **TF 포화 (TF Saturation)**  
>  $$
>  \frac{(k_1+1)tf_i}{k_1\left((1-b) + b\frac{dl}{avdl}\right) + tf_i}
>  $$  
>  - 단어 빈도 $tf_i$가 증가할수록 점수가 증가한다.  
>  - 그러나 분모에도 $tf_i$가 포함되어 있어 일정 수준 이상에서는 증가율이 둔화된다.  
>  → 특정 단어가 반복적으로 등장해도 점수가 무한정 커지지 않도록 제어하는 장치.  
> 
> **원래 포화 함수와의 차이**  
>    - 단순 포화 함수: $\tfrac{tf}{k_1+tf}$ → 값의 범위는 $0 \sim 1$, tf=1일 때 $\tfrac{1}{k_1+1}$로 작음.  
>    - BM25 식: $\tfrac{(k_1+1)tf}{k_1+tf}$ → tf=1일 때 값이 정확히 1이 되도록 보정.  
>    - 즉, **최소 한 번 등장한 단어는 반드시 의미 있는 기여를 하도록 설계**된 것이다.  
>    - 이후 tf가 증가하면 값은 $(k_1+1)$에 점차 포화된다.  
>
> **숫자 예시 (k1=2)**  
>    - 원래 식: tf=1 → 0.33, tf=5 → 0.71, tf=10 → 0.83  
>    - BM25 식: tf=1 → 1.0, tf=5 → 2.5, tf=10 → 2.73  
>    - → BM25는 **등장 여부 자체의 가중치**를 확보하면서도 포화 특성은 유지한다.  
>
>**문서 길이 정규화 (Document Length Normalization)**  
>  $$
>  (1-b) + b \cdot \frac{dl}{avdl}
>  $$  
>  - $dl$: 해당 문서의 길이  
>  - $avdl$: 전체 문서 집합의 평균 문서 길이  
>  - $b$: 문서 길이에 대한 보정 강도 (0 ≤ b ≤ 1)  
>  - 긴 문서일수록 단어가 더 많이 등장할 가능성이 있으므로, 이를 보정하여 **긴 문서와 짧은 문서를 공정하게 비교**할 수 있게 한다.  
>
> **k₁ 파라미터의 역할**  
>  - $k_1$은 TF 포화가 일어나는 속도를 조절하는 하이퍼파라미터이다.  
>  - $k_1$이 크면 → TF가 증가해도 점수가 **꾸준히 선형적으로 증가**한다. 
>  - $k_1$이 작으면 → TF가 조금만 커져도 점수가 **빠르게 포화**되어 더 이상 큰 영향을 주지 않는다.  
>  - 일반적으로 1.2 ~ 2.0 사이의 값이 자주 사용된다.  
>
> **종합적 의의**  
>  - IDF: 희귀 단어에 가중치  
>  - TF 포화: 과도한 단어 반복 억제 + 최소 1회 등장 보장  
>  - 문서 길이 정규화: 긴 문서와 짧은 문서를 공정하게 비교  
>  - → BM25는 이 세 요소를 결합하여 **현실적인 검색 성능을 보장하는 대표적 랭킹 함수**가 된다.  

---

## BM25 모델

- **BM25 vs. tf-idf VSM**
  - Query: "machine learning"
  - Query의 term이 출현한 문서 2개
    - Doc1: learning 1024, machine 1
    - Doc2: learning 16, machine 8  

- **tf-idf 계산**  
  $$
  \log_2 tf \times \log_2 \frac{N}{df}
  $$
  - Doc1: $11 \times 7 + 1 \times 10 = 87$  
  - Doc2: $5 \times 7 + 4 \times 10 = 75$  

- **BM25 (k₁ = 2)**  
  - Doc1: $7 \times 3 + 10 \times 1 = 31$  
  - Doc2: $7 \times 2.67 + 10 \times 2.4 = 42.7$  

---

>**보충설명**
>
> **TF–IDF 수식 (강의 슬라이드 기준)**  
>  $$
>  w_{t,d} = \log_2(1+tf_{t,d}) \cdot \log_2 \frac{N}{df_t}
>  $$  
>
>  - $tf_{t,d}$ : 문서 $d$에서 term $t$가 등장한 횟수  
>  - $N$ : 전체 문서 수  
>  - $df_t$ : term $t$가 등장한 문서 수  
>
>---
>
> **계산 과정 (Doc1: "learning 1024, machine 1")**  
>
>  1. **TF 변환**  
>     - learning: $\log_2(1+1024) = \log_2(1025) \approx 10$  
>     - machine: $\log_2(1+1) = \log_2 2 = 1$  
>
>  2. **IDF 계산 (슬라이드 제시값 사용)**  
>     - $\log_2 \frac{N}{df_{learning}} = 7$  
>     - $\log_2 \frac{N}{df_{machine}} = 10$  
>
>  3. **최종 가중치 합산**  
>     - learning: $10 \times 7 = 70$  
>     - machine: $1 \times 10 = 10$  
>     - 합: $70 + 10 = 87$  
>
>---
>
> **계산 과정 (Doc2: "learning 16, machine 8")**  
>  1. **TF 변환**  
>     - learning: $\log_2(1+16) = \log_2 17 \approx 4$  
>     - machine: $\log_2(1+8) = \log_2 9 \approx 3$  
>
>  2. **IDF 계산 (슬라이드 제시값 사용)**  
>     - $\log_2 \frac{N}{df_{learning}} = 7$  
>     - $\log_2 \frac{N}{df_{machine}} = 10$  
>
>  3. **최종 가중치 합산**  
>     - learning: $4 \times 7 = 28$  
>     - machine: $3 \times 10 = 30$  
>     - 합: $28 + 30 = 75$  
>
>---
>
> **BM25 수식 (강의 슬라이드 기준, $k_1 = 2$ 가정)**  
>  $$
>  w_{t,d} = \frac{(k_1+1) \cdot tf_{t,d}}{k_1 + tf_{t,d}} \cdot idf_t
>  $$  
>
>  - $tf_{t,d}$ : 문서 $d$에서 term $t$의 빈도  
>  - $idf_t$ : 슬라이드에서 제시된 IDF 값 사용  
>  - $k_1 = 2$  
>
>---
>
> **계산 과정 (Doc1: "learning 1024, machine 1")**  
>  1. **TF 보정**  
>     - learning: $\frac{(2+1)\cdot 1024}{2+1024} \approx 3$  
>     - machine: $\frac{(2+1)\cdot 1}{2+1} = 1$  
>
>  2. **IDF 적용 (슬라이드 값 사용)**  
>     - learning: $3 \times 7 = 21$  
>     - machine: $1 \times 10 = 10$  
>
>  3. **최종 합산**  
>     - $21 + 10 = 31$  
>
>---
>
> **계산 과정 (Doc2: "learning 16, machine 8")**  
>  1. **TF 보정**  
>     - learning: $\frac{(2+1)\cdot 16}{2+16} = \frac{48}{18} \approx 2.67$  
>     - machine: $\frac{(2+1)\cdot 8}{2+8} = \frac{24}{10} = 2.4$  
>
>  2. **IDF 적용 (슬라이드 값 사용)**  
>     - learning: $2.67 \times 7 \approx 18.7$  
>     - machine: $2.4 \times 10 = 24$  
>
>  3. **최종 합산**  
>     - $18.7 + 24 \approx 42.7$  
>
>---
>
> **의의**  
>  - TF–IDF는 단어 빈도를 로그 변환으로 완화하고 희귀 단어에 높은 가중치를 준다.  
>  - BM25는 추가적으로 **TF 포화**를 반영하여 특정 단어가 지나치게 많이 등장해도 점수가 무한정 커지지 않게 한다.  
>  - 이 예제에서 TF–IDF는 Doc1이 훨씬 큰 점수를 얻었지만, BM25는 TF 포화 덕분에 Doc2가 더 높은 점수를 받아 **짧지만 중요한 단어가 집중된 문서**를 더 잘 반영한다.  

---

## BM25

**BM25 vs. TF-IDF 요약**

| 특징     | TF-IDF             | BM25                          |
|----------|--------------------|-------------------------------|
| **TF 처리** | 선형 증가            | 포화 함수 적용                  |
| **문서 길이** | 정규화 미흡          | 평균 길이 대비 정규화             |
| **장점**   | 직관적이고 단순함       | 실제 검색 환경에 더 최적화됨         |
| **단점**   | 문서 길이에 취약        | 하이퍼파라미터(k1, b) 튜닝 필요   |
| **활용**   | 간단한 검색 및 분석     | 상용 검색 엔진의 핵심 알고리즘       |