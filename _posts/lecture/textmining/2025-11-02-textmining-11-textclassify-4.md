---
layout: post
title: "[í…ìŠ¤íŠ¸ ë§ˆì´ë‹] 11. Text Classification 4"
date: 2025-11-02 05:00:00 +0900
categories:
  - "ëŒ€í•™ì› ìˆ˜ì—…"
  - "í…ìŠ¤íŠ¸ ë§ˆì´ë‹"
tags: []
---

## p3. ì œí•œëœ ë ˆì´ë¸”ë¡œ í•™ìŠµí•˜ê¸° (Learning with limited labels)

- ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” **ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°(labeled data)** ë¥¼ ì‚¬ìš©í•˜ì—¬  
  **ë¶„ë¥˜ê¸°(classifier)ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•** ì— ëŒ€í•´ ë…¼ì˜í•˜ì˜€ë‹¤.  

  - ê° ì…ë ¥ í…ìŠ¤íŠ¸ $x$ ë§ˆë‹¤, í•´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ ë ˆì´ë¸” $y$ ê°€ ì£¼ì–´ì§„ë‹¤ê³  ê°€ì •í•˜ì˜€ë‹¤.  

- ê·¸ëŸ¬ë‚˜ **í˜„ì‹¤ì—ì„œëŠ” ëª¨ë“  ë°ì´í„° ì¸ìŠ¤í„´ìŠ¤ì— ë ˆì´ë¸”ì´ ì¡´ì¬í• ê¹Œ?**  

<img src="/assets/img/lecture/textmining/9/image_1.png" alt="image" width="360px">

- ì´ì œ ìš°ë¦¬ëŠ” **ë ˆì´ë¸”ì´ ë¶€ì¡±í•œ ë°ì´í„°(scarcity of labeled data)** ë¥¼  
  ì–´ë–»ê²Œ ë‹¤ë£¨ëŠ”ì§€ë¥¼ ì‚´í´ë³¼ ê²ƒì´ë‹¤.  

1. **ì¤€ì§€ë„ í•™ìŠµ(Semi-supervised learning)**  
   - â€œë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆì„ê¹Œ?â€  
     *(How can we effectively leverage unlabeled data?)*  

2. **ë‹¤ì¤‘ ì‘ì—… í•™ìŠµ(Multi-task learning)**  
   - â€œí•˜ë‚˜ì˜ ì‘ì—…ì— ë ˆì´ë¸”ì´ ë¶€ì¡±í•˜ë‹¤ë©´,  
     ê´€ë ¨ëœ ë‹¤ë¥¸ ì‘ì—…ìœ¼ë¡œë¶€í„° ì‹ í˜¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆì„ê¹Œ?â€  
     *(If one task doesnâ€™t have enough labels, can we borrow signals from related tasks?)*  

3. **ì ëŒ€ì  í•™ìŠµ(Adversarial learning)**  
   - â€œë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ì™€ ì—†ëŠ” ë°ì´í„°ê°€ ì„œë¡œ ë‹¤ë¥¸ ë¶„í¬ì—ì„œ ì™”ë‹¤ë©´ ì–´ë–»ê²Œ í• ê¹Œ?â€  
     *(What if labeled and unlabeled data come from different distributions?)*  

---

## p4. ë™ê¸°: ì˜ˆì‹œ (Motivation: example)

- ê°€ì§œ ë‰´ìŠ¤ íƒì§€ ì‹œìŠ¤í…œ(fake news detection system)ì„ ê³ ë ¤í•˜ì.  
  - ìˆ˜ì§‘ëœ **ë‰´ìŠ¤ ë°ì´í„°(news data)** ì™€ **ë ˆì´ë¸”(labels)** ì„ ì‚¬ìš©í•˜ì—¬  
    ì´ì§„ ë¶„ë¥˜(binary classification) ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤.

<img src="/assets/img/lecture/textmining/11/image_1.png" alt="image" width="800px">

---

## p5. ë™ê¸°: ì˜ˆì‹œ (Motivation: example)

- ê°€ì§œ ë‰´ìŠ¤ íƒì§€ ì‹œìŠ¤í…œ(fake news detection system)ì„ ê³ ë ¤í•˜ì.  
  - í•˜ì§€ë§Œ ë¬¸ì œëŠ”â€¦ **ìƒˆë¡œìš´ ì‚¬ê±´ë“¤(new events)** ì´ ê³„ì† ë°œìƒí•œë‹¤!  
  - ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ì€ **ìƒˆë¡œìš´ ì‚¬ê±´ë“¤(new events)** ì— **ì¼ë°˜í™”(generalize)** ë˜ì§€ ëª»í•  ìˆ˜ë„ ìˆë‹¤.

<img src="/assets/img/lecture/textmining/11/image_2.png" alt="image" width="800px">

---

## p6. ë™ê¸°: ì˜ˆì‹œ (Motivation: example)

- ê°€ì§œ ë‰´ìŠ¤ íƒì§€ ì‹œìŠ¤í…œ(fake news detection system)ì„ ê³ ë ¤í•˜ì.  
  - í•˜ì§€ë§Œ ë¬¸ì œëŠ”â€¦ **ìƒˆë¡œìš´ ì‚¬ê±´ë“¤(new events)** ì´ ê³„ì† ë°œìƒí•œë‹¤!  
  - ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ì€ **ìƒˆë¡œìš´ ì‚¬ê±´ë“¤(new events)** ì— **ì¼ë°˜í™”(generalize)** ë˜ì§€ ëª»í•  ìˆ˜ë„ ìˆë‹¤.

<img src="/assets/img/lecture/textmining/11/image_3.png" alt="image" width="720px">

- ì´ ë¬¸ì œëŠ” **ëª¨ë“  ìƒˆë¡œìš´ ë‰´ìŠ¤ ë°ì´í„°(new news data)** ì— ëŒ€í•´ **ë ˆì´ë¸”(labels)** ì´ ìˆë‹¤ë©´ í•´ê²°ë  ìˆ˜ ìˆì§€ë§Œ,  
  **ê·¸ê²ƒì€ ë¹„í˜„ì‹¤ì ì´ë‹¤(thatâ€™s infeasible).**

- **ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ìš°ë¦¬ëŠ” ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œ? (How do we cope with such a problem?)**

---

## p7. ë¬¸ì œ: ë„ë©”ì¸ ì ì‘ (Problem: domain adaptation)

- ì´ ë¬¸ì œëŠ” **ë„ë©”ì¸ ì ì‘(domain adaptation)** ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤.  
- í˜•ì‹ì ìœ¼ë¡œ, ì„œë¡œ ë‹¤ë¥¸ ë¶„í¬(ë„ë©”ì¸, domain)ë¡œë¶€í„° ì˜¨ ë‘ ë°ì´í„° ì§‘í•©ì„ ê°–ëŠ”ë‹¤.

1. **ë ˆì´ë¸”ëœ ì†ŒìŠ¤ ë„ë©”ì¸ (Labeled source domain):**  

   $$ D_S = \lbrace (x_i^S, y_i^S) \rbrace_{i=1}^n \quad \text{from} \quad P_S(x, y) $$

2. **ë ˆì´ë¸”ë˜ì§€ ì•Šì€ íƒ€ê¹ƒ ë„ë©”ì¸ (Unlabeled target domain):**  

   $$ D_T = \lbrace x_i^T \rbrace_{i=1}^m \quad \text{from} \quad P_T(x, y) $$

<img src="/assets/img/lecture/textmining/11/image_4.png" alt="image" width="360px">

- ìš°ë¦¬ëŠ” **ë ˆì´ë¸”ëœ ì†ŒìŠ¤ ë°ì´í„°(labeled source data)** ì— ëŒ€í•œ ì˜¤ì°¨(error)ë¥¼ ìµœì†Œí™”í•  ìˆ˜ ìˆë‹¤:

$$ \displaystyle
\min_\theta \; \mathbb{E}_{(x,y)\sim P_S(x,y)} [L(f_\theta(x), y)] 
\;\approx\; \frac{1}{n} \sum_{i=1}^{n} L(f_\theta(x_i^S), y_i^S)
$$

- ê·¸ëŸ¬ë‚˜ ì´ ëª¨ë¸ì€ **íƒ€ê¹ƒ ë„ë©”ì¸(target domain)** ì— ëŒ€ì‘í•˜ì§€ ëª»í•œë‹¤.  
  (ì¦‰, $ P_T(x, y) $ ê°€ $ P_S(x, y) $ ì™€ ë‹¤ë¥¼ ë•Œ ë¬¸ì œ ë°œìƒ)  
- **SSL ê¸°ë²•(ì˜ˆ: pseudo-labeling)** ì„ ì ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜,  
  ê·¸ë“¤ì˜ ì˜ˆì¸¡(predictions)ì€ ì¢…ì¢… **ë¶€ì •í™•í•˜ê±°ë‚˜(inaccurate)** **í¸í–¥(biased)** ë˜ì–´ ìˆë‹¤.

---

## p8. ë¬¸ì œ: ë„ë©”ì¸ ì ì‘ (Problem: domain adaptation)

- ì´ ë¬¸ì œëŠ” **ë„ë©”ì¸ ì ì‘(domain adaptation)** ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤.  
- í˜•ì‹ì ìœ¼ë¡œ, ì„œë¡œ ë‹¤ë¥¸ ë¶„í¬(ë„ë©”ì¸, domain)ë¡œë¶€í„° ì˜¨ ë‘ ë°ì´í„° ì§‘í•©ì„ ê°–ëŠ”ë‹¤.

1. **ë ˆì´ë¸”ëœ ì†ŒìŠ¤ ë„ë©”ì¸ (Labeled source domain):**  
   $ D_S = \lbrace (x_i^S, y_i^S) \rbrace_{i=1}^n \quad \text{from} \quad P_S(x, y) $

2. **ë ˆì´ë¸”ë˜ì§€ ì•Šì€ íƒ€ê¹ƒ ë„ë©”ì¸ (Unlabeled target domain):**  
   $ D_T = \lbrace x_i^T \rbrace_{i=1}^m \quad \text{from} \quad P_T(x, y) $

<img src="/assets/img/lecture/textmining/11/image_4.png" alt="image" width="360px">

**í•˜ë‚˜ì˜ ê°€ì • (An assumption):**  
- ë‘ ë„ë©”ì¸ ì‚¬ì´ì—ëŠ” ì…ë ¥ $x$ ë¥¼ ë ˆì´ë¸” $y$ ë¡œ ë§¤í•‘í•˜ëŠ” **ê³µí†µëœ ê²°ì • íŒ¨í„´(common decision pattern)** ì´ ì¡´ì¬í•œë‹¤.  

  $$ P_S(y \mid x) = P_T(y \mid x) $$

- ë‹¤ì‹œ ë§í•´, **ë ˆì´ë¸”ë§ ê·œì¹™(labeling rule)** ì´ ë„ë©”ì¸ ê°„ì— ê³µìœ ëœë‹¤.  
- ì´ ê°€ì •ì€ **í•­ìƒ ì—„ë°€í•˜ê²Œ ì°¸ì€ ì•„ë‹ˆì§€ë§Œ(not always strictly true)**,  
  **ë§ì€ ì‹¤ì œ(real-world) ê²½ìš°ì—ì„œ ê½¤ ì˜ ì„±ë¦½í•œë‹¤(holds reasonably well).**

---

## p9. ë°ì´í„° ê°€ì¤‘ì¹˜ ì¡°ì • (Data reweighting)

---

## p10. ë°ì´í„° ê°€ì¤‘ì¹˜ ì¡°ì • (Data reweighting)

- í•œ ê°€ì§€ í•´ê²°ì±…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.  
  (1) ê° ì†ŒìŠ¤ ìƒ˜í”Œ(source sample)ì´ íƒ€ê¹ƒ ë„ë©”ì¸(target domain)ì— **ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ì¶”ì •(estimate)** í•˜ê³   
  (2) ì´ë¥¼ **ì„ íƒì ìœ¼ë¡œ ë°˜ì˜(reflect)** í•˜ëŠ” ê²ƒì´ë‹¤.

- **í•µì‹¬ ì•„ì´ë””ì–´ (Key idea):**  
  íƒ€ê¹ƒ ë¶„í¬ $P_T(x)$ í•˜ì—ì„œ **ë” ë†’ì€ í™•ë¥ (more likely)** ì„ ê°€ì§€ëŠ” ì†ŒìŠ¤ ì˜ˆì‹œ(source examples)ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì¸ë‹¤(upweight).

<img src="/assets/img/lecture/textmining/11/image_5.png" alt="image" width="800px">

- ë‘ ê°œì˜ ì†ŒìŠ¤ ë¶€ë¶„ì§‘í•©(subset) Aì™€ Bë¥¼ ê³ ë ¤í•˜ì.  
  íƒ€ê¹ƒ ë„ë©”ì¸ì„ ì´í•´í•˜ëŠ” ë° ìˆì–´ ì–´ëŠ ìª½ì´ ë” ì¤‘ìš”í• ê¹Œ?

---

## p11. ë°ì´í„° ê°€ì¤‘ì¹˜ ì¡°ì • (Data reweighting)

- í•œ ê°€ì§€ í•´ê²°ì±…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.  
  (1) ê° ì†ŒìŠ¤ ìƒ˜í”Œ(source sample)ì´ íƒ€ê¹ƒ ë„ë©”ì¸(target domain)ì— **ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ì¶”ì •(estimate)** í•˜ê³   
  (2) ì´ë¥¼ **ì„ íƒì ìœ¼ë¡œ ë°˜ì˜(reflect)** í•˜ëŠ” ê²ƒì´ë‹¤.

- **í•µì‹¬ ì•„ì´ë””ì–´ (Key idea):**  
  íƒ€ê¹ƒ ë¶„í¬ $P_T(x)$ í•˜ì—ì„œ **ë” ë†’ì€ í™•ë¥ (more likely)** ì„ ê°€ì§€ëŠ” ì†ŒìŠ¤ ì˜ˆì‹œ(source examples)ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì¸ë‹¤(upweight).

<img src="/assets/img/lecture/textmining/11/image_6.png" alt="image" width="800px">

- ë‘ ê°œì˜ ì†ŒìŠ¤ ë¶€ë¶„ì§‘í•©(subset) Aì™€ Bë¥¼ ê³ ë ¤í•˜ì.  
  íƒ€ê¹ƒ ë„ë©”ì¸ì„ ì´í•´í•˜ëŠ” ë° ìˆì–´ ì–´ëŠ ìª½ì´ ë” ì¤‘ìš”í• ê¹Œ?  

- **íƒ€ê¹ƒ ë„ë©”ì¸(target domain)** ê³¼ **ìœ ì‚¬í•œ ë°ì´í„°(data)** ì— ì§‘ì¤‘í•˜ëŠ” ê²ƒì´ ë” **ìœ ìµí•˜ë‹¤(beneficial)**!

---

## p12. ë°ì´í„° ê°€ì¤‘ì¹˜ ì¡°ì •: ê°œìš” (Data reweighting: overview)

- ìš°ë¦¬ì˜ ëª©í‘œ(goal)ëŠ” **íƒ€ê¹ƒ ë„ë©”ì¸(target domain)** ì—ì„œì˜ ì˜¤ì°¨(error)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.

<img src="/assets/img/lecture/textmining/11/image_7.png" alt="image" width="520px">

---

## p13. ë°ì´í„° ê°€ì¤‘ì¹˜ ì¡°ì •: ê°œìš” (Data reweighting: overview)

- ìš°ë¦¬ì˜ ëª©í‘œ(goal)ëŠ” **íƒ€ê¹ƒ ë„ë©”ì¸(target domain)** ì—ì„œì˜ ì˜¤ì°¨(error)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.

<img src="/assets/img/lecture/textmining/11/image_8.png" alt="image" width="800px">

---

## p14. ë°ì´í„° ê°€ì¤‘ì¹˜ ì¡°ì •: êµ¬ì²´í™” (Data reweighting: instantiation)

- ìµœì¢…ì ìœ¼ë¡œ, ìš°ë¦¬ì˜ ëª©í‘œ(goal)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™”(formulated)ë  ìˆ˜ ìˆë‹¤.

  <img src="/assets/img/lecture/textmining/11/image_9.png" alt="image" width="640px">

  - ì´ì œ ìš°ë¦¬ê°€ í•´ì•¼ í•  ì¼ì€ **ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜(importance weight)** ë¥¼ **ì¶”ì •(estimate)** í•˜ëŠ” ê²ƒì´ë‹¤!

- **ë² ì´ì¦ˆ ê·œì¹™(Bayes rule)** ì„ ì‚¬ìš©í•˜ë©´, ê°€ì¤‘ì¹˜(weight)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤.

<img src="/assets/img/lecture/textmining/11/image_10.png" alt="image" width="800px">

---

## p15. ë°ì´í„° ê°€ì¤‘ì¹˜ ì¡°ì •: êµ¬ì²´í™” (Data reweighting: instantiation)

- ìµœì¢…ì ìœ¼ë¡œ, ìš°ë¦¬ì˜ ëª©í‘œ(goal)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™”ë  ìˆ˜ ìˆë‹¤.

  <img src="/assets/img/lecture/textmining/11/image_9.png" alt="image" width="640px">

  - ì´ì œ ìš°ë¦¬ê°€ í•´ì•¼ í•  ì¼ì€ **ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜(importance weight)** ë¥¼ **ì¶”ì •(estimate)** í•˜ëŠ” ê²ƒì´ë‹¤!

- ê°€ì¤‘ì¹˜ëŠ” **ë³„ë„ì˜ ë„ë©”ì¸ ë¶„ë¥˜ê¸°(separate domain classifier)** ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì •ëœë‹¤.  
  - ì¦‰, ìš°ë¦¬ëŠ” **ì†ŒìŠ¤(source)** ì™€ **íƒ€ê¹ƒ(target)** ìƒ˜í”Œì„ êµ¬ë¶„í•˜ê¸° ìœ„í•œ  
    **ì´ì§„ ë¶„ë¥˜ê¸°(binary classifier)** ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤.

<img src="/assets/img/lecture/textmining/11/image_12.png" alt="image" width="800px">

---

## p16. ë°ì´í„° ê°€ì¤‘ì¹˜ ì¡°ì •: ìš”ì•½ (Data reweighting: summary)

- ë„ë©”ì¸ ì ì‘(domain adaptation) ë¬¸ì œì— ëŒ€í•œ ê°„ë‹¨í•œ í•´ê²°ì±…:  
  (1) ê° ì†ŒìŠ¤ ìƒ˜í”Œ(source sample)ì´ íƒ€ê¹ƒ ë„ë©”ì¸(target domain)ì—  
  **ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ì¶”ì •(estimate)** í•˜ê³ ,  
  (2) ì´ë¥¼ **ì„ íƒì ìœ¼ë¡œ ë°˜ì˜(reflect)** í•œë‹¤.

- **ì ˆì°¨ (Procedure):**
  1. **ì´ì§„ ë¶„ë¥˜ê¸°(binary classifier)** ë¥¼ í•™ìŠµì‹œì¼œ ì†ŒìŠ¤ ë°ì´í„°ì™€ íƒ€ê¹ƒ ë°ì´í„°ë¥¼ êµ¬ë¶„í•œë‹¤.  
  2. ëª¨ë“  ì†ŒìŠ¤ ë°ì´í„° ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•´ **ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜(importance weight)** ë¥¼ ê³„ì‚°í•œë‹¤.  
  3. ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì†ì‹¤ í•¨ìˆ˜(weighted loss function)ë¥¼ ì‚¬ìš©í•˜ì—¬  
     **íƒ€ê¹ƒ ë¶„ë¥˜ê¸°(target classifier)** ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤.

<img src="/assets/img/lecture/textmining/11/image_13.png" alt="image" width="320px">

<img src="/assets/img/lecture/textmining/11/image_14.png" alt="image" width="300px">

---

## p17. ë°ì´í„° ê°€ì¤‘ì¹˜ ì¡°ì •: ìš”ì•½ (Data reweighting: summary)

- **ì œí•œì  (Limitation):**
  - ì—¬ì „íˆ, í•™ìŠµì—ëŠ” **ì†ŒìŠ¤ ë°ì´í„°(source data)** ë§Œ ì‚¬ìš©ëœë‹¤.  
  - íš¨ê³¼ì„±(effectiveness)ì€ **ì†ŒìŠ¤ì™€ íƒ€ê¹ƒ ë„ë©”ì¸ ê°„ì˜ ì¤‘ì²© ì •ë„(degree of overlap)** ì— í¬ê²Œ ì˜ì¡´í•œë‹¤.  
  - ë‹¤ì‹œ ë§í•´, **ë‘ ë„ë©”ì¸ì˜ íŠ¹ì„± ê³µê°„(feature spaces)** ì€ **ì˜ ì •ë ¬ë˜ì–´ì•¼ í•œë‹¤(well aligned).**

<img src="/assets/img/lecture/textmining/11/image_15.png" alt="image" width="800px">

---

## p18. ì ëŒ€ì  í•™ìŠµ (Adversarial learning)

**ë…¼ë¬¸:**  
*Domain-Adversarial Training of Neural Networks*  
*Journal of Machine Learning Research 17 (2016)*

**ì €ì (Authors):**  
Yaroslav Ganin, Evgeniya Ustinova  
Skolkovo Institute of Science and Technology (Skoltech)  
Skolkovo, Moscow Region, Russia  

Hana Ajakan, Pascal Germain  
DÃ©partement dâ€™informatique et de gÃ©nie logiciel, UniversitÃ© Laval  
QuÃ©bec, Canada, G1V 0A6  

Hugo Larochelle  
DÃ©partement dâ€™informatique, UniversitÃ© de Sherbrooke  
QuÃ©bec, Canada, J1K 2R1  

FranÃ§ois Laviolette, Mario Marchand  
DÃ©partement dâ€™informatique et de gÃ©nie logiciel, UniversitÃ© Laval  
QuÃ©bec, Canada, G1V 0A6  

Victor Lempitsky  
Skolkovo Institute of Science and Technology (Skoltech)  
Skolkovo, Moscow Region, Russia

---

## p19. ì ëŒ€ì  í•™ìŠµ: ë™ê¸° (Adversarial learning: motivation)

- ë”°ë¼ì„œ, ë‘ ë„ë©”ì¸ì˜ **íŠ¹ì§• ê³µê°„(feature spaces)** ì´ ì˜ ì •ë ¬(aligned)ë˜ì–´ ìˆë‹¤ë©´,  
  ìš°ë¦¬ëŠ” **ì¼ë°˜í™” ì„±ëŠ¥(generalization)** ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤!

<img src="/assets/img/lecture/textmining/11/image_16.png" alt="image" width="720px">

- (ì™¼ìª½)  
  - íŠ¹ì§•(feature)ë“¤ì´ **ì˜ ì •ë ¬ë˜ì§€ ì•ŠìŒ(poorly aligned)**  
  - ì†ŒìŠ¤ ë ˆì´ë¸”(source labels)ë¡œ í•™ìŠµëœ ëª¨ë¸ì€  
    **ì œí•œëœ ì¼ë°˜í™” ì„±ëŠ¥(limited generalization)** ë§Œì„ ê°€ì§  

- (ì˜¤ë¥¸ìª½)  
  - íŠ¹ì§•(feature)ë“¤ì´ **ë” ì˜ ì •ë ¬ë¨(more aligned)**  
  - ëª¨ë¸ì€ ë„ë©”ì¸ ê°„ì—ì„œ **ë” ë‚˜ì€ ì¼ë°˜í™”(generalize better)** ê°€ ê°€ëŠ¥í•¨  

- ìš°ë¦¬ëŠ” **ë‘ ë„ë©”ì¸ ê°„ì˜ íŠ¹ì§• ë¶„í¬(feature distributions)** ë¥¼  
  **ëª…ì‹œì ìœ¼ë¡œ ì •ë ¬í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆì„ê¹Œ?**

---

## p22. ì ëŒ€ì  í•™ìŠµ: ë™ê¸° (Adversarial learning: motivation)

- ì´ì œ **ì¤‘ê°„ ì¸µ(intermediate layer)** ì˜ **ì¶œë ¥(output)** ì— ì¸ì½”ë”©ëœ ì •ë³´ë¥¼ ìƒê°í•´ë³´ì.  
  - ì´ê²ƒë“¤ì€ **í•™ìŠµëœ íŠ¹ì§•(learned features)** ìœ¼ë¡œ, ì›ì‹œ ì…ë ¥ íŠ¹ì§•(raw input features)ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœë‹¤.  
  - ì´ë“¤ì€ ëª¨ë¸ì´ ì†ì‹¤(loss)ì„ ìµœì†Œí™”í•˜ë„ë¡ ë•ëŠ” ì •ë³´ë¥¼ ì¸ì½”ë”©í•œë‹¤.  
    (ì˜ˆ: ê°€ì§œì¸ì§€ ì•„ë‹Œì§€ êµ¬ë¶„)

<img src="/assets/img/lecture/textmining/11/image_17.png" alt="image" width="800px">

**Features** ëŠ” **ê°€ì§œ ë‰´ìŠ¤ì™€ ì§„ì§œ ë‰´ìŠ¤ë¥¼ êµ¬ë³„(distinguish fake and real news)** í•˜ëŠ” ì •ë³´ë¥¼ í¬í•¨í•œë‹¤.  
ì´ì œ ìš°ë¦¬ëŠ” ì´ **í•˜ìœ„ ê³„ì¸µ(bottom layer)** ì„ **íŠ¹ì§• ì¶”ì¶œê¸°(feature extractor)** ë¼ê³  ë¶€ë¥´ì.

---

## p23. ì ëŒ€ì  í•™ìŠµ: ë™ê¸° (Adversarial learning: motivation)

- ë§Œì•½ ìš°ë¦¬ê°€ **ë„ë©”ì¸ ë¶„ë¥˜ ê³¼ì œ(domain classification task)** ì™€ í•¨ê»˜  
  **ë‹¤ì¤‘ ê³¼ì œ í•™ìŠµ(multi-task learning)** ì„ ì ìš©í•œë‹¤ë©´ ì–´ë–¨ê¹Œ?

- **ê³¼ì œ 1 (Task 1):** ê°€ì§œ vs. ì§„ì§œ ë¶„ë¥˜ (ì£¼ìš” ê³¼ì œ, main task)  
- **ê³¼ì œ 2 (Task 2):** ì†ŒìŠ¤ vs. íƒ€ê¹ƒ ë¶„ë¥˜ (ë„ë©”ì¸ ê³¼ì œ, domain task)

<img src="/assets/img/lecture/textmining/11/image_18.png" alt="image" width="800px">

**íŠ¹ì§•(Features)** ì€  
**ê°€ì§œ ë‰´ìŠ¤(fake news)** ì™€ **ì§„ì§œ ë‰´ìŠ¤(real news)** ë¥¼ êµ¬ë¶„í•  ë¿ë§Œ ì•„ë‹ˆë¼,  
**ì†ŒìŠ¤(source)** ì™€ **íƒ€ê¹ƒ(target)** ë„ë©”ì¸ ì—­ì‹œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ í¬í•¨í•œë‹¤.

---

## p24. ì ëŒ€ì  í•™ìŠµ: ë™ê¸° (Adversarial learning: motivation)

- ë§Œì•½ ìš°ë¦¬ê°€ **ë„ë©”ì¸ ë¶„ë¥˜ ê³¼ì œ(domain classification task)** ì™€ í•¨ê»˜  
  **ë‹¤ì¤‘ ê³¼ì œ í•™ìŠµ(multi-task learning)** ì„ ì ìš©í•œë‹¤ë©´ ì–´ë–¨ê¹Œ?

<img src="/assets/img/lecture/textmining/11/image_19.png" alt="image" width="800px">

**íŠ¹ì§•(Features)** ì€  
**ê°€ì§œ ë‰´ìŠ¤(fake news)** ì™€ **ì§„ì§œ ë‰´ìŠ¤(real news)** ë¥¼ êµ¬ë¶„í•  ë¿ë§Œ ì•„ë‹ˆë¼,  
**ì†ŒìŠ¤(source)** ì™€ **íƒ€ê¹ƒ(target)** ë„ë©”ì¸ë„ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ í¬í•¨í•œë‹¤.

**(ì˜¤ë¥¸ìª½ ê·¸ë¦¼ ì„¤ëª…)**  
- â€œíŠ¹ì§•ë“¤ì€ ì†ŒìŠ¤ì™€ íƒ€ê¹ƒ ë„ë©”ì¸ì„ êµ¬ë¶„í•˜ëŠ” ì •ë³´ë¥¼ í¬í•¨í•œë‹¤.â€  
- ì´ëŠ” ë‘ ë„ë©”ì¸ì´ **íŠ¹ì§• ê³µê°„(feature space)** ì—ì„œ  
  êµ¬ë¶„ ê°€ëŠ¥í•´ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.  
- ì‹œê°í™”í•´ë³´ë©´, ë‘ ë„ë©”ì¸ì˜ **íŠ¹ì§• ë¶„í¬(feature distributions)** ê°€  
  **ë” ë¶„ë¦¬ë˜ì–´ ìˆìŒ(more separable)** ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

---

## p25. ì ëŒ€ì  í•™ìŠµ: ë™ê¸° (Adversarial learning: motivation)

- ë§Œì•½ ìš°ë¦¬ê°€ **ë„ë©”ì¸ ë¶„ë¥˜ ê³¼ì œ(domain classification task)** ì™€ í•¨ê»˜  
  **ë‹¤ì¤‘ ê³¼ì œ í•™ìŠµ(multi-task learning)** ì„ ì ìš©í•œë‹¤ë©´ ì–´ë–¨ê¹Œ?

<img src="/assets/img/lecture/textmining/11/image_20.png" alt="image" width="800px">

**ë§Œì•½ ë„ë©”ì¸ ë¶„ë¥˜ í—¤ë“œ(domain classification head)** ë¡œë¶€í„°ì˜  
**ê·¸ë˜ë””ì–¸íŠ¸(gradient)ì˜ ë¶€í˜¸(sign)ë¥¼ ë°˜ì „(reverse)** ì‹œí‚¨ë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œ?

- íŠ¹ì§• ì¶”ì¶œê¸°(feature extractor)ëŠ” **ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸** ëœë‹¤.  
  ë”°ë¼ì„œ ì—¬ì „íˆ **ê°€ì§œ ë‰´ìŠ¤ì™€ ì§„ì§œ ë‰´ìŠ¤ë¥¼ êµ¬ë³„(distinguish fake and real news)**  
  í•˜ëŠ” ì •ë³´ëŠ” ìœ ì§€í•˜ì§€ë§Œ,  
  **ì†ŒìŠ¤(source)** ì™€ **íƒ€ê¹ƒ(target)** ë„ë©”ì¸ì„ **êµ¬ë³„í•˜ì§€ ì•Šê²Œ(not to distinguish)** ëœë‹¤.

---

## p26. ì ëŒ€ì  í•™ìŠµ: ë™ê¸° (Adversarial learning: motivation)

- ë§Œì•½ ìš°ë¦¬ê°€ **ë„ë©”ì¸ ë¶„ë¥˜ ê³¼ì œ(domain classification task)** ì™€ í•¨ê»˜  
  **ë‹¤ì¤‘ ê³¼ì œ í•™ìŠµ(multi-task learning)** ì„ ì ìš©í•œë‹¤ë©´ ì–´ë–¨ê¹Œ?

<img src="/assets/img/lecture/textmining/11/image_21.png" alt="image" width="800px">

**ë§Œì•½ ë„ë©”ì¸ ë¶„ë¥˜ í—¤ë“œ(domain classification head)** ë¡œë¶€í„°ì˜  
**ê·¸ë˜ë””ì–¸íŠ¸(gradient)ì˜ ë¶€í˜¸(sign)** ë¥¼ **ë°˜ì „(reverse)** ì‹œí‚¨ë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œ?

- íŠ¹ì§• ì¶”ì¶œê¸°(feature extractor)ëŠ” **ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸** ëœë‹¤.  
  ë”°ë¼ì„œ ì—¬ì „íˆ **ê°€ì§œ ë‰´ìŠ¤(fake)** ì™€ **ì§„ì§œ ë‰´ìŠ¤(real)** ë¥¼ êµ¬ë³„í•˜ëŠ” ì •ë³´ëŠ” ìœ ì§€í•˜ì§€ë§Œ,  
  **ì†ŒìŠ¤(source)** ì™€ **íƒ€ê¹ƒ(target)** ë„ë©”ì¸ì„ **êµ¬ë³„í•˜ì§€ ì•Šê²Œ(not to distinguish)** ëœë‹¤.

**(ì˜¤ë¥¸ìª½ ê·¸ë¦¼ ì„¤ëª…)**  
- â€œíŠ¹ì§•ë“¤ì€ ì†ŒìŠ¤ì™€ íƒ€ê¹ƒ ë„ë©”ì¸ì„ êµ¬ë³„í•˜ì§€ ì•Šê²Œ(not to distinguish) í•˜ëŠ” ì •ë³´ë¥¼ í¬í•¨í•œë‹¤.â€  
- ì´ëŠ” ë‘ ë„ë©”ì¸ì´ **íŠ¹ì§• ê³µê°„(feature space)** ì—ì„œ **êµ¬ë³„ ë¶ˆê°€ëŠ¥í•´ì§(indistinguishable)** ì„ ì˜ë¯¸í•œë‹¤.  
- ì´ë¥¼ ì‹œê°í™”í•˜ë©´, ë‘ ë„ë©”ì¸ì˜ **íŠ¹ì§• ë¶„í¬(feature distributions)** ê°€  
  **ëœ ë¶„ë¦¬ë¨(less separable)** ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

---

## p27. ì ëŒ€ì  í•™ìŠµ: ê·¸ë˜ë””ì–¸íŠ¸ ë°˜ì „ (Adversarial learning: gradient reversal)

- ì´ ë©”ì»¤ë‹ˆì¦˜ì€ **ê·¸ë˜ë””ì–¸íŠ¸ ë°˜ì „ ì¸µ(Gradient Reversal Layer, GRL)** ì´ë¼ê³  ë¶ˆë¦°ë‹¤.  
- GRLì€ ë„ë©”ì¸ ë¶„ë¥˜ê¸°(domain classifier)ë¡œë¶€í„°ì˜ **ê·¸ë˜ë””ì–¸íŠ¸(gradient)** ë¥¼ **ë°˜ì „(reverse)** ì‹œì¼œ,  
  í•˜ìœ„ ê³„ì¸µ(bottom layers)ì´ **ë„ë©”ì¸ì„ êµ¬ë³„í•˜ì§€ ëª»í•˜ëŠ” íŠ¹ì§•(features that cannot distinguish domains)** â€”  
  ì¦‰, **ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§•(domain-invariant features)** ì„ ìƒì„±í•˜ë„ë¡ ê°•ì œí•œë‹¤.

<img src="/assets/img/lecture/textmining/11/image_22.png" alt="image" width="800px">

$\lambda$ ëŠ” **ì—­ì „íŒŒëœ ê·¸ë˜ë””ì–¸íŠ¸(reverse gradient)** ì˜ ì˜í–¥ì„ ì¡°ì ˆí•˜ëŠ” **í•˜ì´í¼íŒŒë¼ë¯¸í„°(hyperparameter)** ì´ë‹¤.

---

## p28. ì ëŒ€ì  í•™ìŠµ: ê·¸ë˜ë””ì–¸íŠ¸ ë°˜ì „ (Adversarial learning: gradient reversal)

- GRLì„ ì‚¬ìš©í•˜ë©´, í•™ìŠµ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™”ëœë‹¤:

$$
\min_{\theta_f, \theta_y} \max_{\theta_d} \mathcal{L}_y(\theta_f, \theta_y) - \lambda \mathcal{L}_d(\theta_f, \theta_d)
$$

$\lambda$ ëŠ” **ì—­ì „ëœ ê·¸ë˜ë””ì–¸íŠ¸(reverse gradient)** ì˜ ì˜í–¥ì„ ì¡°ì ˆí•˜ëŠ” **í•˜ì´í¼íŒŒë¼ë¯¸í„°(hyperparameter)** ì´ë‹¤.

1. $\mathcal{L}_y(\theta_f, \theta_y)$: **ë ˆì´ë¸” ë¶„ë¥˜ ì†ì‹¤(label classification loss)** â€”  
   ëª¨ë¸ì´ ì£¼ëœ ê³¼ì œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•˜ë„ë¡ ìœ ë„í•œë‹¤.  

2. $\mathcal{L}_d(\theta_f, \theta_d)$: **ë„ë©”ì¸ ë¶„ë¥˜ ì†ì‹¤(domain classification loss)** â€”  
   ì†ŒìŠ¤(source)ì™€ íƒ€ê¹ƒ(target) ë„ë©”ì¸ì„ êµ¬ë³„í•˜ë„ë¡ ì‹œë„í•œë‹¤.  

  - $\theta_f$ ëŠ” **ìŒì˜ $\mathcal{L}_d$ (ì¦‰, $\mathcal{L}_d$ ìµœëŒ€í™”)** ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ í•™ìŠµë˜ì–´  
    ë„ë©”ì¸ í˜¼ë™(domain confusion)ì„ ì´‰ì§„í•œë‹¤.  

  - $\theta_d$ ëŠ” **ìŒì˜ $\mathcal{L}_d$ (ì¦‰, $\mathcal{L}_d$ ìµœì†Œí™”)** ë¥¼ ìµœëŒ€í™”í•˜ë„ë¡ í•™ìŠµë˜ì–´  
    ë„ë©”ì¸ ë¶„ë¥˜(domain classification)ë¥¼ ì´‰ì§„í•œë‹¤.  

<img src="/assets/img/lecture/textmining/11/image_23.png" alt="image" width="800px">

---

## p29. ì ëŒ€ì  í•™ìŠµ: ê·¸ë˜ë””ì–¸íŠ¸ ë°˜ì „ (Adversarial learning: gradient reversal)

- GRLì„ ì‚¬ìš©í•˜ë©´, í•™ìŠµ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™”ëœë‹¤:

$$
\min_{\theta_f, \theta_y} \max_{\theta_d} \mathcal{L}_y(\theta_f, \theta_y) - \lambda \mathcal{L}_d(\theta_f, \theta_d)
$$

$\lambda$ ëŠ” **ì—­ì „ëœ ê·¸ë˜ë””ì–¸íŠ¸(reverse gradient)** ì˜ ì˜í–¥ì„ ì¡°ì ˆí•˜ëŠ” **í•˜ì´í¼íŒŒë¼ë¯¸í„°(hyperparameter)** ì´ë‹¤.

1. $\mathcal{L}_y(\theta_f, \theta_y)$: **ë ˆì´ë¸” ë¶„ë¥˜ ì†ì‹¤(label classification loss)** â€”  
   ëª¨ë¸ì´ ì£¼ëœ ê³¼ì œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•˜ë„ë¡ ìœ ë„í•œë‹¤.  

2. $\mathcal{L}_d(\theta_f, \theta_d)$: **ë„ë©”ì¸ ë¶„ë¥˜ ì†ì‹¤(domain classification loss)** â€”  
   ì†ŒìŠ¤(source)ì™€ íƒ€ê¹ƒ(target) ë„ë©”ì¸ì„ êµ¬ë³„í•˜ë ¤ê³  ì‹œë„í•œë‹¤.  

- $\theta_f$ ëŠ” **ìŒì˜ $\mathcal{L}_d$ (ì¦‰, $\mathcal{L}_d$ ìµœëŒ€í™”)** ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ í•™ìŠµë˜ì–´  
  ë„ë©”ì¸ í˜¼ë™(domain confusion)ì„ ì´‰ì§„í•œë‹¤.  

- $\theta_d$ ëŠ” **ìŒì˜ $\mathcal{L}_d$ (ì¦‰, $\mathcal{L}_d$ ìµœì†Œí™”)** ë¥¼ ìµœëŒ€í™”í•˜ë„ë¡ í•™ìŠµë˜ì–´  
  ë„ë©”ì¸ ë¶„ë¥˜(domain classification)ë¥¼ ì´‰ì§„í•œë‹¤.  

âœ“ **íŠ¹ì§• ì¶”ì¶œê¸°(feature extractor)** ì™€ **ë„ë©”ì¸ ë¶„ë¥˜ê¸°(domain classifier)** ëŠ”  
  **ì ëŒ€ì  ê²Œì„(adversarial game)** ì„ ìˆ˜í–‰í•œë‹¤ â€”  
  í•˜ë‚˜ëŠ” **í˜¼ë€(confuse)** ì„ ì¼ìœ¼í‚¤ë ¤ í•˜ê³ ,  
  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” **ë„ë©”ì¸ì„ êµ¬ë³„(discriminate)** í•˜ë ¤ í•œë‹¤.

---

## p30. ì ëŒ€ì  í•™ìŠµ: ê·¸ë˜ë””ì–¸íŠ¸ ë°˜ì „ (Adversarial learning: gradient reversal)

- GRLì„ ì‚¬ìš©í•˜ë©´, í•™ìŠµ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™”ëœë‹¤:

$$
\min_{\theta_f, \theta_y} \max_{\theta_d} \mathcal{L}_y(\theta_f, \theta_y) - \lambda \mathcal{L}_d(\theta_f, \theta_d)
$$

$\lambda$ ëŠ” **ì—­ì „ëœ ê·¸ë˜ë””ì–¸íŠ¸(reverse gradient)** ì˜ ì˜í–¥ì„ ì¡°ì ˆí•˜ëŠ” **í•˜ì´í¼íŒŒë¼ë¯¸í„°(hyperparameter)** ì´ë‹¤.

---

ğŸ’» **ì•Œê³ ë¦¬ì¦˜: GRLì„ ì´ìš©í•œ SGD**

1. $\theta_f, \theta_y, \theta_d$ ë¥¼ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”í•œë‹¤.  
2. **ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤:**  
   - ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•œë‹¤: $(x^S, y^S) \in D_S$ ê·¸ë¦¬ê³  $x^T \in D_S$  
   - $\mathcal{L}_y(\theta_f, \theta_y)$ ì™€ $\mathcal{L}_d(\theta_f, \theta_d)$ ë¥¼ ê³„ì‚°í•œë‹¤.  
   - íŒŒë¼ë¯¸í„°ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°±ì‹ í•œë‹¤:  

$$
\theta_y \leftarrow \theta_y - \eta \frac{\partial \mathcal{L}_y}{\partial \theta_y}
$$

$$
\theta_d \leftarrow \theta_d - \eta \frac{\partial \mathcal{L}_d}{\partial \theta_d}
$$

$$
\theta_f \leftarrow \theta_f - \eta \left( \frac{\partial \mathcal{L}_y}{\partial \theta_f} - \lambda \frac{\partial \mathcal{L}_d}{\partial \theta_f} \right)
$$

---

$\eta$: ë‹¨ê³„ í¬ê¸°(step size) ë˜ëŠ” í•™ìŠµë¥ (learning rate)  
$\lambda$: ì—­ì „ëœ ê·¸ë˜ë””ì–¸íŠ¸ì˜ ì˜í–¥ì„ ì¡°ì ˆí•œë‹¤.

---

## p31. ì ëŒ€ì  í•™ìŠµ: ê·¸ë˜ë””ì–¸íŠ¸ ë°˜ì „ (Adversarial learning: gradient reversal)

- ì—¬ëŸ¬ë¶„ì€ ì´ë ‡ê²Œ ê¶ê¸ˆí•´í•  ìˆ˜ ìˆë‹¤ â€”  
  **â€œì™œ ë‹¨ìˆœíˆ ë„ë©”ì¸ ë¶„ë¥˜ ì†ì‹¤(domain classification loss)ì„ ìµœëŒ€í™”í•˜ì§€ ì•ŠëŠ”ê°€?â€**

- ë§Œì•½ ìš°ë¦¬ê°€ ë‹¨ìˆœíˆ ë„ë©”ì¸ ì†ì‹¤ì„ **ìµœëŒ€í™”**í•œë‹¤ë©´,  
  **ì „ì²´ í•™ìŠµ ê³¼ì •ì´ ë¶•ê´´ëœë‹¤(the whole training collapse).**

  - ê°€ì¥ ì¤‘ìš”í•œ ì ì€, $\theta_d$ ê°€ **ì˜ëª»ëœ ë°©í–¥(wrong direction)** ìœ¼ë¡œ ì´ë™í•˜ì—¬  
    ë¹ ë¥´ê²Œ **ë¬´ì˜ë¯¸í•´ì§„ë‹¤(useless).**
  - ì´ë•Œ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ëŠ” íŠ¹ì§• ì¶”ì¶œê¸°ì— ìœ ì˜ë¯¸í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì „ë‹¬í•˜ì§€ ëª»í•œë‹¤.
  - ê²°ê³¼ì ìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œê¸°ëŠ” **ì •ë³´ê°€ ì—†ëŠ” ê·¸ë˜ë””ì–¸íŠ¸(uninformative gradients)** ë§Œ ë°›ì•„  
    **íŠ¹ì§• ë¶•ê´´(feature collapse)** ê°€ ë°œìƒí•œë‹¤.

<img src="/assets/img/lecture/textmining/11/image_24.png" alt="image" width="800px">

- ë§Œì•½ íŠ¹ì§• ì¶”ì¶œê¸°(feature extractor)ê°€ **ëª¨ë“  0ì¸ íŠ¹ì§•(all-zero features)** ì„ ì¶œë ¥í•œë‹¤ë©´,  
  ë„ë©”ì¸ ë¶„ë¥˜ê¸°(domain classifier)ëŠ” **ë„ë©”ì¸ì„ ì „í˜€ êµ¬ë³„í•  ìˆ˜ ì—†ë‹¤(cannot distinguish domains at all)!**

- ì´ëŠ” ë„ë©”ì¸ ë¶„ë¥˜ ì†ì‹¤(domain classification loss)ì„ ì™„ë²½í•˜ê²Œ ìµœëŒ€í™”í•˜ì§€ë§Œ,  
  **ì£¼ìš” ê³¼ì œ(main task)** ì— í•„ìš”í•œ **ëª¨ë“  ìœ ìš©í•œ ì •ë³´(all useful information)** ë¥¼ íŒŒê´´í•œë‹¤.

âœ“ **ì ëŒ€ì  í•™ìŠµ(adversarial learning)** ì€  
  ìœ ìš©í•œ ì •ë³´ë¥¼ ìƒì§€ ì•Šìœ¼ë©´ì„œë„ **ë„ë©”ì¸ í˜¼ë™(domain confusion)** ì„ ë‹¬ì„±í•œë‹¤.

---

## p32. ì ëŒ€ì  í•™ìŠµ: íŠ¹ì§• ì •ë ¬ (Adversarial learning: feature alignment)

- **ì ëŒ€ì  í•™ìŠµ(adversarial learning)** ì€  
  íŠ¹ì§• ì¶”ì¶œê¸°(feature extractor)ê°€ **ë„ë©”ì¸ ë¶ˆë³€ í‘œí˜„(domain-invariant representations)** ì„  
  ìƒì„±í•˜ë„ë¡ ìœ ë„í•œë‹¤.

  - **ì ëŒ€ì  í•™ìŠµ(GRL)** ì„ ì‚¬ìš©í•˜ë©´,  
    ë‘ ë„ë©”ì¸ì€ **íŠ¹ì§• ê³µê°„(feature space)** ì—ì„œ ì˜ ì •ë ¬(well aligned)ë˜ì–´,  
    ëª¨ë¸ì´ ë³´ì§€ ëª»í•œ íƒ€ê¹ƒ ë°ì´í„°(unseen target data)ì— ëŒ€í•´  
    ë” ì˜ ì¼ë°˜í™”(generalize)í•  ìˆ˜ ìˆê²Œ ëœë‹¤.

<img src="/assets/img/lecture/textmining/11/image_25.png" alt="image" width="800px">

---

## p33. ì ëŒ€ì  í•™ìŠµ: ì‘ìš© (ê°€ì§œ ë‰´ìŠ¤ íƒì§€)  
*(Adversarial learning: applications â€” fake news detection)*

- ê°€ì§œ ë‰´ìŠ¤ íƒì§€ ì‹œìŠ¤í…œì„ ìƒê°í•´ë³´ì.  
  ìƒˆë¡œìš´ ì‚¬ê±´(new events)ì€ ê³„ì†í•´ì„œ ë°œìƒí•œë‹¤!

- **ë¬¸ì œì :** ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ì€  
  ìƒˆë¡œìš´ ì‚¬ê±´(new events)ì— **ì¼ë°˜í™”(generalize)** í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤.

<img src="/assets/img/lecture/textmining/11/image_26.png" alt="image" width="800px">

---

## p34. ì ëŒ€ì  í•™ìŠµ: ì‘ìš© (ê°€ì§œ ë‰´ìŠ¤ íƒì§€)  
*(Adversarial learning: applications â€” fake news detection)*

- **ë¬¸ì œ(Problem):**  
  ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ì€  
  ìƒˆë¡œìš´ ì‚¬ê±´(new events)ì— **ì¼ë°˜í™”(generalize)** í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤.

- **í•´ê²°ì±…(Solution):**  
  ì ëŒ€ì  í•™ìŠµ(adversarial learning)ì„ í†µí•´  
  ì‚¬ê±´ ê°„ì— êµ¬ë³„ë˜ì§€ ì•ŠëŠ”(event-invariant) íŠ¹ì§•ë“¤ì„ í•™ìŠµí•œë‹¤.  
  (ì¦‰, ì‚¬ê±´ë“¤ì´ êµ¬ë³„ ë¶ˆê°€ëŠ¥í•˜ê²Œ(indistinguishable) ëœë‹¤.)

<img src="/assets/img/lecture/textmining/11/image_27.png" alt="image" width="800px">

**ì°¸ê³ (Note):**  
ìƒˆë¡œìš´ ì‚¬ê±´(new events)ì— ëŒ€í•´ **ê°€ì§œ/ì§„ì§œ ë ˆì´ë¸”(fake/real labels)** ì€ í•„ìš”í•˜ì§€ ì•Šë‹¤.  
ì´ë¯¸ ì•Œë ¤ì§„ **ì‚¬ê±´ ìœ í˜•(event type)** ë§Œì„ ì‚¬ìš©í•˜ì—¬  
**ì‚¬ê±´ ë¶ˆë³€ í‘œí˜„(event-invariant representations)** ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•œë‹¤.

---

## p35. ì ëŒ€ì  í•™ìŠµ: ì‘ìš© (ê°€ì§œ ë‰´ìŠ¤ íƒì§€)  
*(Adversarial learning: applications â€” fake news detection)*

- **í•´ê²°ì±…(Solution):**  
  ì ëŒ€ì  í•™ìŠµ(adversarial learning)ì„ í†µí•´  
  ì‚¬ê±´ ê°„ êµ¬ë³„ ë¶ˆê°€ëŠ¥(event-invariant)í•œ íŠ¹ì§•ë“¤ì„ í•™ìŠµí•œë‹¤.  
  (ì¦‰, ì‚¬ê±´ë“¤ì´ ì„œë¡œ êµ¬ë³„ë˜ì§€ ì•Šê²Œ(indistinguishable) ëœë‹¤.)

<img src="/assets/img/lecture/textmining/11/image_28.png" alt="image" width="600px">

*EANN: Event Adversarial Neural Networks for Multi-Modal Fake News Detection, KDDâ€™18*

---

## p36. ì ëŒ€ì  í•™ìŠµ: ìš”ì•½  
*(Adversarial learning: summary)*

- ì ëŒ€ì  í•™ìŠµ(adversarial learning)ì€  
  ë„ë©”ì¸ ê°„ íŠ¹ì§•ì„ ì •ë ¬(alignment)í•¨ìœ¼ë¡œì¨  
  **ë„ë©”ì¸ ë¶ˆë³€ í‘œí˜„(domain-invariant representations)** ì„ í•™ìŠµí•œë‹¤.  
  - ì´ëŠ” **ë ˆì´ë¸”ì´ ìˆëŠ”(labeled)** ë°ì´í„°ì™€ **ë ˆì´ë¸”ì´ ì—†ëŠ”(unlabeled)** ë°ì´í„° ë¶„í¬ ê°„  
    **ì¼ë°˜í™”(generalization)** ë¥¼ ì´‰ì§„í•œë‹¤.

- **ì¥ì (Pros):**
  - **ë‹¨ìˆœí•˜ê³  íš¨ê³¼ì (Simple and effective)** â€”  
    GRL(Gradient Reversal Layer)ì„ í†µí•´ ê¸°ì¡´ ë„¤íŠ¸ì›Œí¬ì— ì‰½ê²Œ í†µí•© ê°€ëŠ¥í•˜ë‹¤.
  - ë°ì´í„° ì¬ê°€ì¤‘(data reweighting)ì— ë¹„í•´  
    **ì†ŒìŠ¤â€“íƒ€ê¹ƒ ê°„ ì¤‘ì²©(sourceâ€“target overlap)** ì— ëœ ì˜ì¡´í•œë‹¤.  
    - ì‚¬ì „ ì¡´ì¬í•˜ëŠ” ì¤‘ì²©(pre-existing overlap)ì— ì˜ì¡´í•˜ì§€ ì•Šê³ ,  
      **íŠ¹ì§• ë¶„í¬(feature distributions)** ë¥¼ ì •ë ¬í•˜ë„ë¡ í•™ìŠµí•œë‹¤.

- **ë‹¨ì (Cons):**
  - **ì „ì—­ ì •ë ¬(Global alignment):**  
    í´ë˜ìŠ¤ êµ¬ì¡°(class structure)ë¥¼ ë¬´ì‹œí•˜ê³  ë„ë©”ì¸ì„ ì „ì—­ì ìœ¼ë¡œ ì •ë ¬í•œë‹¤.  
    - ì´ëŠ” **í´ë˜ìŠ¤ ë¶ˆì¼ì¹˜(class misalignment)** ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆë‹¤.  
      (ì„œë¡œ ë‹¤ë¥¸ í´ë˜ìŠ¤ê°€ ì„ì´ê²Œ ë¨)  

<img src="/assets/img/lecture/textmining/11/image_29.png" alt="image" width="720px">

- **ì£¼ì˜:**  
  ì •ë ¬ í›„ì—ë„ í´ë˜ìŠ¤ë“¤ì´ ì˜ ë¶„ë¦¬ëœ ìƒíƒœë¡œ ë‚¨ëŠ”ë‹¤ëŠ”  
  **ë³´ì¥ì€ ì—†ë‹¤. (No guarantee that classes remain well separated after alignment.)**

*(Figure credit):*  
*Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation, CVPRâ€™21*

---

## p37. ì œí•œëœ ë ˆì´ë¸”ë¡œ í•™ìŠµí•˜ê¸°: ìš”ì•½  
*(Learning with limited labels: summary)*

<img src="/assets/img/lecture/textmining/9/image_1.png" alt="image" width="360px">

- ìš°ë¦¬ëŠ” ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°ì´í„°ì˜ ë¶€ì¡±(scarcity of labeled data)ì„  
  ì–´ë–»ê²Œ ë‹¤ë£° ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì‚´í´ë³´ì•˜ë‹¤.

1. **ë°˜ì§€ë„ í•™ìŠµ (Semi-supervised learning)**  
   - â€œ**ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°(unlabeled data)** ë¥¼ ì–´ë–»ê²Œ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©(leverage)í•  ìˆ˜ ìˆì„ê¹Œ?â€  
   - Pseudo-label, Self-training, Consistency regularization, Temporal ensemble

2. **ë‹¤ì¤‘ ê³¼ì œ í•™ìŠµ (Multi-task learning)**  
   - â€œë§Œì•½ í•œ ê³¼ì œ(task)ì— ì¶©ë¶„í•œ ë ˆì´ë¸”ì´ ì—†ë‹¤ë©´,  
      **ê´€ë ¨ëœ ê³¼ì œ(related tasks)** ë¡œë¶€í„° ì‹ í˜¸ë¥¼ ë¹Œë ¤ì˜¬ ìˆ˜ ìˆì„ê¹Œ?â€  
   - MMoE, GradNorm

3. **ì ëŒ€ì  í•™ìŠµ (Adversarial learning)**  
   - â€œë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ì™€ ì—†ëŠ” ë°ì´í„°ê°€  
      **ì„œë¡œ ë‹¤ë¥¸ ë¶„í¬(different distributions)** ì—ì„œ ì˜¨ë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œ?â€  
   - Data reweighting, GRL (Gradient Reversal Layer)

---

## p38. ì¶”ì²œ ì½ê¸° ìë£Œ  
*(Recommended readings)*

- **ë…¼ë¬¸(Papers):**
  - *Domain-Adversarial Training of Neural Networks*, JMLRâ€™16  
  - *EANN: Event Adversarial Neural Networks for Multi-Modal Fake News Detection*, KDDâ€™18