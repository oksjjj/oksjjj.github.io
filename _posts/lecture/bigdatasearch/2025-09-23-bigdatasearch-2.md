---
layout: post
title: "[빅데이터와 정보검색] 2주차 검색개요 - 색인과 검색랭킹 모델(part1)"
date: 2025-09-23 17:00:00 +0900
categories:
  - "대학원 수업"
  - "빅데이터와 정보검색"
tags: []
---

> 출처: 빅데이터와 정보검색 – 황영숙 교수님, 고려대학교 (2025)

## p2. 정보 검색 개요

<img src="/assets/img/lecture/bigdatasearch/2/image_1.png" alt="image" width="540px">


1. User task (사용자 과제)  
   - 사용자가 실제로 수행하려는 작업  
   - 예: 리포트 작성, 문제 해결, 특정 사실 확인  

2. Info need (정보 요구)  
   - 사용자가 과제를 수행하는 과정에서 느끼는 정보의 필요성  
   - 내적이고 추상적인 개념이며, 명확히 표현되지 않을 수도 있음  

3. Query (질의)  
   - 정보 요구를 표현한 구체적 형태  
   - 검색 엔진에 입력하는 키워드나 문장  
   - Info need(정보 요구) ↔ Query(질의) 구분 필요  

4. Search engine (검색 엔진)  
   - Query를 기반으로 Collection(문서 집합)에서 관련 문서를 검색  
   - 검색 알고리즘에 따라 결과가 달라짐  
   - "찾는 방법"에 해당  

5. Results (검색 결과)  
   - 검색 엔진이 반환한 문서 집합  
   - 사용자가 결과를 보고 만족도를 평가  

6. Query refinement (질의 개선)  
   - 검색 결과가 만족스럽지 않을 경우 질의를 수정하거나 새로운 키워드 추가  
   - "얼마나 잘 찾았는지 보고 나서 수정" 과정  
   - Search engine → Results 과정을 반복  

7. Collection (문서 집합)  
   - 검색 대상이 되는 전체 데이터베이스  
   - 예: 웹 문서, 논문, 뉴스 기사 등  

---

✅ 정리  
User task → Info need → Query → Search engine → Results → Query refinement  
이러한 순환 과정을 통해 사용자의 정보 요구가 충족된다.

---

## p3. 정보 검색 개요

**정보 검색의 핵심 구성요소**

- **색인(Indexing)** : 수많은 문서를 효율적으로 검색할 수 있도록 구조화 하는 과정  
    - **전처리**  
      - **토큰화(Tokenization)** : 문서를 단어 단위로 쪼개는 과정  
      - **불용어 제거(Stop Word Removal)** : '은', '는', '이', '가', 'the', 'a' 등 의미없는 단어 제거  
      - **어간추출(Stemming)/표제어 추출(Lemmatization)** : 단어의 원형 복원(예: runs→run, running→run, 먹었다→먹다)  
    - **키워드 역색인(Inverted Index)** : 각 토큰이 어느 문서에서 출현했는지 기록하는 자료구조  

- **질의(Query)** : 사용자가 찾는 정보의 표현  
    - **키워드 질의**  
    - **자연어 질의** : '인공지능의 최신 연구동향에 대한 논문을 찾아줘'와 같은 자연어 질문  

- **랭킹(Ranking)** : 문서와 질의간 유사도를 계산하여 질의와 관련된 문서를 중요도 순으로 나열하는 과정  
    - **TF-IDF** : 토큰 빈도(문서 내 특정 단어의 출현 빈도)와 역문서 빈도(전체 문서 중 특정 단어가 포함된 문서의 수)를 사용하여 문서 내 용어의 중요도를 평가하는 가장 기본적인 방법  
    - **BM25** : TF-IDF를 개선한 확률적 모델로, 많은 검색 엔진에서 기본 랭킹 알고리즘으로 사용  
    - **벡터공간 모델(VectorSpaceModel)** : 문서와 질의를 고차원 벡터로 표현하고, 두 벡터 간의 코사인 유사도(Cosine Similarity)를 측정하여 랭킹. 최근 워드임베딩, BERT와 같은 딥러닝 기반의 언어 모델을 많이 사용

---

## p4. 정보 검색 개요

- **정확성(Relevance)**: 검색된 문서가 사용자의 질의와 얼마나 관련 있는지를 판단하는 주관적인 기준으로 전문가 그룹이 판단  

- **정밀도(Precision)**: 검색된 문서 중 실제로 관련 있는 문서의 비율  

$$
정밀도 = \frac{|\{관련\ 있는\ 문서들\} \cap \{검색된\ 문서들\}|}{|\{검색된\ 문서들\}|}
$$  
 

- **재현율(Recall)**: 전체 관련 문서 중 실제로 검색된 문서의 비율  

$$
재현율 = \frac{|\{관련\ 있는\ 문서들\} \cap \{검색된\ 문서들\}|}{|\{괸련\ 있는\ 문서들\}|}
$$  

• **F1-점수(F1-score)**: 정밀도와 재현율의 조화 평균  

$$
F1 = 2 \times \frac{정밀도 \times 재현율}{정밀도 + 재현율}
$$  

- 그 외 지표  

  - **MAP(Mean Average Precision)**: 질의에 대한 평균 정밀도를 여러 질의에 대해 평균낸 값  

  - **nDCG(normalized Discounted Cumulative Gain)**: 랭킹 순서를 고려하여 상위 랭크의 중요도를 높게 평가하는 지표

---

> - 쉽게 얘기해서  
> - **정밀도** 는 **검색된 문서들** 기준으로, 그 안에 **관련 있는 문서들** 이 얼마나 포함되어 있느냐? 
> - **재현율** 은 **관련 있는 문서들** 기준으로, 그 중에서 얼마나 실제로 **검색** 되었느냐?
> - 를 평가하는 것

---

## p5. 인터넷 정보검색 시스템

- **인터넷에 있는 대량의 정보를 수집하여 사용자의 입맛에 맞게 검색해주는 것**

- **검색엔진에게 정보란?**  
    - **관심이 있는 문서의 집합**  
    - **문서란?**  
        - **종류** : 신문기사, 게시판 글, 웹 문서, 상품정보  
        - **문서의 내용** : 제목, 본문(상품명, 상품설명)  
        - **문서의 메타정보** : 작성자, 작성일자(가격, 인기도)  
        - **문서의 형식** : 언어(한글, 영어 …), 파일형식  
        - **문서의 저장형태** : DB, 웹 문서, 파일서버  

- **검색**  
    - **정의** : 관심이 있는 문서들을 획득하기 위하여 하는 행동  
    - **단순한 질문** : ‘정보검색’이랑 관련(?) 있는 문서는?  
    - **복잡한 질문** : 문서의 내용에 ‘정보검색’이라는 단어가 들어가는 문서 중에 ‘홍길동’이 ‘최근 1주일’ 이내에 작성한 문서는?  
    - **비교** : 단순한 질문 vs. 복잡한 질문

---

## p6. 인터넷 정보검색 시스템

- **엄청난 양의 정보**  
    - **2025년 웹 데이터 총량?**  
        - 전 세계적으로 웹에 존재하는 데이터는 181~182 ZB  
    - **하루 또는 매월 생성되는 웹 데이터?**  
        - 약 0.4 ZB /일, 약 12 ZB /월  
    - **빅데이터**  
        - 구글, 페이스북, 아마존 같은 글로벌 기업들은 PB 또는 EB 단위의 데이터를 저장  
    - **미래기술**  
        - 인공지능(AI), 사물 인터넷(IoT), 자율 주행차 등은 막대한 양의 데이터를 끊임없이 생성하고 처리  
    - **참고) 데이터 단위**  
        • 1 MB = 1,000 KB  
        • 1 GB = 1,000 MB  
        • 1 TB = 1,000 GB  
        • 1 PB = 1,000 TB  
        • 1 EB = 1,000 PB  
        • **1 ZB = 1,000 EB**  

- **정확한 검색의 어려움**  
    - **Efficiency is not a bottleneck**  
    - **Effectiveness is bottleneck**  
        - 낮은 **정확률(Precision)**  
        - 낮은 **Ranking 성능**  

---

> - “전 세계적으로 웹에 존재하는 데이터는 181~182 ZB”는 *2025년 한 해 동안 생성될(data generated) 데이터의 양(projection)*을 말하는 것이다. (참고: Exploding Topics – Data Generated Per Day, IDC Data Growth Report)  
> - “‘하루 또는 매월 생성되는 웹 데이터: 약 0.4 ZB/일, 약 12 ZB/월’” 또한 생성량(generation) 정보이며, 축적(total stored or existing data)된 전체 웹 데이터량과는 동일한 개념이 아니다. (참고: Exploding Topics – Data Generated Per Day)  
> - 요약하면, 두 수치는 “미래 생성될 연간/월간 데이터 양” vs “현재 존재 또는 누적된 웹 전체 데이터 저장량”을 혼동한 것이므로, 장표에서는 ‘2025년에 생성될 양’이라는 표현으로 수정하는 것이 정확하다.

---

## p7. 인터넷 정보검색 시스템  
  
**색인**

- **색인(Indexing)** : 수많은 문서를 효율적으로 검색할 수 있도록 구조화 하는 과정  
    - **문서수집**  
        - 웹서버, DB서버, 파일서버로부터 문서를 수집  
    - **문서전처리**  
        - 문서의 형식에 따라 문서로부터 내용과 메타정보를 구분하여 텍스트 형태로 추출  
        - **토큰화(Tokenization)** : 문서를 단어 단위로 쪼개는 과정  
        - **불용어 제거(Stop Word Removal)** : '은', '는', '이', '가', 'the', 'a' 등 의미없는 단어 제거  
        - **어간추출(Stemming)/표제어 추출(Lemmatization)** : 단어의 원형 복원  
          (예: runs → run, running → run, 먹었다 → 먹다)  
    - **문서저장**  
        - 색인 및 검색 시 해당 문서의 정보를 바로 제공하기 위해 서버에 특정 형태로 저장  
    - **역파일 구조(Inverted Index) 생성**  
        - 검색어가 들어왔을 때 해당 검색어가 포함된 문서를 빠르게 찾아주기 위해  
          각 검색어 토큰이 들어 있는 문서의 리스트를 특정한 자료구조로 구축

---

## p8. 검색

- **역 파일 구조 탐색**  
    - 검색키워드를 포함하는 문서의 집합을 역파일 구조로부터 가져온다.  

- **필터링(filtering)**  
    - 검색키워드를 포함하는 문서들 중에서 검색 조건에 부합되는지 여부를 판단한다.  

- **정렬(sorting)**  
    - 최종 선택된 문서들을 정확도 또는 특정 정렬조건에 따라 재정렬한다.  
    - 기본적으로는 색인된 순서(최근 색인된 문서 우선)로 정렬되어 있다.  

- **그룹핑(grouping)**  
    - 검색결과 문서들을 메타정보에 따라 그룹화 한다.

---

## p9. 검색

- **불리언 연산(boolean)** : AND, OR, NOT  

- **자연어 질의** : 형태소 분석, 바이그램  

- **근접 연산** : Near, Order  

- **유사어 확장**  
    - 동의어(상하위어) 확장 : 방향성 고려 필요  
    - 철자 오류 보정  
    - 다국어 확장  
    - 구문 확장  

- **필드 지정**  
    - 필드의 특성에 따른 추출방법 적용 필요  
    - 필드 간 중요도 차등 적용 필요  

- **질의 보관**  
    - 질의로그 : 사용자의 의도 파악

> - 불리언(Boolean) 검색은 정보 검색에서 가장 기본적인 질의 방식으로, 논리 연산자 AND, OR, NOT을 이용한다.  
> - **AND** : 두 검색어를 모두 포함하는 문서를 검색 (교집합)  
>   - 예: `정보 AND 검색` → 두 단어가 모두 포함된 문서만 검색  
> - **OR** : 두 검색어 중 하나라도 포함하는 문서를 검색 (합집합)  
>   - 예: `정보 OR 데이터` → 두 단어 중 하나라도 포함된 문서를 검색  
> - **NOT** : 특정 검색어가 포함된 문서를 제외 (차집합)  
>   - 예: `검색 NOT 광고` → ‘검색’은 포함하지만 ‘광고’는 포함하지 않는 문서 검색  
> - 불리언 연산은 단순하고 직관적이지만, 검색 결과가 지나치게 많거나 적을 수 있어 **정밀도(precision)**와 **재현율(recall)**의 균형 조절이 어려운 단점이 있다.  

---

## p10. 랭킹

- **Boolean**  
    - 해당 키워드가 나오면 1, 나오지 않으면 0  
    - Boolean Queries 연산자: AND, OR, NOT  
    - 문서에 나타나는 검색키워드의 가지수(등장 여부)가 해당 문서의 가중치를 결정  

- **TF*IDF**  
    - **TF** : 문서에 해당 키워드가 몇 번 나오는가?  
    - **IDF** : 1/DF (DF = 해당 키워드가 몇 개의 문서에 나오는가?)  
    - 문서에 나타나는 검색 키워드의 TF*IDF 값의 합이 해당 문서의 가중치를 결정  

- **가중치 응용**  
    - 카테고리 정보, 문서의 길이 정보 등 추가적인 정보 적용  
    - 두 개 이상의 검색필드에 검색 시 필드 별 가중치의 합산

---

### 보충 설명

#### 1. 검색에서 랭킹이 필요한 이유  
- 검색 시스템은 사용자의 질의와 관련된 문서를 다수 찾아낼 수 있다.  
- 하지만 모든 결과를 단순 나열하면 사용자가 원하는 정보를 빠르게 찾기 어렵다.  
- 따라서 검색된 결과를 **관련성이 높은 순서대로 정렬(랭킹)** 하여 보여주는 과정이 필수적이다.  
- 이는 사용자의 검색 요구에 더 적합한 문서를 상위에 배치해 정보 접근 효율을 높인다.  

#### 2. 본문에 명기된 랭킹 방법의 장단점  
- **Boolean 검색**  
  - 장점: 단순하고 직관적이며 조건을 엄격히 만족하는 문서 검색 가능  
  - 단점: 관련성의 강약을 구분하지 못하고, 결과가 너무 많거나 적을 수 있음  
- **TF*IDF**  
  - 장점: 단어 빈도와 희소성을 동시에 고려해 문서 중요도를 수치화할 수 있음  
  - 단점: 단어 의미나 맥락을 반영하지 못하며, 문서 길이에 따른 편향이 발생할 수 있음  

#### 3. 가중치 응용의 의미  
- 랭킹 계산 시 키워드의 TF*IDF 값뿐 아니라 문서의 추가 속성을 반영하는 방식이다.  
- 예: 문서 카테고리(뉴스, 블로그, 논문), 문서 길이, 제목·본문·태그 등 필드별 중요도를 다르게 적용  
- 두 개 이상의 필드에 걸쳐 검색할 때, 각 필드에 가중치를 부여하고 합산함으로써 더 정교한 검색 결과 제공이 가능하다.  

---

## p11. 색인 

<img src="/assets/img/lecture/bigdatasearch/2/image_2.png" alt="image" width="600px">

- **색인 대상 문서 예시**  
  - Doc#1: 인공지능 기술의 발전과 일자리 전망은  
  - Doc#2: 인공지능 시대의 핵심 ‘합성 데이터’, 세 가지로 알아보는 장단점과 전망은?  
  - Doc#3: 한국문화와 인공지능 기술을 융합한 콘텐츠의 전망  

- **형태소 분석**  
  - 문장을 형태소 단위로 분해하여 의미 단위로 나눔  

- **색인어 추출**  
  - 불필요한 조사·어미 등을 제거하고 핵심 단어만 남김  

- **색인기 구축**  
  - 추출된 색인어를 이용해 역파일(Inverted Index) 생성  
  - 각 단어가 어떤 문서에 등장했는지를 문서 ID와 함께 기록 
  
---

### 보충 설명

#### 1. 색인어 추출 전략의 중요성  
- 색인어를 어떤 기준으로 추출할지에 따라 검색 성능이 크게 달라진다.  
- 명사만 추출할 수도 있지만, 필요에 따라 동사·형용사 등을 포함해야 할 때도 있다.  
- 따라서 **검색 목적과 데이터 특성에 맞는 색인어 추출 전략을 직접 수립**해야 한다.  

#### 2. 복합어 처리  
- “한국문화”와 같은 복합어는 그대로 색인하면 검색 성공률이 떨어질 수 있다.  
- 이를 보완하기 위해 “한국”, “문화”, “한국문화”를 모두 색인어로 등록하기도 한다.  
- 이렇게 하면 단일어 검색과 복합어 검색 모두 대응할 수 있다.  

#### 3. Inverted Index 저장 방식  
- 색인어에 매칭되는 문서의 수는 가변적이다.  
- 이를 관리하기 위해 보통 **linked list** 구조를 사용한다.  
- 하지만 linked list는 노드마다 메모리 할당 연산이 발생해 성능 저하를 일으킬 수 있다.  
- 그래서 실제 구현에서는 **linked list를 기반으로 하되, 배열처럼 일정 크기의 블록을 미리 할당**하고, 공간이 부족하면 2배 단위로 확장하여 추가 메모리를 확보하는 방식을 쓴다.  
- 이렇게 하면 메모리 할당 부담을 줄이면서도 linked list의 유연성을 유지할 수 있다.  

#### 4. 검색 과정  
- 검색은 색인의 역방향 과정이다.  
- 즉, 형태소 분석 → 색인어 추출 → Inverted Index 참조 단계를 거쳐 관련 문서를 찾아낸다.  >

#### 5. 색인과 검색의 일관성  
- 색인 시 사용한 형태소 분석·색인어 추출 방식과, 검색 시의 처리 방식은 반드시 동일해야 한다.  
- 만약 색인 단계와 검색 단계가 다르게 적용되면, 검색 결과가 왜곡되거나 누락될 수 있다.  

---

## p12. 색인 

<img src="/assets/img/lecture/bigdatasearch/2/image_3.png" alt="image" width="600px">

---

### 보충 설명

- 기존 색인에서는 각 단어에 대해 **등장한 문서 목록**(docID 리스트)만 관리했다.  
- 이번 장표에서는 여기에 더해, **해당 단어와 관련된 전체 문서의 개수** 정보를 함께 저장한다.  
- 예: “인공지능” → 관련 문서 수 = 3 (Doc#1, Doc#2, Doc#3)  
- 이 정보는 특정 단어가 **얼마나 널리 분포되어 있는지**를 보여주며,  
  검색 결과의 **랭킹을 조정**하는 데 활용된다.  

---

## p13. 색인 

- **색인 용어의 결정**  
  - 출현빈도가 높은 용어는 부적합  
    - 일상적인 용어는 문서의 주제와 특별한 관련이 없음  
  - 출현빈도가 아주 낮은 용어는 거의 사용되지 않는 용어로 부적합  

- **Zipf의 법칙**  

<img src="/assets/img/lecture/bigdatasearch/2/image_4.png" alt="image" width="480px">

  - 대량의 텍스트에 사용된 어구의 빈출 순위와 빈도를 집계하면,  
    빈출 순위가 r번째인 빈도는 빈출 순위 첫 번째 빈도를 1/r한 값이 되는 법칙(제타 분포)  
  - 자연어 텍스트에서 단어의 빈도가 그 순위에 반비례한다는 것을 의미  
  - $frequency(rank \ r) \propto \frac{1}{rank \ r}$  

- **정리**  
  - 순위 × 출현 빈도 = 상수

---

> - **빈도가 너무 높은 단어**는 거의 모든 문서에 등장하는 불용어와 같아, 검색에 도움이 되지 않으므로 제외한다.  
> - **빈도가 너무 낮은 단어**는 거의 사용되지 않아 검색 결과에 의미 있는 기여를 하지 못하므로 역시 제외한다.  
> - 실제로는 **소수의 단어가 전체 빈도의 대부분을 차지**하는 경향이 있으며, 이는 Zipf의 법칙으로 설명된다.  

---

## p14. 검색

- **Query Optimization**  
  - Query 처리의 최적 순서는 ?  
  - 여러 개의 질의어를 포함한 Query의 경우 어떤 순서로 ?  
    - 가장 작은 집합에서 시작해서, 계속해서 잘라내기  
    - “Start with smallest set, then keey cutting further”  

- **예) Query**: “인공지능” and “기술” and “전망”  

<img src="/assets/img/lecture/bigdatasearch/2/image_5.png" alt="image" width="540px">


- (인공지능 and 전망) and 기술  

- Dictionary에 문서 빈도를 유지함으로써 처리 효율 도모

---

### 보충 설명

#### 1. 가장 작은 집합에서 시작해서, 계속해서 잘라내기  
- 여러 개의 질의어가 AND 조건으로 결합될 경우, 각 질의어는 해당 단어를 포함하는 문서 집합과 연결된다.  
- 이때 **문서 수가 가장 적은 집합**부터 시작하는 것이 효율적이다.  
- 작은 집합에서 먼저 후보 문서를 추려내면, 이후 연산에서 다뤄야 할 문서 수가 급격히 줄어든다.  
- 결과적으로 전체 교집합 연산의 비용이 줄어들고 검색 속도가 빨라진다.  

#### 2. 가장 큰 집합에서 시작했을 때의 비효율성  
- 만약 문서 수가 많은 집합부터 시작하면, 초기 단계에서 다루어야 할 후보 문서가 너무 많아진다.  
- 이후 단계에서 다른 질의어와 교집합을 계속 취하더라도, 불필요하게 큰 집합을 처음부터 탐색해야 하므로 연산 비용이 커진다.  
- 예를 들어,  
  - "기술" → 관련 문서 7개  
  - "전망" → 관련 문서 5개  
  - "인공지능" → 관련 문서 3개  
  - “기술” 집합(7개)부터 시작하면, 불필요하게 7개 전체를 탐색한 후 다시 5개, 3개와 비교해야 한다.  
- 반대로 “인공지능”(3개)부터 시작하면, 처음부터 소수의 문서만 다루면 되므로 효율적이다.  

---

## p15. Phrase Query와 위치 색인

- **“합성 데이터”와 같이 Query가 구문으로 주어진 경우**  
  - 다음의 문장들이 검색될까?  
    - “합성한 데이터로 학습하기”, “데이터를 합성하여 학습하였다”  
  - <term: docs> 형식의 색인만으로는 충분하지 않음  

- **Biword Index 구성하기**  
  - 텍스트에서 연속해서 출현하는 Terms의 쌍을 phase로 색인하는 방법  
    - 품사 정보와 언어학적 패턴을 활용하여 biwords 추출하여 색인  
      - 예1) “합성하다 데이터”, “데이터 학습하다”, “데이터 합성하다”, “합성하다 학습”  
      - 예2) “합성 데이터”, “데이터 학습”, “데이터 합성”, “합성 학습”  
    - biwords를 dictionary의 term으로 사용  

- **Query가 2단어보다 더 많은 단어로 되어 있다면?**  
  - 구문을 2단어씩 쪼개서 Boolean Query 처리 → 문제는?

---

### 보충 설명

#### 1. **Biword Index의 개념**  
- Biword Index란 텍스트 내에서 **연속된 두 단어(2-gram)**를 하나의 색인어(term)로 취급하여 저장하는 방식이다.  
- 예를 들어 “합성 데이터로 학습”이라는 문장에서 biword는  
  - “합성 데이터”, “데이터로 학습”  
  와 같이 추출된다.  

#### 2. **Biword Index를 사용하는 이유**  
- 단일 단어 색인만으로는 구문의 정확한 검색(phrase query)을 처리하기 어렵다.  
- 예: “합성 데이터”라는 구문을 검색할 때,  
  - 단어 단위 색인만 있으면 “합성”과 “데이터”가 떨어져 있어도 검색될 수 있다.  
- 따라서 연속된 단어 조합을 색인하면 **구문 단위 검색**을 정확히 처리할 수 있다.  
- 하지만 너무 많은 단어를 복합해서 색인(n-gram, 예: 3-gram, 4-gram 등)하면 **vocabulary 크기가 급격히 커진다.**  
  - 예를 들어, 단어가 10만 개일 때, 모든 3-gram 조합은 최대 $10^{15}$ 가지 가능 → 저장과 탐색이 비효율적  
  - 이렇게 되면 색인 공간이 기하급수적으로 커지고, 검색 과정에서 매칭 효율도 떨어진다.  
- 그래서 현실적으로 **2단어 조합(Biword)**이 구문 검색 효율성과 색인 크기 사이에서 가장 적절한 타협점으로 많이 사용된다.  

#### 3. **장단점**  
- 장점:  
  - 연속된 구문 검색(phrase query)을 빠르고 정확하게 지원할 수 있음   
  - 단순한 Boolean Query보다 더 세밀한 검색이 가능  
- 단점:  
  - 모든 연속된 단어 쌍을 색인해야 하므로, 색인 크기가 커지고 저장 공간 부담이 증가  
  - 단어 쌍을 넘어서는 긴 구문 검색에는 추가적인 처리가 필요  

---

## p16. Phrase Query와 위치 색인

- **Biword 색인의 문제 및 Issue**  
  - False positives  
    - Query  
    - “Stanford university palo alto” → “stanford university” and “university palo” and “palo alto”  

- **Term Dictionary의 크기**

---

### 보충 설명

#### 1. **False positives 문제**  
- Biword 색인은 단어 쌍을 기준으로 구문을 색인하기 때문에,  
  Query가 주어졌을 때 실제 구문과는 다른 조합이 매칭되는 경우가 발생할 수 있다.  
- 예: Query = “Stanford university palo alto”  
  - Biword 분해 → “stanford university”, “university palo”, “palo alto”  
  - 하지만 “university palo”처럼 실제 구문과 무관한 결과가 포함될 수 있어 잘못된 검색 결과(False positives)가 나온다.  

#### 2. **Term Dictionary의 크기 문제**  
- 모든 연속된 단어 쌍(bigram)을 vocabulary에 추가하면,  
  텍스트의 크기와 다양성에 비례해 vocabulary 크기가 급격히 커진다.  
- 이는 저장 공간 부담과 검색 효율 저하로 이어진다.  

#### 3. **Dictionary 크기 해결 방안**  
- 모든 bigram을 vocabulary에 추가하지 않고, **필터링 기준**을 적용한다.  
- 예: 두 단어가 **각각 단독으로 나타나는 빈도**와 **함께 나타나는 빈도**를 비교하여,  
  후자의 비율이 충분히 높은 경우에만 vocabulary에 추가한다.  
- 이렇게 하면 불필요하게 큰 vocabulary 생성을 막으면서도,  
  실제 의미 있는 구문(bigram)만 효율적으로 색인할 수 있다.  

---

## p17. 검색

- **Boolean Query Processing**  
  - Query: “인공지능” and “기술”  
    - “인공지능”을 포함하고 있는 문서 리스트와 “기술”을 포함하고 있는 문서 리스트를 가져오고  
    - 두 문서 리스트에 공통적으로 등장하는 문서를 찾는다  

<img src="/assets/img/lecture/bigdatasearch/2/image_6.png" alt="image" width="540px">


  - Query: “인공지능” and “기술” and “전망”

<img src="/assets/img/lecture/bigdatasearch/2/image_7.png" alt="image" width="540px">

---

### 보충 설명

#### 1. **Boolean Query의 기본 개념**  
- Boolean Query는 “AND, OR, NOT” 연산을 이용해 검색 결과를 결합하는 가장 단순하고 기본적인 검색 방식이다.  
- 예:  
  - “인공지능 AND 기술” → 두 단어 모두 포함된 문서만 검색  
  - “인공지능 OR 기술” → 두 단어 중 하나라도 포함된 문서 검색  
  - “인공지능 NOT 기술” → “인공지능”은 포함하지만 “기술”은 제외  

#### 2. **교집합 연산 과정**  
- 색인어마다 해당 단어가 등장하는 문서 리스트(posting list)를 가져온다.  
- AND 연산은 두 리스트의 교집합을 찾는 과정으로 구현된다.  
- 예:  
  - “인공지능” = {1, 2, 3}, “기술” = {1, 3}  
  - 교집합 = {1, 3}  

#### 3. **여러 단어 Query 처리**  
- Query가 “인공지능 AND 기술 AND 전망”처럼 세 단어 이상일 경우,  
  - 각 단어에 해당하는 문서 리스트를 순차적으로 교집합 처리한다.  
- 이때 효율을 위해 **문서 수가 적은 리스트부터 처리**하는 것이 최적화된 방식이다.  

#### 4. **장점과 한계**  
- 장점:  
  - 구현이 단순하고 직관적이다.  
  - 빠른 검색 속도를 보장할 수 있다.  
- 한계:  
  - 관련성(Relevance)을 고려하지 않고 단순히 포함 여부만 따지므로, 검색 품질이 낮을 수 있다.  
  - 대규모 문서 집합에서 리스트 교집합 연산이 많아질 경우 성능 부담이 커진다.  

---

## p18. Phrase Query와 위치 색인

- **위치 색인(Positional Indexing): 표준적으로 사용**  
  - 각 색인어에 대해 색인어의 토큰이 출현했던 위치를 색인  

  - <Term, # of term을 포함한 문서의 수;  
    doc1: position1, position2, ...;  
    doc2: position1, position2, ...;  
    etc.>  

  - <인공지능: 9934;  
    1: 7, 18, 72, 84, 231;  
    2: 3, 149;  
    3: 17, 190, 430, 544;  
    5: 1, 25, 39, ...?>  

- **Phrase query 프로세싱**  
  - doc:position lists를 모아서 **근접검색(Proximity Search)**  
    - 근접검색: 단어들간의 상대적 거리나 순서를 고려하여 검색하는 방법  

- **위치색인 크기**  
  - 위치 색인을 하지 않는 대비 2~4배  
  - 원래 텍스트 크기의 35~50% 정도

---

### 보충 설명

#### 1. **옵셋(offset)이란?**  
- 위치 색인에서 옵셋은 **문서 내에서 특정 단어가 출현한 위치 번호**를 의미한다.  
- 예: "인공지능 기술의 전망"이라는 문장에서  
  - "인공지능" = 위치 1  
  - "기술" = 위치 2  
  - "전망" = 위치 4  
- 이렇게 단어별 위치 정보를 기록하면, 이후 구문 검색이나 근접 검색에서 활용할 수 있다.  

#### 2. **옵셋과 옵셋이 근접하는지 체크**  
- Phrase Query나 Proximity Search에서는 두 단어가 단순히 같은 문서에 등장하는 것만으로는 부족하다.  
- 예를 들어 "인공지능 기술"을 찾으려면,  
  - "인공지능"의 옵셋 = 1, "기술"의 옵셋 = 2 → 두 단어가 연속하므로 매칭  
  - 만약 "인공지능"(1)과 "기술"(10)이라면, 거리가 멀어 매칭되지 않음   
- 따라서 위치 색인은 **옵셋 간의 상대적 거리와 순서**를 확인하여 검색의 정확도를 높인다.  

#### 3. **위치 색인의 크기가 왜 2~4배 커지는가?**  
- 일반 역색인(Inverted Index)은 단어가 등장한 **문서 ID 목록만** 저장한다.  
- 위치 색인은 각 문서 안에서의 **단어별 출현 위치(옵셋)까지 기록**해야 한다.  
- 따라서 저장해야 하는 데이터 양이 크게 늘어나, 색인 크기가 보통 2~4배 정도 더 커진다.  

#### 4. **원래 텍스트의 35~50% 정도라는 의미**  
- 위치 색인의 전체 크기를 원래 텍스트 파일의 크기와 비교하면,  
  - 대략 원문 텍스트의 35~50% 정도 용량이 추가로 필요하다.  
- 즉, 원문이 1GB라면 위치 색인은 약 350MB~500MB 정도가 된다.  
- 이는 색인을 저장하는 데 필요한 **추가 공간 오버헤드**를 정량적으로 나타낸 것이다.  

---

## p19. Boolean 검색

- **Boolean Queries**  
  - 문서의 포함 또는 제외 여부를 결정  

- **검색 결과의 순위화 또는 그룹화에 대한 사용자 요구**  
  - 질의와 각 문서의 근접성(proximity) 측정 필요  
  - 사용자에게 제시된 문서가 단일문서인지, 질의의 다양한 측면을 다루는 문서그룹인지 결정 필요  

---

### 보충 설명

#### 1. **Boolean Query의 한계와 필요성**  
- Boolean 검색은 **AND, OR, NOT** 연산을 이용해 관련 문서를 판별한다.  
- 하지만 결과가 단순히 **관련 있음 / 없음**(binary)으로만 구분되므로,  
  문서가 얼마나 관련성이 높은지는 알 수 없다.  
- 따라서 단순한 포함 여부뿐만 아니라, **순위화(ranking)** 과정이 필요하다.  

#### 2. **근접성(Proximity)의 의미**  
- 예를 들어 "인공지능 기술 전망"이라는 질의가 있을 때,  
  세 단어가 서로 가까이 위치한 문서일수록 더 관련성이 높다고 본다.  
- Boolean 검색만 사용하면 단어가 흩어져 있어도 검색 결과에 포함되므로,  
  이를 보완하기 위해 **근접성 측정**이 필요하다.  

#### 3. 단일문서 vs 문서그룹 결정  
- **단일문서로 제시**: 질의가 구체·협소(예: 고유명사, 정확한 문서명/구문), 상위 결과의 점수 격차가 큼(Top-1이 월등), 사이트/작성자 등 메타 제약이 명확할 때.  
- **문서그룹으로 제시**: 질의가 광범위·모호(예: 단일 일반 키워드), 상위 결과가 다양한 관점·주제·출처로 분산, 점수 격차가 작아 우선순위가 불확실할 때.  
   
---

## 20. 색인 – Scalable Index 구성

- 예시 데이터 셋: Reuters newswire (part of 1995 and 1996)

<img src="/assets/img/lecture/bigdatasearch/2/image_8.png" alt="image" width="720px">

---

### 보충 설명

- **Reuters Newswire 데이터셋**  
  - 로이터 통신사가 배포한 뉴스 기사 모음으로, 정보검색(IR) 및 자연어처리(NLP) 분야에서 가장 널리 사용되는 벤치마크 데이터셋 중 하나이다.  
  - 1995~1996년에 작성된 뉴스 기사 일부를 포함하며, 문서 분류, 텍스트 마이닝, 색인 및 검색 알고리즘의 성능 평가에 자주 활용된다.  
  - 문서는 주로 경제, 금융, 정치 등 시사 이슈를 다루고 있으며, **문서 ID, 카테고리 라벨, 본문 텍스트** 등의 구조를 가진다.  
  - 대표적으로 `Reuters-21578`과 `Reuters Corpus Volume 1 (RCV1)`이 연구에 널리 사용되며, 이 장표에서 언급된 것은 그 중 특정 연도(1995~1996년)의 부분 데이터셋이다.  

---

## 21. 색인 – Scalable Index 구성

| symbol | statistic                                   | value        | 설명 |
|--------|---------------------------------------------|--------------|------|
| **N**  | documents                                   | 800,000      | 뉴스 문서 80만 개 |
| **L**  | avg. # tokens per doc                       | 200          | 문서(뉴스) 1개당 평균 200개 토큰 |
| **M**  | terms (= word types)                        | 400,000      | 고유한 term 약 40만 개 |
|        | avg. # bytes per token (incl. spaces/punct.)| 6            | 토큰 1개당 평균 6바이트 (공백/구두점 포함) |
|        | avg. # bytes per token (without spaces/punct.) | 4.5       | 구두점 제외 시 토큰 1개당 평균 4.5바이트 |
|        | avg. # bytes per term                       | 7.5          | term 1개당 평균 7.5바이트 |
|        | non-positional postings                     | 100,000,000  | 위치 고려 없는 postings 약 1억 개 |

---

### 보충 설명

#### 1. **Posting이란?**  
- 정보 검색에서 **posting**은 `(term, docID)` 형태의 **튜플(tuple)**로 표현된다.  
- 즉, 특정 단어(**term**)가 어떤 문서(**docID**)에 등장했는지를 나타내는 최소 단위이다.  
- 필요에 따라 빈도(frequency)나 위치(position) 같은 추가 속성을 포함할 수 있다.  
- 예: `"인공지능"`이라는 term이 `Doc3`에 등장했다면 → `(인공지능, Doc3)`  

#### 2. **Non-positional postings란?**  
- posting에서 단어의 위치 정보는 저장하지 않고,  
  `(term, docID, frequency)` 정도만 기록하는 방식이다.  
- 예: `"인공지능"`이 `Doc1`에 3번, `Doc3`에 2번 등장했다면 →  
  `(인공지능, Doc1, f=3)`, `(인공지능, Doc3, f=2)`  
- 따라서 구체적인 구절 검색(phrase query)에는 한계가 있지만,  
  Boolean 검색이나 TF–IDF 기반 검색에는 충분히 활용 가능하다.  

#### 3. **왜 1억 개의 non-positional postings이 되는가?**  
- 전체 문서 수: **800,000개**  
- 문서당 평균 토큰 수: **200개**  
- 전체 토큰 수 = `800,000 × 200 = 160,000,000 (1억 6천만 개)`  
- 같은 문서 내 중복 term들은 하나의 `(term, docID)` posting에 합쳐지므로,  
  실제 posting의 총 개수는 약 **100,000,000 (1억 개)** 수준으로 계산된다.  

---

## p22. 색인 – Scalable Index 구성

**Single-pass in-memory indexing**

```pseudo
SPIMI-INVERT(token_stream)
1   output_file = NEWFILE()                       // 새로운 출력 파일 생성
2   dictionary = NEWHASH()                        // 해시 기반의 빈 dictionary 초기화
3   while (free memory available)                 // 메모리가 허용되는 동안 반복
4       do token ← next(token_stream)             // 다음 토큰을 읽어옴
5       if term(token) ∉ dictionary               // 토큰의 term이 dictionary에 없다면
6           then postings_list = ADDTODICTIONARY(dictionary, term(token))  // 새 term 추가
7           else postings_list = GETPOSTINGSLIST(dictionary, term(token))  // 있다면, 기존 postings list 가져옴
8       if full(postings_list)                    // postings list가 꽉 찼다면
9           then postings_list = DOUBLEPOSTINGSLIST(dictionary, term(token)) // 크기를 2배로 확장
10      ADDTOPOSTINGSLIST(postings_list, docID(token)) // postings list에 해당 문서 ID 추가
11  sorted_terms ← SORTTERMS(dictionary)          // dictionary의 term들을 정렬
12  WRITEBLOCKTODISK(sorted_terms, dictionary, output_file) // 정렬된 결과를 디스크에 기록
13  return output_file                            // 출력 파일 반환
```

---

### 보충 설명

#### 1. 의사코드의 의미  
- **1~2행:** 색인을 저장할 출력 파일과 dictionary를 초기화한다.  
- **3~4행:** 메모리가 허용되는 동안 토큰 스트림에서 단어를 하나씩 읽는다.  
- **5~6행:** 단어가 dictionary에 없다면 새 postings list를 생성한다.  
- **7행:** dictionary에 있다면 기존 postings list를 가져온다.  
- **8~9행:** postings list가 가득 차면 크기를 2배로 확장한다.  
- **10행:** postings list에 현재 토큰이 등장한 문서의 docID를 추가한다.  
- **11~12행:** 메모리가 다 차면 dictionary를 정렬하고, 결과를 디스크에 저장한다.  
- **13행:** 최종적으로 출력 파일을 반환한다.  

#### 2. 2배 확장 방식의 의미  
- 일반적으로 postings list는 **linked list** 형태로 관리될 수 있다.  
- 하지만 linked list는 노드마다 메모리 할당이 발생하여 비효율적이다.  
- 따라서 실제 구현에서는 linked list를 기반으로 하되, **배열처럼 일정 크기의 블록을 미리 확보**한다.  
- 공간이 부족할 경우, 기존 블록 크기의 **2배 단위로 확장**하여 추가 메모리를 확보한다.  
- 이렇게 하면 메모리 할당 연산의 빈도를 줄이면서도 linked list의 유연성을 유지할 수 있다.  

#### 3. 설명 포인트

- SPIMI(Single-Pass In-Memory Indexing)는 메모리가 허용하는 범위 내에서 한 번에 색인을 만들어 디스크에 저장하는 방식이다.
- 각 term마다 postings list가 동적으로 확장되며, 전체 과정을 반복하여 대규모 인덱스를 완성한다.

---

## p23. 색인 – distributed Indexing

- 입력 문서 집합을 **splits** 단위로 분할  
- **Master**: 유휴 상태의 파서(parser) 머신에 split을 할당  
- **Parser**: 문서를 읽고 (term, doc) 쌍을 생성  
- **Inverter**: 모든 (term, doc) 쌍(=postings)을 수집하여 정렬한 뒤 postings list에 기록  

---

**Map-Reduce의 개요**  
- map: 입력 → list(k, v)  
- reduce: (k, list(v)) → 출력  

---

**색인 구축 과정**  
- map: 문서 집합 → list(termID, docID)  
- reduce: (<termID1, list(docID)>, <termID2, list(docID)>, …)  
  → (postings list1, postings list2, …)  

<img src="/assets/img/lecture/bigdatasearch/2/image_9.png" alt="image" width="600px">

---

### 보충 설명

#### 1. **Map-Reduce란 무엇인가?**  
- Map-Reduce는 대규모 데이터를 **분산 처리(distributed processing)** 하기 위한 대표적인 프로그래밍 모델이다.  
- **Map 단계**: 입력 데이터를 여러 조각(**splits**)으로 나누고, 각각의 조각을 병렬로 처리하여 `(key, value)` 쌍을 생성한다.  
- **Reduce 단계**: 동일한 key를 기준으로 value들을 모아 집계/정리하여 최종 결과를 산출한다.  
- 예: 단어 빈도 세기(word count)의 경우, map은 `(단어, 1)`을 내보내고, reduce는 동일 단어에 대한 빈도를 합산한다.  

#### 2. **색인(distributed indexing) 과정의 이해**  
- 입력 문서 집합을 여러 **splits**로 나누어 처리한다 → 이를 **샤딩(sharding)** 이라고도 한다.  
- **Master 노드**는 idle 상태의 **Parser** 노드에 작업을 분배한다.  
- **Parser**는 문서를 읽고 `(term, docID)` 쌍을 생성한다.  
- 생성된 `(term, docID)`들은 **Segment files**에 저장된다.  
- 이후 **Inverter**가 이를 모아 정렬(sorting)하고, 최종적으로 **postings list**를 구축한다.  

#### 3. **그림 속 과정 설명**  
- **Master**: 전체 문서를 관리하고, 각 문서 조각(splits)을 parser들에게 **할당(assign)** 한다.  
- **Parser**: 문서를 읽어 `(term, docID)`로 변환하는 단계 → 여기서 문서가 분할되어 처리된다.  
- **Segment files**: term의 알파벳 구간별(a–f, g–p, q–z 등)로 나눠 저장된다 → 이는 **샤딩(sharding)** 기법으로 병렬처리를 가능하게 한다.  
- **Inverter**: 같은 term에 해당하는 `(term, docID)`들을 모아 **정렬하고 그룹화** 한다.  
- 최종적으로 postings list가 만들어져 효율적인 검색에 활용된다.  

---

## p24. 색인 – 동적 색인

- **정적 문서 집합 색인 대비 동적으로 변화하는 문서 색인은?**  
  - 문서들은 시간이 지남에 따라 추가되고, 삽입됨  
  - 문서들은 삭제되거나 수정되기도 함  

- **색인 문서의 변화는 dictionary, postings lists의 수정 요구**  
  - dictionary에 이미 있는 용어에 대한 posting lists 업데이트  
  - 새로운 용어의 사전에 추가  

- **간단한 접근방법**  
  - 색인 업데이트 전략: 보조 색인 활용  
    - 대규모 주(Main) 색인을 유지  
    - 새로운 문서는 소규모 보조(Auxiliary) 색인에 추가  
    - 검색 시, 두 색인에서 동시에 검색하고 결과를 병합  
  - 문서 삭제 처리  
    - 삭제된 문서는 무효화 비트 벡터(invalidation bit-vector)로 표시  
    - 검색 결과 출력 시, 이 비트 벡터를 이용해 삭제된 문서를 필터링  
  - 주기적인 재색인  
    - 주기적으로 보조 색인을 주 색인에 통합하여 하나의 주 색인으로 재색인  

---

### 보충 설명

#### 1. **동적 색인의 개념**  
- 현실의 문서 집합은 고정되지 않고 계속 변한다.  
- 새로운 문서가 추가되고, 기존 문서는 삭제되거나 수정되므로 색인도 동적으로 유지해야 한다.  

#### 2. **보조 색인(Auxiliary Index)의 역할**  
- 기존의 대규모 색인(Main Index)은 그대로 두고,  
  새로운 문서는 보조 색인에만 반영한다.  
- 검색 시에는 **메인 색인 + 보조 색인**을 동시에 탐색하고 결과를 합친다.  
- 일정 주기마다 보조 색인을 메인 색인에 병합하여 재색인 과정을 거친다.  
  
#### 3. **삭제 처리 방식**  
- 삭제된 문서를 바로 색인에서 제거하는 대신,  
  **무효화 비트 벡터(invalidation bit-vector)**를 사용하여 표시한다.  
- 검색 결과를 출력할 때는 이 비트 벡터를 참조해 삭제된 문서를 걸러낸다. 
- 이는 **bit masking** 기법으로 효율적인 삭제 관리가 가능하다.  

#### 4. **정리**  
- 메인 색인은 안정적으로 유지하고,  
- 보조 색인은 빠른 업데이트를 담당하며,  
- 일정 기간마다 **메인 + 보조 색인**을 합쳐 새로운 주 색인으로 만드는 방식이  
  동적 색인의 핵심 전략이다.  