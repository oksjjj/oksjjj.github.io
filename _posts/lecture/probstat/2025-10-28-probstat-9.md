---
layout: post
title: "[확률과 통계] 9주차"
date: 2025-10-28 23:00:00 +0900
categories:
  - "대학원 수업"
  - "확률과 통계"
tags: []
---

> 출처: 확률과 통계 – 박성우 교수님, 고려대학교 (2025)

## p2. 생성 모델의 일반 개념 (General Concept of Generative Models)  

<img src="/assets/img/lecture/probstat/9/image_1.png" alt="image" width="800px">

---

### 강의 내용  

- 이 그림은 **생성 모델(Generative Model)**의 일반적인 구조를 보여준다.  
- 생성 모델은 이름 그대로 **데이터를 생성하는 모델**로, 예를 들어 텍스트가 주어졌을 때 그에 맞는 **이미지나 형태(Shape)**를 만들어내는 AI 모델을 말한다.  
- 이러한 모델의 핵심 구성요소는 **생성자(Generator)**이며, 실제로 데이터를 만들어내는 역할을 한다.  

- 생성자에는 두 가지 주요 입력이 존재한다.  
  1. **조건부 입력(conditional input)**:  
    - 예를 들어 “Bird(새)”라는 단어, 혹은 텍스트 토큰(token) 시퀀스 등이다.  
    - 이미지 생성 모델의 경우, 텍스트나 개념(Concept)을 벡터로 표현한 형태가 입력으로 들어간다.  
  2. **무작위 입력(random input)**:  
    - 생성 모델은 확률적 특성을 가지므로, **랜덤성(randomness)**을 반드시 포함해야 한다.  
    - 이는 통계적 확률 모델에서 매우 중요한 개념으로, **랜덤 변수(random variable)**를 통해 표현된다.  
    - 그림 속 주사위는 이러한 **확률적 입력(latent variable)**을 상징한다.  

- 이 무작위 입력은 모델이 단일 이미지만 반복해서 생성하지 않도록 도와준다.  
  - 예를 들어, 조건이 “Bird”일 때 항상 같은 새의 이미지만 나온다면 생성 모델의 의미가 없다.  
  - 랜덤성을 주입함으로써 “Bird”라는 조건 안에서도 다양한 **새의 변형(variation)**과 **다양성(diversity)**을 표현할 수 있게 된다.  

- 따라서 생성 모델은 **조건부 정보(conditional information)**와 **확률적 요소(randomness)**를 결합하여  
  주어진 조건에 맞는 다양한 샘플들을 생성하도록 학습되는 모델이다.  

---

## p3. 생성 모델의 확률적 표현 (Probabilistic Representation of Generative Models)  

<img src="/assets/img/lecture/probstat/9/image_2.png" alt="image" width="800px">

---

### 강의 내용  

- 현대적인 **생성 모델(Generative Model)**은 기본적으로 확률적 랜덤성을 입력으로 받는 **파이프라인 구조**를 가진다.  
- 여기서 주사위는 **랜덤 변수(random variable)**를 의미하며, 각각의 주사위는 생성될 이미지의 특정 속성(feature)에 영향을 준다.  
  - 예를 들어,  
    - 첫 번째 랜덤 변수는 색상(color)을,  
    - 두 번째 랜덤 변수는 각도(angle)를,  
    - 세 번째 랜덤 변수는 크기(size)를 결정하도록 작용할 수 있다.  
- 즉, **랜덤성의 각 요소가 이미지의 다양한 속성을 통계적으로 결정**하는 역할을 한다.  

- 이러한 랜덤 변수들은 단일 차원으로 존재하지 않고, **고차원(high-dimensional) 확률 공간**에서 정의된다.  
  - 다시 말해, 생성 모델의 입력으로 들어가는 랜덤 벡터는 보통 **다변량 가우시안 분포(multivariate Gaussian distribution)**를 따른다.  
  - 이 벡터의 각 차원은 처음에는 의미 없는 수치이지만, 학습이 진행되면서 특정한 **의미적 속성(semantic feature)**과 자연스럽게 연관된다.  

- 초기에는 이 벡터의 축(axis)에 “이건 색상, 이건 각도”와 같은 의미가 부여되어 있지 않다.  
  그러나 모델이 충분히 학습되면, **첫 번째 차원은 색상 변화**, **두 번째 차원은 각도 변화**, **세 번째 차원은 크기 변화**처럼  
  실제 데이터의 속성과 대응되는 구조가 **자연스럽게 형성된다.**  

- 이 현상은 생성 모델의 매우 흥미로운 특성 중 하나로,  
  **명시적인 피처(feature) 정의 없이도 학습을 통해 의미적 공간(semantic space)이 형성되는 과정**을 보여준다.  

---

## p4. 데이터 생성기의 분류 (Categorization of Data Generators)  

두 가지 접근 방식이 있다.  

1. **직접 접근(Direct approach)**: 데이터를 직접 생성하는 함수를 학습한다.  
(혼동스럽게도, 때때로 “암묵적 생성 모델(implicit generative model)”이라고도 불린다.)

   $$
   G : \mathcal{Z} \rightarrow \mathcal{X}
   $$

2. **간접 접근(Indirect approach)**: 데이터를 평가(score)하는 함수를 학습하고,  
   이 함수 아래에서 점수가 높은 지점을 찾아 데이터를 생성한다.  

   $$
   E : \mathcal{X} \rightarrow \mathbb{R}
   $$

---

### 강의 내용  

- 데이터 생성 모델은 **직접 접근 방식**과 **간접 접근 방식**으로 나뉜다.  
  이 두 접근은 데이터가 생성되는 **함수의 형태**와 **확률적 구조를 반영하는 방식**에서 차이가 있다.  

#### 1. 직접 접근 (Direct Approach)  
- **정의**: 랜덤 변수를 입력으로 받아 **데이터를 직접 생성하는 함수**를 학습하는 방식이다.  
  예를 들어, 잠재 변수 $ \mathbf{z} $ 를 입력받아 이미지 $ \mathbf{x} $ 를 생성한다.  
- **대표 모델**:  
  - **GAN(Generative Adversarial Network)**  
  - **VAE(Variational Autoencoder)** (직접 접근과 간접 접근의 중간적 성격을 가짐)  
- **특징**:  
  - 랜덤 변수 $ \mathbf{z} $ 로부터 바로 샘플을 생성한다.  
  - 즉, $ \mathbf{z} $ 가 주어지면 $ G(\mathbf{z}) $ 가 바로 데이터 샘플이 된다.  
  - 이러한 모델은 **명시적 확률 분포를 정의하지 않아도** 데이터를 직접 생성할 수 있기 때문에  
    “암묵적 생성 모델(implicit generative model)”이라고도 불린다.  

#### 2. 간접 접근 (Indirect Approach)  
- **정의**: 데이터를 직접 생성하지 않고, 데이터의 **좋은 정도(goodness)** 혹은 **우도(likelihood)**를 평가하는  
  **스코어(score)** 또는 **에너지(energy)** 함수를 학습한다.  
- **대표 모델**:  
  - **Diffusion model**, **Energy-based model**, **Score-based model** 등이 이에 해당한다.  
- **특징**:  
  - 단순히 랜덤 변수 $ \mathbf{z} $ 를 네트워크에 넣는 것이 아니라,  
    여러 단계의 **파이프라인**을 거쳐 데이터를 점진적으로 생성한다.  
  - 생성 과정에서 **에너지 함수 $ E(\mathbf{x}) $** 또는 **스코어 함수**를 이용해  
    “얼마나 데이터가 잘 생성되었는가”를 반복적으로 평가하고 조정한다.  

#### 3. 정리 및 비교  
- 직접 접근은 **생성 함수 $ G $**를 학습하여 데이터를 바로 만들어내는 반면,  
  간접 접근은 **평가 함수 $ E $**를 학습하고 이를 통해 생성 과정을 제어한다.  
- **VAE**는 두 접근 방식의 중간에 위치한 모델로,  
  **확률적 인코더-디코더 구조**를 통해 데이터를 생성하면서도 잠재 공간의 분포를 명시적으로 모델링한다.  

---

## p5. 직접 접근(Direct Approach)의 학습과 샘플링 과정  

<img src="/assets/img/lecture/probstat/9/image_3.png" alt="image" width="800px">

---

### 강의 내용  

- 생성 모델의 전체 과정을 **학습(Training)** 단계와 **샘플링(Sampling)** 단계로 나눌 수 있다.  
- 학습 과정은 주어진 **데이터셋(Data)**을 바탕으로 **모델의 파라미터(θ)**를 학습하는 단계이며,  
  샘플링 과정은 학습된 파라미터를 이용해 새로운 데이터를 생성하는 단계이다.  

#### 1. 학습(Training) 단계  
- 왼쪽의 데이터셋은 사람 얼굴 이미지처럼 대량의 실제 데이터를 의미한다.  
- **Learner**(또는 생성 네트워크)는 이 데이터로부터 **최적의 파라미터 θ**를 학습한다.  
- 이 파라미터 θ는 학습이 끝난 후 **생성기(generator)**의 내부 가중치로 사용된다.  
- 즉, 학습 단계에서는 “주어진 데이터로부터 어떻게 새로운 데이터를 생성할 것인가”를 배우는 과정이다.  

#### 2. 샘플링(Sampling) 단계  
- 학습이 끝난 후, 테스트 혹은 추론 시점(**inference time / test time**)에는  
  학습된 파라미터 $ \theta $ 를 고정한 채로, **랜덤 변수 $ z $**를 입력으로 주입한다.  
- 이 랜덤 벡터 $ z $는 잠재 공간(latent space)의 확률적 입력으로,  
  이를 **디코더(decoder)** 혹은 생성기 $ g_\theta $에 통과시켜 새로운 샘플(이미지)을 생성한다.  
- 즉,  
  $$
  \mathbf{x}_{\text{sample}} = g_\theta(\mathbf{z})
  $$
  의 형태로 새로운 데이터가 만들어진다.  

#### 3. 분류(classification) 문제와의 차이  
- 일반적인 분류 모델(classification model)은  
  학습 시점과 추론 시점의 입력–출력 구조가 거의 동일하다.  
- 반면 **생성 모델(generative model)**은  
  학습(Training)과 샘플링(Sampling)의 **파이프라인이 분리되어 있다는 점**이 큰 차이이다.  
  - 학습 단계에서는 데이터로부터 파라미터를 학습하고,  
  - 샘플링 단계에서는 확률적 입력으로부터 새로운 데이터를 생성한다.  
- 이러한 구조적 분리는 생성 모델의 핵심적인 특성이며,  
  **트레이닝 파이프라인과 생성 파이프라인이 서로 다른 형태로 존재한다는 점**을 이해하는 것이 중요하다.  
