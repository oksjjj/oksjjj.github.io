---
layout: post
title: "[확률과 통계] 9주차"
date: 2025-10-28 23:00:00 +0900
categories:
  - "대학원 수업"
  - "확률과 통계"
tags: []
---

> 출처: 확률과 통계 – 박성우 교수님, 고려대학교 (2025)

## p2. 생성 모델의 일반 개념 (General Concept of Generative Models)  

<img src="/assets/img/lecture/probstat/9/image_1.png" alt="image" width="720px">

---

### 강의 내용  

- 이 그림은 **생성 모델(Generative Model)**의 일반적인 구조를 보여준다.  
- 생성 모델은 이름 그대로 **데이터를 생성하는 모델**로, 예를 들어 텍스트가 주어졌을 때 그에 맞는 **이미지나 형태(Shape)**를 만들어내는 AI 모델을 말한다.  
- 이러한 모델의 핵심 구성요소는 **생성자(Generator)**이며, 실제로 데이터를 만들어내는 역할을 한다.  

- 생성자에는 두 가지 주요 입력이 존재한다.  
  1. **조건부 입력(conditional input)**:  
    - 예를 들어 “Bird(새)”라는 단어, 혹은 텍스트 토큰(token) 시퀀스 등이다.  
    - 이미지 생성 모델의 경우, 텍스트나 개념(Concept)을 벡터로 표현한 형태가 입력으로 들어간다.  
  2. **무작위 입력(random input)**:  
    - 생성 모델은 확률적 특성을 가지므로, **랜덤성(randomness)**을 반드시 포함해야 한다.  
    - 이는 통계적 확률 모델에서 매우 중요한 개념으로, **랜덤 변수(random variable)**를 통해 표현된다.  
    - 그림 속 주사위는 이러한 **확률적 입력(latent variable)**을 상징한다.  

- 이 무작위 입력은 모델이 단일 이미지만 반복해서 생성하지 않도록 도와준다.  
  - 예를 들어, 조건이 “Bird”일 때 항상 같은 새의 이미지만 나온다면 생성 모델의 의미가 없다.  
  - 랜덤성을 주입함으로써 “Bird”라는 조건 안에서도 다양한 **새의 변형(variation)**과 **다양성(diversity)**을 표현할 수 있게 된다.  

- 따라서 생성 모델은 **조건부 정보(conditional information)**와 **확률적 요소(randomness)**를 결합하여  
  주어진 조건에 맞는 다양한 샘플들을 생성하도록 학습되는 모델이다.  

---

## p3. 생성 모델의 확률적 표현 (Probabilistic Representation of Generative Models)  

<img src="/assets/img/lecture/probstat/9/image_2.png" alt="image" width="720px">

---

### 강의 내용  

- 현대적인 **생성 모델(Generative Model)**은 기본적으로 확률적 랜덤성을 입력으로 받는 **파이프라인 구조**를 가진다.  
- 여기서 주사위는 **랜덤 변수(random variable)**를 의미하며, 각각의 주사위는 생성될 이미지의 특정 속성(feature)에 영향을 준다.  
  - 예를 들어,  
    - 첫 번째 랜덤 변수는 색상(color)을,  
    - 두 번째 랜덤 변수는 각도(angle)를,  
    - 세 번째 랜덤 변수는 크기(size)를 결정하도록 작용할 수 있다.  
- 즉, **랜덤성의 각 요소가 이미지의 다양한 속성을 통계적으로 결정**하는 역할을 한다.  

- 이러한 랜덤 변수들은 단일 차원으로 존재하지 않고, **고차원(high-dimensional) 확률 공간**에서 정의된다.  
  - 다시 말해, 생성 모델의 입력으로 들어가는 랜덤 벡터는 보통 **다변량 가우시안 분포(multivariate Gaussian distribution)**를 따른다.  
  - 이 벡터의 각 차원은 처음에는 의미 없는 수치이지만, 학습이 진행되면서 특정한 **의미적 속성(semantic feature)**과 자연스럽게 연관된다.  

- 초기에는 이 벡터의 축(axis)에 “이건 색상, 이건 각도”와 같은 의미가 부여되어 있지 않다.  
  그러나 모델이 충분히 학습되면, **첫 번째 차원은 색상 변화**, **두 번째 차원은 각도 변화**, **세 번째 차원은 크기 변화**처럼  
  실제 데이터의 속성과 대응되는 구조가 **자연스럽게 형성된다.**  

- 이 현상은 생성 모델의 매우 흥미로운 특성 중 하나로,  
  **명시적인 피처(feature) 정의 없이도 학습을 통해 의미적 공간(semantic space)이 형성되는 과정**을 보여준다.  

---

## p4. 데이터 생성기의 분류 (Categorization of Data Generators)  

두 가지 접근 방식이 있다.  

1. **직접 접근(Direct approach)**: 데이터를 직접 생성하는 함수를 학습한다.  
(혼동스럽게도, 때때로 “암묵적 생성 모델(implicit generative model)”이라고도 불린다.)

   $$
   G : \mathcal{Z} \rightarrow \mathcal{X}
   $$

2. **간접 접근(Indirect approach)**: 데이터를 평가(score)하는 함수를 학습하고,  
   이 함수 아래에서 점수가 높은 지점을 찾아 데이터를 생성한다.  

   $$
   E : \mathcal{X} \rightarrow \mathbb{R}
   $$

---

### 강의 내용  

- 데이터 생성 모델은 **직접 접근 방식**과 **간접 접근 방식**으로 나뉜다.  
  이 두 접근은 데이터가 생성되는 **함수의 형태**와 **확률적 구조를 반영하는 방식**에서 차이가 있다.  

#### 1. 직접 접근 (Direct Approach)  
- **정의**: 랜덤 변수를 입력으로 받아 **데이터를 직접 생성하는 함수**를 학습하는 방식이다.  
  예를 들어, 잠재 변수 $ \mathbf{z} $ 를 입력받아 이미지 $ \mathbf{x} $ 를 생성한다.  
- **대표 모델**:  
  - **GAN(Generative Adversarial Network)**  
  - **VAE(Variational Autoencoder)** (직접 접근과 간접 접근의 중간적 성격을 가짐)  
- **특징**:  
  - 랜덤 변수 $ \mathbf{z} $ 로부터 바로 샘플을 생성한다.  
  - 즉, $ \mathbf{z} $ 가 주어지면 $ G(\mathbf{z}) $ 가 바로 데이터 샘플이 된다.  
  - 이러한 모델은 **명시적 확률 분포를 정의하지 않아도** 데이터를 직접 생성할 수 있기 때문에  
    “암묵적 생성 모델(implicit generative model)”이라고도 불린다.  

#### 2. 간접 접근 (Indirect Approach)  
- **정의**: 데이터를 직접 생성하지 않고, 데이터의 **좋은 정도(goodness)** 혹은 **우도(likelihood)**를 평가하는  
  **스코어(score)** 또는 **에너지(energy)** 함수를 학습한다.  
- **대표 모델**:  
  - **Diffusion model**, **Energy-based model**, **Score-based model** 등이 이에 해당한다.  
- **특징**:  
  - 단순히 랜덤 변수 $ \mathbf{z} $ 를 네트워크에 넣는 것이 아니라,  
    여러 단계의 **파이프라인**을 거쳐 데이터를 점진적으로 생성한다.  
  - 생성 과정에서 **에너지 함수 $ E(\mathbf{x}) $** 또는 **스코어 함수**를 이용해  
    “얼마나 데이터가 잘 생성되었는가”를 반복적으로 평가하고 조정한다.  

#### 3. 정리 및 비교  
- 직접 접근은 **생성 함수 $ G $**를 학습하여 데이터를 바로 만들어내는 반면,  
  간접 접근은 **평가 함수 $ E $**를 학습하고 이를 통해 생성 과정을 제어한다.  
- **VAE**는 두 접근 방식의 중간에 위치한 모델로,  
  **확률적 인코더-디코더 구조**를 통해 데이터를 생성하면서도 잠재 공간의 분포를 명시적으로 모델링한다.  

---

## p5. 직접 접근(Direct Approach)의 학습과 샘플링 과정  

<img src="/assets/img/lecture/probstat/9/image_3.png" alt="image" width="800px">

---

### 강의 내용  

- 생성 모델의 전체 과정을 **학습(Training)** 단계와 **샘플링(Sampling)** 단계로 나눌 수 있다.  
- 학습 과정은 주어진 **데이터셋(Data)**을 바탕으로 **모델의 파라미터(θ)**를 학습하는 단계이며,  
  샘플링 과정은 학습된 파라미터를 이용해 새로운 데이터를 생성하는 단계이다.  

#### 1. 학습(Training) 단계  
- 왼쪽의 데이터셋은 사람 얼굴 이미지처럼 대량의 실제 데이터를 의미한다.  
- **Learner**(또는 생성 네트워크)는 이 데이터로부터 **최적의 파라미터 θ**를 학습한다.  
- 이 파라미터 θ는 학습이 끝난 후 **생성기(generator)**의 내부 가중치로 사용된다.  
- 즉, 학습 단계에서는 “주어진 데이터로부터 어떻게 새로운 데이터를 생성할 것인가”를 배우는 과정이다.  

#### 2. 샘플링(Sampling) 단계  
- 학습이 끝난 후, 테스트 혹은 추론 시점(**inference time / test time**)에는  
  학습된 파라미터 $ \theta $ 를 고정한 채로, **랜덤 변수 $ z $**를 입력으로 주입한다.  
- 이 랜덤 벡터 $ z $는 잠재 공간(latent space)의 확률적 입력으로,  
  이를 **디코더(decoder)** 혹은 생성기 $ g_\theta $에 통과시켜 새로운 샘플(이미지)을 생성한다.  
- 즉,  

  $$
  \mathbf{x}_{\text{sample}} = g_\theta(\mathbf{z})
  $$

  의 형태로 새로운 데이터가 만들어진다.  

#### 3. 분류(classification) 문제와의 차이  
- 일반적인 분류 모델(classification model)은  
  학습 시점과 추론 시점의 입력–출력 구조가 거의 동일하다.  
- 반면 **생성 모델(generative model)**은  
  학습(Training)과 샘플링(Sampling)의 **파이프라인이 분리되어 있다는 점**이 큰 차이이다.  
  - 학습 단계에서는 데이터로부터 파라미터를 학습하고,  
  - 샘플링 단계에서는 확률적 입력으로부터 새로운 데이터를 생성한다.  
- 이러한 구조적 분리는 생성 모델의 핵심적인 특성이며,  
  **트레이닝 파이프라인과 생성 파이프라인이 서로 다른 형태로 존재한다는 점**을 이해하는 것이 중요하다.  

---

## p6. 간접 접근 (Indirect Approach)  

<img src="/assets/img/lecture/probstat/9/image_4.png" alt="image" width="800px">

---

### 강의 내용  

- **간접 접근(Indirect Approach)**은 데이터를 직접 생성하지 않고,  
  데이터의 “좋은 정도(goodness)”를 평가하는 **스코어링 함수(Scoring function)**를 학습하는 방식이다.  
- 이 스코어링 함수는 데이터의 **우도(likelihood)**, **에너지(energy)**, 혹은 **스코어(score)** 개념으로 해석될 수 있다.  

#### 1. 학습(Training) 단계  
- 주어진 데이터셋 $ \{ \mathbf{x}^{(i)} \}_{i=1}^{N} $ 을 이용해 **학습자(Learner)** 가  
  각 데이터의 위치에 대해 확률적 “품질”을 평가하는 함수를 학습한다.  
- 이 함수는 모델의 종류에 따라 다음과 같이 달라진다.  
  - **에너지 기반 모델(Energy-based model)**:  
    실제 데이터의 **에너지 $E_\theta(\mathbf{x})$를 낮추고**,  
    비현실적인 샘플의 에너지를 높이도록 학습한다.  
  - **스코어 기반 모델(Score-based model)**:  
    데이터의 로그 확률의 기울기, 즉 **스코어 함수 $\nabla_{\mathbf{x}} \log p_\theta(\mathbf{x})$**를 직접 근사하도록 학습한다.  
- 이 과정을 통해 모델은 “어떤 데이터가 더 자연스럽고 가능성 높은가”를 구분할 수 있는 함수를 얻게 된다.  


#### 2. 샘플링(Sampling) 단계  
- 학습된 스코어 함수나 에너지 함수를 활용하여  
  새로운 데이터를 생성하기 위해 **샘플링 알고리즘(sampling algorithm)**을 사용한다.  
- 대표적인 예로 **MCMC(Markov Chain Monte Carlo)** 방법이 있으며,  
  이는 스코어 함수가 높은 영역(즉, 가능성이 높은 데이터 영역)을 따라 새로운 샘플을 생성한다.  
- 수식적으로 표현하면,  

  $$
  \{ \hat{\mathbf{x}}^{(i)} \}_{i=1}^{N} \sim p_\theta(\mathbf{x}) \propto \exp(-E_\theta(\mathbf{x}))
  $$

  와 같이, 에너지 함수 $E_\theta(\mathbf{x})$가 낮을수록 샘플이 생성될 확률이 높아진다.  

#### 3. 직접 접근과의 비교  
- 직접 접근에서는 **랜덤 변수 $z$**를 입력으로 받아 데이터를 바로 생성하는 반면,  
  간접 접근은 **스코어 함수**를 학습한 후, 그 함수를 기반으로 **샘플링 알고리즘**을 통해 데이터를 생성한다.  
- 즉, 직접 접근은 “데이터를 바로 생성하는 함수”를 학습하는 것이고,  
  간접 접근은 “데이터가 얼마나 좋은지를 평가하는 함수”를 학습한 뒤  
  이 함수를 기반으로 데이터를 찾아내는 방식이다.  

- 이러한 접근 방식은 **디퓨전 모델(Diffusion Model)**이나  
  **에너지 기반 모델(Energy-based Model)** 등에서 핵심적인 역할을 하며,  
  뒤에서 배울 **Score-based Model**의 이론적 기반이 된다.  

---

## p7. 생성 모델 (Generative Models)  

목표는 학습 데이터를 그대로 복제하는 것이 아니라,  
**새로운(new)** 데이터를 만드는 것이다.  
그 데이터는 **현실적인(realistic)** 데이터여야 하며,  
실제 데이터의 **본질적인 속성(essential properties)**을 포착해야 한다.  

이것을 정량화하는 한 가지 방법은  
모델 하에서의 **테스트 데이터의 가능도(likelihood)**를 이용하는 것이다.  
(학습 데이터를 기억하는 모델은,  
분류기(classifier)가 과적합(overfit)되는 것과  
정확히 같은 의미에서 과적합된 것이다.)  

$$
\{x_{\text{test}}^{(i)}\}_{i=1}^{N}, \quad x_{\text{test}}^{(i)} \sim p_{\text{data}}
$$  

$$
\text{generalization error} = \sum_i \log p_\theta (x_{\text{test}}^{(i)})
$$  

---

### 강의 내용  

- 생성 모델의 목표는 학습 데이터를 그대로 복제하는 것이 아니라,  
  학습 데이터의 분포를 잘 포착하여 **그와 닮은(realistic)** 새로운 데이터를 생성하는 것이다.  

- 학습 데이터를 완벽히 재현하는 모델은  
  데이터를 단순히 암기한 것이며,  
  이는 분류기에서의 과적합(overfitting)과 동일하다.  

- 좋은 생성 모델은 학습 데이터 분포를 일반화(generalization)하여  
  보지 못한 데이터(test data)에 대해서도  
  높은 가능도(likelihood)를 부여할 수 있어야 한다.  

- GPT 같은 대형 언어 모델 또한 이러한 생성 모델의 예로,  
  기존 문장을 복사하지 않고  
  학습된 통계적·의미적 구조를 바탕으로 새로운 문장을 만들어낸다.  
  이때 새로운 데이터가 현실의 지식(real-world knowledge)과 잘 부합하면  
  “창의적 일반화(extrapolation)”로 볼 수 있지만,  
  현실과 어긋나면 **할루시네이션(hallucination)**으로 간주된다.  

- 따라서 생성 모델의 성능 평가는  
  “학습 데이터를 얼마나 잘 복제하는가?”가 아니라  
  “학습 데이터의 본질을 얼마나 잘 포착하고,  
   현실적인 새로운 데이터를 얼마나 잘 만들어내는가?”에 초점을 맞춘다.  

- 테스트 데이터의 로그 가능도(log-likelihood)는  
  이러한 일반화 능력을 수치로 평가하는 지표가 된다.  
  일반화 오차(generalization error)가 0이면  
  학습 데이터와 테스트 데이터가 완전히 일치한다는 뜻으로,  
  이는 과적합 상태를 의미한다.  
  반대로 오차가 너무 크면  
  학습된 분포를 벗어나 비현실적인 데이터를 생성할 수 있다.  

- 결국 생성 모델의 목표는  
  **현실적인(realistic) 데이터 생성**과 **새로운 정보 창출(extrapolation)** 사이의  
  균형을 유지하는 것이다.  

---

## p8. 밀도 기반 모델 (Density-based Models)  

<img src="/assets/img/lecture/probstat/9/image_5.png" alt="image" width="720px">

---

### 강의 내용  

- **밀도 기반 모델(Density-based Model)**은 데이터의 **확률 밀도(probability density)**를 직접 모델링하는 생성 모델이다.  
- 이때 확률 밀도 함수 $p_\theta(x)$는 모든 가능한 $x$에 대해 정의되며,  
  그 값은 0 이상 무한대($[0, \infty)$)의 범위를 가진다.  

- 학습 데이터 $$ \{x^{(i)}\}_{i=1}^{N} $$ 가 주어졌을 때,  
  모델은 이 데이터들이 나올 확률 밀도 $$p_\theta(x)$$를 최대화하도록 학습된다.  

- 즉, 모델이 학습 데이터 근처에서 높은 확률 밀도 값을 가지도록 만들고,  
  전체 확률 공간에 대해 밀도의 합(적분)이 1이 되도록 조정한다.  
  이것은 우리가 이전에 배웠던 **확률 밀도 함수의 성질**—  
  “전체 영역에서의 면적(합)은 1이 된다”—와 같은 원리이다.  

- 따라서 밀도 기반 모델의 핵심은  
  주어진 데이터 분포를 잘 근사하는 **연속적인 확률 밀도 함수 $p_\theta(x)$**를 학습하는 것이다.  

- 이러한 방식은 생성 모델을 평가할 때 사용되는 **우도(likelihood)** 기반 접근과 밀접한 관련이 있으며,  
  모델이 학습 데이터의 분포를 얼마나 잘 설명하는지를 수치적으로 평가할 수 있게 해준다.  

---

## p10. 밀도 기반 모델 (Density-based Models)  

<img src="/assets/img/lecture/probstat/9/image_6.png" alt="image" width="720px">

---

### 보충 설명  

- 그림은 확률 밀도 함수 $p_\theta(x)$의 형태를 보여준다.  
- **Constant mass(질량 보존)**은 전체 확률 질량(probability mass), 즉  
  확률 밀도의 적분값이 항상 1로 유지되어야 함을 의미한다.  
- 따라서 특정 구간에서 밀도가 높아지면,  
  다른 구간에서는 상대적으로 낮아져 전체 면적(확률의 총합)이 일정하게 유지된다.  
- 이 원리는 모든 확률 밀도 기반 생성 모델이 따라야 하는  
  기본적인 확률적 제약 조건이다.  

---

## p11. 밀도 기반 모델 (Density-based Models)  

<img src="/assets/img/lecture/probstat/9/image_7.png" alt="image" width="800px">

$$
\begin{aligned}
p_\theta^* 
&= \arg \min_{p_\theta} \text{KL}(p_{\text{data}}, p_\theta) \\
&= \arg \min_{p_\theta} \mathbb{E}_{x \sim p_{\text{data}}} 
   \left[- \log \frac{p_\theta(x)}{p_{\text{data}}(x)} \right] \\
&= \arg \max_{p_\theta} 
   \mathbb{E}_{x \sim p_{\text{data}}} [ \log p_\theta(x) ] 
   - \mathbb{E}_{x \sim p_{\text{data}}} [ \log p_{\text{data}}(x) ] \\
&= \arg \max_{p_\theta} 
   \mathbb{E}_{x \sim p_{\text{data}}} [ \log p_\theta(x) ]
   \quad \text{(두 번째 항은 } p_\theta \text{에 의존하지 않기 때문에 생략)} \\
&\approx \arg \max_{p_\theta} 
   \frac{1}{N} \sum_{i=1}^{N} \log p_\theta(x^{(i)})
\end{aligned}
$$  

**max likelihood**

---

### 강의 내용  

- 밀도 기반 모델은 데이터의 확률 밀도 함수 $p_\theta(x)$를 직접 추정(estimation)하는 생성 모델이다.  
- 학습의 목적은 **실제 데이터 분포 $p_{\text{data}}$와 모델 분포 $p_\theta$의 차이(KL divergence)**를 최소화하는 것이다.  

- 직관적으로 말하면,  
  - 데이터가 존재하는 구간에서는 확률 밀도(높이)를 **높게** 평가하고,  
  - 데이터가 존재하지 않는 구간에서는 확률 밀도를 **낮게** 평가하도록 학습된다.  
- 이렇게 함으로써 모델은 학습 데이터의 분포를 따라가는 형태로  
  확률 질량(probability mass)을 재분배하게 된다.  

- 오른쪽 그림에서 보듯이,  
  - 회색 곡선은 모델 분포 $p_\theta$를,  
  - 점과 초록색 화살표는 실제 데이터 분포 $p_{\text{data}}$를 나타낸다.  
  - 학습이 진행되면, 데이터가 있는 부분에서는 확률이 점점 증가하고 (초록색 영역),  
    데이터가 없는 부분에서는 확률이 감소한다 (빨간색 영역).  

- 이러한 최적화는 단 한 번에 이루어지는 것이 아니라,  
  **반복적인(iterative) 최대 우도 추정(maximum likelihood estimation)** 과정을 통해 점진적으로 이루어진다.  

- 수학적으로 보면, 최적의 분포 $p_\theta^*$는 다음과 같이  
  KL 발산을 최소화하는 분포로 정의된다.  

  $$
  p_\theta^* = \arg\min_{p_\theta} \text{KL}(p_{\text{data}} \,||\, p_\theta)
  $$

  이 식을 전개하면 결국 **로그 가능도(log-likelihood)**를 최대화하는 형태로 귀결된다.  

- 즉, **최대우도추정(Maximum Likelihood Estimation)**은  
  “데이터가 주어진 모델에서 나올 가능도를 가장 크게 만드는”  
  확률 밀도 함수를 찾는 과정이며,  
  이는 곧 KL 발산 최소화 문제와 동등하다.  

- 이러한 방식은 **에너지 기반 모델(Energy-based Model)**이나  
  **디퓨전 모델(Diffusion Model)**에서도 동일한 원리로 적용되며,  
  모델이 학습 데이터의 분포를 점진적으로 근사하도록 만든다.  
