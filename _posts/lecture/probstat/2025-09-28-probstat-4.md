---
layout: post
title: "[확률과 통계] 4주차"
date: 2025-09-28 14:01:00 +0900
categories:
  - "대학원 수업"
  - "확률과 통계"
tags: []
---

> 출처: 확률과 통계 – 박성우 교수님, 고려대학교 (2025)

## p2. 신경망을 학습시키는 방법?  

- **경사하강법(gradient descent), 확률적 경사하강법(SGD) 복습**  
- **계산 그래프(computation graphs)**  
- **사슬 구조에서의 역전파(backprop through chains)**  
- **다층 퍼셉트론(MLPs)에서의 역전파(backprop through MLPs)**  
- **유향 비순환 그래프(DAGs)에서의 역전파(backprop through DAGs)**  
- **미분 가능한 프로그래밍(differentiable programming)**  

---

## p3. 기본 원리? (Basic rationale?)

<img src="/assets/img/probstat/4/image_1.png" alt="image" width="720px">

---

### 보충 설명  

#### 1. **기본 원리(Basic rationale)**  
- 딥러닝의 목적은 데이터 인스턴스 $x^{(i)}$ 를 입력받아 올바른 분류 레이블 $y^{(i)}$ 를 출력하도록 모델 $f_\theta$ 를 학습하는 것이다.  
- 이를 위해 **손실 함수(loss)** $\mathcal{L}(f_\theta(x^{(i)}), y^{(i)})$ 를 정의하여, 예측과 실제 정답 사이의 차이를 수치로 측정한다.  
- 손실 함수는 보통 **거리 기반 함수**로 정의되며, “어떻게 정의하느냐”가 모델 학습 성능에 큰 영향을 준다.  

#### 2. **모델 파라미터와 학습 과정**  
- 각 레이어는 $\theta_1, \theta_2, \dots, \theta_6$ 등 **파라미터 집합**을 가진다.  
- 학습의 목표는 모든 데이터 인스턴스에 대해 손실의 합을 최소화하는 파라미터 $\theta^\ast$ 를 찾는 것이다.  

  $$
  \theta^\ast = \arg\min_{\theta} \sum_{i=1}^N \mathcal{L}\big(f_\theta(x^{(i)}), y^{(i)}\big)
  $$  

- 이 과정을 통해 모델은 데이터에 **가장 잘 맞는 매개변수**를 학습한다.  

#### 3. **Loss function과 Objective function의 관계**  
- **Loss function (손실 함수)**: 개별 데이터 인스턴스 $(x^{(i)}, y^{(i)})$ 에 대한 오차를 측정한다.  
  $$
  \mathcal{L}(f_\theta(x^{(i)}), y^{(i)})
  $$  
- **Objective function (목적 함수)**: 전체 데이터셋에 대해 손실을 합산/평균한 값으로, 실제로 최소화해야 하는 대상이다.  
  $$
  J(\theta) = \frac{1}{N}\sum_{i=1}^N \mathcal{L}(f_\theta(x^{(i)}), y^{(i)})
  $$  
- 따라서, 학습은 결국 **Objective function $J(\theta)$ 를 최소화하는 최적화 문제**로 귀결된다.  
- 손실 함수는 그 목적 함수를 구성하는 **기본 단위**라고 볼 수 있다.  

---

## p4. 최적화 (Optimization)  

<img src="/assets/img/probstat/4/image_2.png" alt="image" width="360px">

$$
\theta^\ast = \arg\min_{\theta} J(\theta)
$$  

**우리가 $J$에 대해 알고 있는 지식은 무엇인가?**  
- 우리는 $J(\theta)$ 를 계산할 수 있다.  
  → Black box optimization 
- 우리는 $J(\theta)$ 와 $\nabla_\theta J(\theta)$ 를 계산할 수 있다. ($\nabla_\theta$는 Gradient)  
  → First order optimization  
- 우리는 $J(\theta)$, $\nabla_\theta J(\theta)$, 그리고 $H_\theta(J(\theta))$ 를 계산할 수 있다. ($H_\theta$는 Hessian)  
  → Second order optimization  

---

### 보충 설명  

1. **블랙박스 최적화**  
- 내부 구조를 알지 못한 채 단순히 함수값 $J(\theta)$ 만을 이용하여 최소화를 시도하는 방식이다.  
- 신경망을 처음 보면 내부 계산 과정을 명확히 알 수 없다는 점에서 **black box optimization** 으로 이해할 수 있다.  

2. **1차 최적화 (First-order optimization)**  
- 함수값과 함께 **기울기(gradient)** $\nabla_\theta J(\theta)$ 를 사용할 수 있다.  
- 대표적으로 **경사하강법(gradient descent), 확률적 경사하강법(SGD)** 등이 이 범주에 속한다.  

3. **2차 최적화 (Second-order optimization)**  
- 함수값과 기울기뿐 아니라, **헤시안(Hessian)** $H_\theta(J(\theta))$ 까지 활용한다.  
- 곡률 정보를 이용해 더 정밀한 최적화가 가능하지만, 계산량이 매우 크다는 단점이 있다.  

---

## p5. 경사하강법 (Gradient Descent)  

<img src="/assets/img/probstat/4/image_3.png" alt="image" width="600px">

---

### 보충 설명  

#### 1. **경사하강법(Gradient Descent)의 기본 개념**  
- 목적 함수 $J(\theta)$ 를 최소화하기 위해 파라미터 $\theta$ 를 반복적으로 갱신하는 방법이다.  
- 현재 위치에서 기울기(gradient)의 반대 방향으로 이동하여 $J(\theta)$ 값을 줄인다.  

#### 2. **지역 최소값(Local minima)과 전역 최소값(Global minimum)**  
- 최적화 곡면은 여러 개의 골짜기를 가질 수 있다.  
- 이 중 일부는 **지역 최소값(local minima)** 으로, 더 낮은 값이 존재하더라도 그 안에 머무를 수 있다.  
- 반면 가장 낮은 지점은 **전역 최소값(global minimum)** 으로, 이론적으로는 도달해야 하는 목표점이다.  
- 실제로는 지역 최소값에 도달하는 경우가 대부분이며, 흔히 “local minima에 빠지는 게 99.9% 이상”이라고 말할 정도로 전역 최소값 도달은 드물다.  

#### 3. **파라미터 공간과 양자화(Quantization)**  
- 실제 학습에서 사용하는 파라미터 공간은 전체 범위를 다 활용하지 못하고, 비효율적으로 사용되는 경우가 많다.  
- 이런 상황에서 **양자화(quantization)** 기법을 적용하면 파라미터를 압축해 표현할 수 있으며, 계산 및 저장 효율을 크게 높일 수 있다.  

---

## p6. 확률적 경사하강법 (Stochastic Gradient Descent, SGD)  

- 전체 손실 함수 $J$ (각 샘플별 개별 손실의 합)를 최소화하고자 한다.  

- 확률적 경사하강법에서는 데이터의 부분집합(batch)에 대해 그래디언트를 계산한다.  
  - 배치 크기(batchsize)=1인 경우, 각 샘플마다 $\theta$ 가 갱신된다.  
  - 배치 크기(batchsize)=N (전체 집합)인 경우, 이는 표준 경사하강법(standard gradient descent)이 된다.  

- 그래디언트 방향은 모든 샘플에 대해 평균을 취한 경우(표준 경사하강법)에 비해 **노이즈가 많다(noisy)**.  

- **장점**  
  - 더 빠르다: 작은 샘플로 전체 그래디언트를 근사한다.  
  - 암묵적인 정규화 효과(implicit regularizer)가 있다.  

- **단점**  
  - 분산이 크고(high variance), 갱신이 불안정하다(unstable updates).  

---

### 보충 설명  

#### 1. **확률적(Stochastic)이라는 명칭의 이유**  
- 확률적 경사하강법(SGD)은 전체 데이터를 한 번에 학습하는 대신, **데이터를 작은 덩어리(chunk, mini-batch)** 로 나누어 학습한다.  
- 이때 어떤 chunk(샘플 집합)를 사용할지는 **확률적으로(random)** 선택된다.  
- 따라서 학습 과정에서 매번 다른 부분 집합을 기반으로 파라미터가 갱신되며, 이러한 무작위성이 반영되어 “Stochastic”이라는 이름이 붙는다.  

#### 2. **암묵적인 정규화 효과(Implicit Regularization)**  
- SGD는 전체 데이터에 대해 정확한 그래디언트를 계산하지 않고, 무작위로 선택된 작은 샘플 집합으로 근사한다.  
- 이 과정에서 갱신 방향에 노이즈가 섞이게 되는데, 이런 노이즈가 모델이 **특정 데이터에 과적합(overfitting) 되는 것을 방지하는 역할**을 한다.  
- 즉, 명시적으로 정규화 항(regularization term)을 추가하지 않아도, 확률적 샘플링으로 인한 변동성이 일종의 **규제(regularization)** 로 작용하여 일반화 성능을 높여준다.  

---

## p7. 모멘텀 (Momentum)  

- 언덕을 굴러 내려가는 무거운 공이 속도를 얻는 것과 같다.  
- 그래디언트 스텝은 이전 업데이트 방향을 계속 따르는 쪽으로 편향된다.  

$$
\theta^{t+1} \leftarrow \theta^t - \eta \nabla f(\theta^t) + \alpha \cdot m^t
$$  

- 도움을 줄 수도 있고, 방해가 될 수도 있다.  
- 모멘텀의 강도(strength of momentum)는 **하이퍼파라미터(hyperparameter)** 이다.  

<img src="/assets/img/probstat/4/image_4.png" alt="image" width="720px">  

---

### 보충 설명  

1. **모멘텀에 대한 직관적인 설명**  
- 단순한 경사하강법은 현재 기울기에만 의존해 파라미터를 이동시킨다.  
- 모멘텀을 적용하면 이전 단계의 이동 방향이 누적되어, 마치 공이 굴러가며 가속도를 얻는 것처럼 파라미터 갱신에 관성이 붙는다.  
- 이로 인해 **지역 최소값(local minima)** 에 갇히지 않고 벗어날 수 있는 가능성이 커진다.  

2. **수식 해석**  
- $\eta$ : 학습률(learning rate), 업데이트 크기를 조절한다.  
- $\nabla f(\theta^t)$ : 현재 시점에서의 그래디언트.  
- $\alpha \cdot m^t$ : 이전 업데이트에서 온 관성(모멘텀 항), $\alpha$ 가 클수록 더 큰 영향을 준다.  
- 결과적으로 이동 방향은 "현재 그래디언트"와 "과거의 업데이트"가 결합된 형태가 된다.  

3. **모멘텀의 효과**  
- **장점**: 진동(oscillation)을 줄이고, 평평한 지역을 빠르게 통과하며, 지역 최소값에서 빠져나오는 데 도움을 준다.  
- **단점**: 하이퍼파라미터 $\eta$ 와 $\alpha$ 를 적절히 조정하지 않으면 오히려 발산하거나 최적점 근처에서 크게 진동할 수 있다.  

4. **하이퍼파라미터로서의 $\alpha$의 의미**  
- 모멘텀의 강도 $\alpha$ 는 문제와 데이터셋에 따라 달라지는 값이며, 반드시 실험적으로 조정해야 한다.  
- $\alpha$는 학습률 $\eta$ 와 함께 모델의 수렴 속도와 안정성을 좌우하는 핵심 요소다.  

---

## p8. 예시: SGD의 동작  

<img src="/assets/img/probstat/4/image_5.png" alt="image" width="720px">  

- 스텝 크기 $\alpha = 0.02$  
- 모멘텀 $\beta = 0.99$  

모멘텀은 보통 진동을 억제하고 반복(iteration)의 속도를 높여 더 빠른 수렴을 가능하게 하는 수단으로 이해된다.  
하지만 모멘텀은 또 다른 흥미로운 동작을 보인다.  
더 넓은 범위의 스텝 크기를 사용할 수 있게 하고, 동시에 자체적인 진동을 만들어내기도 한다.  
그렇다면 어떤 일이 일어나고 있는 것일까?  

[참고 링크: <a href="https://distill.pub/2017/momentum/" target="_blank">https://distill.pub/2017/momentum/</a>]  

---

### 보충 설명  

1. **SGD의 이동 경로**  
- 그림에서 주황색 궤적은 확률적 경사하강법(SGD)이 출발점에서 해(solution)로 이동하는 과정을 나타낸다.  
- 단순 경사하강법은 불안정하거나 수렴이 느릴 수 있는데, 모멘텀을 적용하면 궤적이 달라진다.  

2. **모멘텀의 영향**  
- 모멘텀은 진동을 줄이고 더 빠른 방향으로 이동하게 만든다.  
- 그러나 동시에, 스텝 크기와 결합될 때 새로운 진동을 만들어낼 수도 있다.  
- 따라서 모멘텀은 단순히 학습 속도를 높이는 요소를 넘어서, **최적화 경로 자체를 바꾸는 요인**이 된다.  

3. **스텝 크기와 모멘텀의 조합**  
- 학습률(스텝 크기, $\alpha$)과 모멘텀($\beta$)은 서로 긴밀히 상호작용한다.  
- $\alpha$가 지나치게 크면 발산할 수 있고, $\beta$가 너무 크면 진동이 심해진다.  
- 적절한 조합을 선택하면 빠른 수렴과 안정성을 동시에 얻을 수 있다.  

---

## p9. 목적 함수의 예시 (손실 함수)  

<img src="/assets/img/probstat/4/image_6.png" alt="image" width="720px">  

---

### 보충 설명  

#### 1. (상단의 첫 번째 그림)  
- 목적 함수 $J(\theta)$ 가 매끄럽고 단일한 곡선 형태를 보이는 경우이다.  
- 전역 최소값(global minimum)을 안정적으로 찾을 수 있으며, 학습이 빠르고 안정적으로 수렴한다.  

#### 2. (상단의 두 번째 그림) Local minima  
- 여러 개의 골짜기를 가지는 경우, 전역 최소값이 아닌 지역 최소값(local minima)에 머무를 수 있다.  
- 이런 경우 더 좋은 해를 찾지 못하고 학습 성능이 제한될 수 있다.  

#### 3. (상단의 세 번째 그림) Vanishing gradient  
- 그래프가 상수 함수 형태를 보인다.  
- 상수 함수 $J(\theta) = c$ 를 미분하면  
  $$
  \nabla_\theta J(\theta) = 0
  $$  
  이므로 파라미터 업데이트가 전혀 일어나지 않는다.  
- 이로 인해 학습이 멈추게 된다.  

#### 4. (하단의 첫 번째 그림) Vanishing gradient (단계적)  
- 계단 함수 형태의 그래프에서는 특정 구간에서 기울기가 0이 된다.  
- 이런 경우에도 $\nabla_\theta J(\theta) = 0$ 이 되어 학습이 정체된다.  

#### 5. (하단의 두 번째 그림) Exploding gradient  
- 그래프 모양은 두 곡선이 아래쪽에서 만나면서 원점 근처에서 값이 급격히 커지는 형태이다.  
- 예를 들어 다음과 같은 함수가 있다:  
  $$
  J(\theta) = \frac{1}{|\theta|}
  $$  
- 이때 그래디언트는  
  $$
  \nabla_\theta J(\theta) 
  = \frac{d}{d\theta}\left(\frac{1}{|\theta|}\right) 
  = -\frac{\text{sgn}(\theta)}{\theta^2}
  $$  
  로 계산된다.  
- $\theta \to 0$ 으로 다가가면 분모 $\theta^2$ 가 0에 수렴하므로 그래디언트가 폭발적으로 커진다.  
- 이로 인해 파라미터 업데이트가 매우 불안정해지고, 학습이 발산하거나 최적점을 지나칠 수 있다.  

#### 6. (하단의 세 번째 그림)  
- 불연속적이거나 급격한 변화가 있는 그래프의 경우, 그래디언트 방향이 일관성을 가지지 못한다.  
- 이런 상황에서는 최적화가 불안정해지고, 수렴하기 어려워진다. 

---

## p10. 볼록 함수 (Convex function)  

<img src="/assets/img/probstat/4/image_7.png" alt="image" width="480px">   

단순한 경우:  
- 볼록(convex)  
- 단일 최소값(single minimum)  
- 모든 지점에서 그래디언트는 최소값을 향한다  
- 최소값에 가까워질수록 그래디언트가 점진적으로 0에 수렴한다  

---

### 보충 설명  

1. **볼록 함수의 특징**  
- 볼록 함수는 전체 영역에서 오직 하나의 최소값만 존재한다.  
- 따라서 최적화 과정에서 여러 지역 최소값(local minima)에 갇히는 문제가 발생하지 않는다.  

2. **그래디언트의 방향성**  
- 함수의 어느 지점에서나 그래디언트는 항상 최소값 쪽을 가리킨다.  
- 이 성질 덕분에 경사하강법은 단순하면서도 안정적으로 전역 최소값에 도달할 수 있다.  

3. **그래디언트의 크기 변화**  
- 최소값 근처에 접근할수록 그래디언트의 크기가 점차 줄어든다.  
- 이는 학습이 급격하게 요동치지 않고, 점진적으로 수렴하게 만드는 중요한 성질이다.  

---

## p11. 선형 불연속 함수 (구간별 선형 함수, Piecewise linear function)  

<img src="/assets/img/probstat/4/image_8.png" alt="image" width="480px"> 

불연속적(discontinuous):  
- 하지만 한쪽에서 정의된 편도 도함수(one-sided derivatives)가 존재한다.  
- PyTorch에서는 문제가 되지 않는다.  

---

### 보충 설명  

1. **구간별 선형 함수의 특징**  
- 함수가 여러 구간에서 서로 다른 직선으로 정의되어 있어, 특정 지점에서 불연속적일 수 있다.  
- 예를 들어 ReLU 계열 함수처럼 구간에 따라 기울기(그래디언트)가 달라지는 구조를 가질 수 있다.  

2. **편도 도함수의 정의 가능성**  
- 불연속 지점에서도 왼쪽과 오른쪽에서 접근하는 **편도 도함수(one-sided derivative)** 는 잘 정의된다.  
- 따라서 최적화 과정에서 해당 지점을 지나더라도 업데이트 방향을 정할 수 있다.  

3. **PyTorch와 같은 자동 미분 프레임워크의 처리 방식**  
- PyTorch는 구간별로 정의된 함수에 대해 편도 도함수를 활용해 자동으로 그래디언트를 계산한다.  
- 따라서 불연속적인 형태의 함수라도 실제 학습 과정에서는 문제가 되지 않는다.  

---

## p12. 그래디언트 폭발 (Exploding Gradient)  

<img src="/assets/img/probstat/4/image_9.png" alt="image" width="480px">  

Exploding gradient:  
- 최소점(minimizer)에 가까워질수록 그래디언트가 무한대로 발산한다.  
- 불안정한 업데이트와 overshoot(최적점을 지나쳐 버림)이 발생한다.  

**함수 $1/x$ 의 원점에서의 도함수는 무엇인가?**  

---

### 보충 설명  

1. **그래프의 형태**  
- $J(\theta)$ 가 두 곡선이 아래에서 만나면서 $\theta=0$ 근처에서 매우 가파르게 꺾인다.  
- 이 지점에서 그래디언트가 급격히 커지며 발산하는 것이 특징이다.  

2. **수학적 해석**  
- 예를 들어 $J(\theta) = \frac{1}{\theta}$ 라고 하면,  
  $$
  \nabla_\theta J(\theta) = -\frac{1}{\theta^2}
  $$  
- $\theta \to 0$ 에 가까워지면 분모가 0에 수렴하므로 그래디언트가 무한대로 커진다.  
- 이로 인해 학습 과정에서 파라미터가 큰 폭으로 갱신되면서 발산하거나 최적점을 지나쳐 버리는 문제가 생긴다.  

3. **학습 과정에서의 영향**  
- 그래디언트 폭발은 특히 심층 신경망에서 층(layer)의 곱이 누적되면서 발생할 수 있다.  
- 파라미터 업데이트가 지나치게 커져 네트워크가 안정적으로 학습하지 못하게 된다.  
- 이를 막기 위해 gradient clipping, 정규화, 적절한 가중치 초기화와 같은 기법이 사용된다.  

---

## p13. 손실 함수(loss function)에 중요한 것은 무엇인가?  

- **어디에서나 연속적(continuous)** 일 것  
- **어디에서나 미분 가능(differentiable)** 할 것  
- **어디에서나 매끄럽(smooth)** 게 변할 것  

---

이러한 성질들을 가진 **“가능한 최선의” 비선형성**을 우리는 어떻게 모델링할 수 있을까?  

---

## p14. 손실 함수(loss function)에 중요한 것은 무엇인가?   

- **어디에서나 연속적(continuous)** ✔️  
- **어디에서나 미분 가능(differentiable)** ✔️  
- **어디에서나 매끄럽다(smooth)** ✔️  

<img src="/assets/img/probstat/4/image_10.png" alt="image" width="240px">  

- $\Phi$ 는 임의의 매끄러운 함수(smoothing function)이다.  

---

### 보충 설명  

1. **GeLU의 정의**  
- GeLU(Gaussian Error Linear Unit)는 활성화 함수로, 입력 $z$ 에 대해 확률적 매끄러움(smoothing)을 적용한 형태이다.  
- 수식에서 $\Phi(z)$ 는 정규분포의 누적분포함수(CDF)로 주어지는 경우가 많다.  

2. **연속성, 미분 가능성, 매끄러움의 충족**  
- GeLU는 모든 구간에서 연속적이고 미분 가능하다.  
- 또한 곡선이 부드럽게 연결되어 있어 매끄러운(smooth) 성질을 만족한다.  

3. **반복 적용 시의 안정성**  
- 이러한 세 가지 성질(연속성, 미분 가능성, 매끄러움)을 동시에 만족하는 함수는, 여러 층(layer)에 반복 적용되더라도 해당 성질이 유지된다.  
- 따라서 GeLU는 딥러닝에서 안정적인 학습을 가능하게 하는 활성화 함수로 널리 활용된다.  

---

## p15. 계산 그래프 (Computation Graphs)  

<img src="/assets/img/probstat/4/image_11.png" alt="image" width="240px">  

- 함수적 변환들의 그래프, 즉 노드(□)로 이루어져 있으며, 이들이 연결되면 어떤 유용한 계산을 수행한다.  
- 딥러닝은 주로 **유향 비순환 그래프(directed acyclic graphs, DAGs)** 형태의 계산 그래프를 다루며, 각 노드는 미분 가능하다.  

---

### 보충 설명  

1. **계산 그래프의 개념**  
- 계산 그래프는 입력 데이터(예: 텐서)가 여러 연산을 거쳐 출력으로 변환되는 과정을 구조적으로 표현한 것이다.  
- 각 노드는 덧셈, 곱셈, 활성화 함수 등 특정 연산을 나타낸다.  

2. **DAG 구조의 특징**  
- 사이클이 없는 방향 그래프이므로 연산의 흐름이 입력에서 출력으로 일방향으로 진행된다.  
- 이 구조 덕분에 순전파(forward pass)와 역전파(backpropagation)를 체계적으로 수행할 수 있다.  

3. **프레임워크에서의 활용**  
- PyTorch, TensorFlow 같은 딥러닝 프레임워크는 모델을 정의하면 내부적으로 자동으로 계산 그래프를 생성한다.  
- 이를 통해 복잡한 미분 과정을 자동 미분(autograd)으로 처리할 수 있으며, 사용자는 직접 도함수를 계산할 필요가 없다.  

---

## p16. 계산 그래프 (Computation Graph)  

<img src="/assets/img/probstat/4/image_12.png" alt="image" width="720px">  

- 입력 $x$ 가 가중치 $W_1$ 과 결합되어 선형 변환을 거친 후 $z$ 가 된다.  
- $z$ 는 활성화 함수(ReLU)를 거쳐 은닉 표현 $h$ 가 된다.  
- $h$ 는 다시 가중치 $W_2$ 와 결합된 선형 변환을 거쳐 출력 $y$ 로 이어진다.  

---

### 보충 설명  

1. **중간 노드(Intermediate node) $z$**  
- $z$ 는 입력 $x$ 와 가중치 $W_1$ 를 곱하고 편향을 더한 뒤 얻어지는 중간 결과물이다.  
- 계산 그래프에서는 이러한 중간 노드가 이후 연산의 입력이 되며, 역전파 시 그래디언트 전파에도 중요한 역할을 한다.  

2. **계산 흐름의 구조**  
- 선형 변환(linear)과 비선형 활성화 함수(relu)가 번갈아 적용되면서 표현력이 강화된다.  
- 계산 그래프는 이를 순차적으로 보여주어, 입력에서 출력으로 이어지는 과정을 직관적으로 이해할 수 있게 한다.  

3. **계산 그래프의 해석**  
- 신경망(neural network)의 복잡한 연산 과정을 단순한 노드와 엣지의 조합으로 분해할 수 있다.  
- 이는 모델 구조를 분석하거나, 자동 미분으로 역전파를 수행할 때 핵심적인 틀이 된다.  

---

## p17. 순전파 (Forward pass)  

<img src="/assets/img/probstat/4/image_13.png" alt="image" width="360px"> 

---

### 보충 설명  

1. **순전파의 정의**  
- 순전파(forward pass)는 입력 데이터 $x_{\text{in}}$ 과 모델의 매개변수 $\theta$ 를 함수 $f$ 에 적용하여 출력 $x_{\text{out}}$ 을 얻는 과정이다.  
- 이는 신경망에서 예측을 생성하는 단계에 해당한다.  

2. **함수 $f$ 의 역할**  
- $f$ 는 선형 변환, 비선형 활성화 함수, 합성곱(convolution) 연산 등 다양한 연산으로 구성될 수 있다.  
- 계산 그래프 상에서는 일련의 노드와 연산으로 표현된다.  

3. **학습과의 연계성**  
- 순전파에서 얻은 $x_{\text{out}}$ 은 손실 함수(loss function)에 입력되어 모델의 성능을 측정한다.  
- 이후 역전파(backpropagation)를 통해 $\theta$ 에 대한 그래디언트가 계산되고, 파라미터 업데이트로 이어진다.  

---

## p18. 순전파: 다층 구조 (Forward Path : Multiple Layers)  

<img src="/assets/img/probstat/4/image_14.png" alt="image" width="720px">  

- 예를 들어, 이 계산 그래프(computation graph)는 다층 퍼셉트론(Multi-Layer Perceptron, MLP)을 나타낼 수 있다.   
 
주어진 계산 그래프 MLP는 “통계적 모델(statistical model)”의 좋은 예시이다.  

---

### 보충설명  

1. **순전파의 다층 구조**  
- 입력 $x_0$ 가 첫 번째 함수 $f_1$ 과 매개변수 $\theta_1$ 에 의해 변환되어 $x_1$ 이 된다.  
- 이후 $x_1$ 은 $f_2$ 와 $\theta_2$ 를 거쳐 $x_2$ 로 변환되고, 이런 과정이 $L$ 개의 층(layer)을 따라 반복된다.  
- 마지막 층의 출력 $x_L$ 은 손실 함수 $\mathcal{L}$ 에 입력되어 최종적인 목적 함수 $J$ 가 계산된다.  

2. **계산 그래프의 해석**  
- 각 함수 $f_i$ 는 선형 변환이나 비선형 활성화 함수를 포함할 수 있으며, $\theta_i$ 는 해당 층의 학습 가능한 매개변수이다.  
- 이 구조를 통해 입력에서 출력, 그리고 손실 함수에 이르는 전체 모델의 흐름이 명확히 드러난다.  

3. **MLP와 통계적 모델**  
- 다층 퍼셉트론(MLP)은 계산 그래프 구조를 가장 대표적으로 보여주는 신경망 모델이다.  
- 여러 층을 통해 특징을 변환하고, 확률적 또는 통계적 관점에서 해석 가능한 모델로 이어진다.  

---

## p19. 다층 신경망 학습 (Training Multi-layer Neural Networks)  

<img src="/assets/img/probstat/4/image_15.png" alt="image" width="720px">   

- 우리는 비용 함수 $J$ 의 그래디언트를 **모델 매개변수(model parameters)** 에 대해 계산해야 한다.  
- 설계상, 각 층은 자신의 입력(데이터와 매개변수)에 대해 미분 가능하도록 구성된다.  

---

### 보충 설명  

1. **그래디언트의 전파**  
- 최종 출력 $J$ (비용 함수)에서 그래디언트가 계산되고, 연쇄법칙(chain rule)에 의해 역전파된다.  
- 이 과정에서 각 층 $f_1, f_2, \dots, f_L$ 에 대응하는 매개변수 $\theta_1, \theta_2, \dots, \theta_L$ 에 대한 그래디언트가 계산된다.  

2. **비용 함수 표면에서의 최적화**  
- 오른쪽 그림은 비용 함수 $J(\theta)$ 의 표면을 나타내며, 매개변수 $\theta$ 가 반복(iteration)을 거치며 업데이트되는 과정을 보여준다.  
- 그래디언트는 현재 지점에서의 기울기를 바탕으로 파라미터가 이동할 방향과 크기를 결정한다.  

---

## p20. 벡터(행렬) 미적분학 요약  

- **x**: 크기가 $[n \times 1]$인 열 벡터(column vector)  

$$
\mathbf{x} =
\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}
$$

- 벡터 **x**에 대한 함수를 정의한다:  

$$
\mathbf{y} = f(\mathbf{x})
$$

- **$y$가 스칼라인 경우**  

$$
\frac{\partial y}{\partial \mathbf{x}}
=
\left(
\frac{\partial y}{\partial x_1},
\frac{\partial y}{\partial x_2},
\dots,
\frac{\partial y}{\partial x_n}
\right)
$$  

- → 이때 도함수는 크기가 $[1 \times n]$인 **행 벡터(row vector)** 가 된다.  

- **$\mathbf{y}$가 $[m \times 1]$ 벡터인 경우 (야코비안, Jacobian 표현식)**  

$$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}}
=
\begin{pmatrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{pmatrix}
$$  

- → 이때 도함수는 크기가 $[m \times n]$인 **행렬(matrix)** 이 된다.  
(행의 개수는 $m$, 열의 개수는 $n$)  

---

## p21. 벡터(행렬) 미적분학 요약  

- $y$가 스칼라(scalar)이고, **X**가 크기 $[n \times m]$인 행렬(matrix)이라면  

$$
\frac{\partial y}{\partial \mathbf{X}}
=
\begin{pmatrix}
\frac{\partial y}{\partial x_{11}} & \frac{\partial y}{\partial x_{21}} & \cdots & \frac{\partial y}{\partial x_{n1}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial y}{\partial x_{1m}} & \frac{\partial y}{\partial x_{2m}} & \cdots & \frac{\partial y}{\partial x_{nm}}
\end{pmatrix}
$$  

- 결과는 크기가 $[m \times n]$인 행렬(matrix)이다.  

---

## p22. 벡터(행렬) 미적분학 요약  

- 연쇄 법칙(Chain rule):  

  함수 $h(\mathbf{x}) = f(g(\mathbf{x}))$ 에 대하여,  
  그 도함수는 다음과 같다:  

  $$
  h'(\mathbf{x}) = f'(g(\mathbf{x})) g'(\mathbf{x})
  $$  

- $\mathbf{z} = f(\mathbf{u}), \ \mathbf{u} = g(\mathbf{x})$ 로 두면:  

  $$
  \left.\frac{\partial \mathbf{z}}{\partial \mathbf{x}}\right|_{\mathbf{x}=a}
  =
  \left.\frac{\partial \mathbf{z}}{\partial \mathbf{u}}\right|_{\mathbf{u}=g(a)}
  \cdot
  \left.\frac{\partial \mathbf{u}}{\partial \mathbf{x}}\right|_{\mathbf{x}=a}
  $$  

  - $\frac{\partial \mathbf{z}}{\partial \mathbf{x}} \in \mathbb{R}^{m \times n}$  
  - $\frac{\partial \mathbf{z}}{\partial \mathbf{u}} \in \mathbb{R}^{m \times p}$  
  - $\frac{\partial \mathbf{u}}{\partial \mathbf{x}} \in \mathbb{R}^{p \times n}$  

- 여기서 $p =$ 벡터 $\mathbf{u}$의 길이($\mid \mathbf{u} \mid$),  
  $m = \mid \mathbf{z} \mid$, $n = \mid \mathbf{x} \mid$.  

- 예시:  
  $\mid \mathbf{z} \mid = 1$, $\mid \mathbf{u} \mid = 2$, $\mid \mathbf{x} \mid = 4$ 라면  

<img src="/assets/img/probstat/4/image_16.png" alt="image" width="360px"> 

---

## p23. 연쇄 법칙(Chain Rule)에 의한 동적 프로그래밍  

<img src="/assets/img/probstat/4/image_17.png" alt="image" width="720px">  

<img src="/assets/img/probstat/4/image_18.png" alt="image" width="300px">  

- 우리는 연쇄 법칙을 사용하여 모든 도함수를 개별적으로 계산할 수 있다.  
- 그러나 회색 박스 안의 항들은 **공유**된다. 따라서 이 값은 한 번만 계산하면 된다.  
- **역전파(Backpropagation)** 는 계산 그래프 전체에서 공유되는 항들을 전파(propagating)하는 알고리즘이다.  

---

## p24. 정보의 순방향 / 역방향 (Forward / Backward direction of information)  

**Forward pass**  

네트워크를 통해 데이터를 순방향으로 전달하며, 출력을 계산하고 손실(loss)을 계산한다.  

<img src="/assets/img/probstat/4/image_19.png" alt="image" width="680px"> 

---

**Backward pass**  

출력과 손실로부터의 오차 신호(그래디언트)를 네트워크를 통해 역방향으로 전달하여, 입력과 매개변수로 되돌려 준다.  

<img src="/assets/img/probstat/4/image_20.png" alt="image" width="720px">  

---

## p25. 일반적인 층(Generic Layer)의 역전파  

<img src="/assets/img/probstat/4/image_21.png" alt="image" width="480px">  

---

- 우리는 두 종류의 편도함수 배열을 추적할 것이다:  
  - **L** : 층의 출력에 대한 층의 입력의 그래디언트 (행렬)  
  - **g** : 활성화(activation)에 대한 비용 함수의 그래디언트 (행 벡터)  

<img src="/assets/img/probstat/4/image_22.png" alt="image" width="360px"> 

---

## p26. 일반적인 층(Generic Layer)의 역전파 (계속)  

<img src="/assets/img/probstat/4/image_23.png" alt="image" width="480px"> 

---

- 한 층에 대해 **L** 과 **g** 를 알고 있다면, **매개변수 업데이트(parameter update)** 는 간단하다.  

<img src="/assets/img/probstat/4/image_24.png" alt="image" width="300px">

---

## p27. 일반적인 층(Generic Layer)의 역전파 (계속)  

<img src="/assets/img/probstat/4/image_25.png" alt="image" width="480px">  

---

- 그런데, 각 층에 대해 $L$ 과 $g$ 는 어떻게 구할 수 있을까?  

- **L** 은 해당 층의 도함수 함수 $f'$ 로부터 얻어진다 (주어진다고 가정함):  

<img src="/assets/img/probstat/4/image_26.png" alt="image" width="150px">

- **g** 는 다음의 점화식을 통해 반복적으로 계산될 수 있다:  

<img src="/assets/img/probstat/4/image_27.png" alt="image" width="150px"> 

---

- 오차 신호의 역전파(backpropagation of error signals)를 통해 그래디언트가 전달된다.  

---

## p28. 일반적인 층(Generic Layer)의 역전파  

<img src="/assets/img/probstat/4/image_28.png" alt="image" width="480px">   

---

- 이 모든 과정은 **매개변수 갱신 방향(parameter update directions)** 을 계산하기 위한 것이다.  

---

## p29. 모든 것을 종합하기 (Putting it all together)
  
**Forward**  
  
데이터를 네트워크를 따라 순방향으로 전달하여 출력과 손실(loss)을 계산한다.  

<img src="/assets/img/probstat/4/image_29.png" alt="image" width="680px">  

---

**Backward**  
  
출력과 손실로부터 발생한 **오차 신호(그래디언트)** 를 네트워크를 통해 역방향으로 전달한다.  

<img src="/assets/img/probstat/4/image_30.png" alt="image" width="720px">  

---

### Update:
- 매개변수 갱신  

<img src="/assets/img/probstat/4/image_31.png" alt="image" width="240px">  


그리고 이 과정을 반복한다.  

---

## p30. 일반적인 층(Generic Layer)에 대한 역전파  

<img src="/assets/img/probstat/4/image_32.png" alt="image" width="360px">  

- 학습하는 도중 층(layer) $l$은 세 개의 입력을 가진다:

<img src="/assets/img/probstat/4/image_33.png" alt="image" width="150px">  

---

- 그리고 세 개의 출력을 가진다:

<img src="/assets/img/probstat/4/image_34.png" alt="image" width="360px">  

---

- 주어진 입력이 있을 때, 우리는 다음을 계산하면 된다:  

<img src="/assets/img/probstat/4/image_35.png" alt="image" width="240px">  

---

## p31. 요약 (Summary)

<img src="/assets/img/probstat/4/image_36.png" alt="image" width="400px">  

---

1. **Forward pass**: 각 학습 예제에 대해, 모든 층의 출력을 계산한다.  

$$
x_l = f_l(x_{l-1}, \theta_l)
$$  

---

2. **Backward pass**: 위에서부터 아래로 순차적으로 손실 함수의 도함수를 계산한다.  

$$
\frac{\partial J}{\partial x_{l-1}} 
= \frac{\partial J}{\partial x_l} \cdot \frac{\partial f_l}{\partial x_{l-1}}
$$  

---

3. **Parameter update**: 가중치에 대한 그래디언트를 계산하고, 가중치를 갱신한다.  

$$
\frac{\partial J}{\partial \theta_l} 
= \frac{\partial J}{\partial x_l} \cdot \frac{\partial f_l}{\partial \theta_l}
$$  

---

## p32. 다층 퍼셉트론(MLP)? 장점과 단점

<img src="/assets/img/probstat/4/image_37.png" alt="image" width="500px">  

---

**장점**  
- **보편성 (Universal)**  
- **단순함, 우아한 이론 (Simple, elegant theory)**  
- **병렬화가 매우 쉬움 (Embarrassingly parallel)**  

---

**단점**  
- **약한 귀납적 편향 (Weak inductive biases)**  
- **샘플 비효율적, 데이터 많이 필요 (Sample inefficient / data hungry)**  
- **밀집된(fully-connected) 선형 계층**은 **많은 계산 자원**을 소모함  

---

## p33. 왜 다른 아키텍처(architecture)를 사용해야 하는가?  

<img src="/assets/img/probstat/4/image_38.png" alt="image" width="600px">  

---

### 보충 설명  

- **전체 회색 사각형의 의미**  
  입력 공간 $\mathcal{X}$ 에서 출력 공간 $\mathcal{Y}$ 로 가는 **모든 가능한 함수들의 집합**을 의미한다. 즉, 이 안에는 우리가 상상할 수 있는 모든 입력-출력 매핑이 포함되어 있다.  

- **초록색 타원의 의미**  
  주어진 **데이터를 설명할 수 있는 함수들의 집합**을 나타낸다. 이 영역 안의 함수들은 최소한 관찰된 데이터에는 들어맞는다.  

- **위 그림의 종합적인 의미**  
  실제로는 우리가 모든 가능한 함수(회색 영역)를 다 고려하는 것이 아니라, 데이터에 맞는 함수(초록색 영역) 중에서 학습 알고리즘이 하나의 해답(파란색 X)을 선택한다는 것을 보여준다. 하지만 선택된 해답이 참 해답(초록색 원)과 항상 일치하는 것은 아니다.  

---

## p34. 왜 다른 아키텍처(architecture)를 사용해야 하는가?  

<img src="/assets/img/probstat/4/image_39.png" alt="image" width="600px">  

---

### 보충 설명  

- **데이터의 역할**  
  학습에 사용되는 데이터가 많아질수록, 주어진 데이터와 잘 맞는 해답의 범위가 점점 좁아진다.  

- **그림의 의미**  
  33페이지에서 보여준 초록색 영역(Fits the data)이 더 작은 타원으로 표현되며,  
  이는 모델이 찾을 수 있는 해답의 불확실성이 줄어드는 과정을 나타낸다.  

---

## p35. 왜 다른 아키텍처(architecture)를 사용해야 하는가?  

<img src="/assets/img/probstat/4/image_40.png" alt="image" width="600px">  

---

데이터가 적을수록, 더 나은 아키텍처가 필요하다.  

우리는 참 해답에 더 가까워질 수 있다.  
- 방법 1: 더 많은 데이터를 추가하는 것  
- 방법 2: 더 제한적인 아키텍처를 사용하는 것  

---

### 보충 설명  

- **가설 공간(hypothesis space)** 은 모델이 만들 수 있는 함수들의 전체 집합을 뜻한다.  
- 초록색 타원은 **데이터와 일치하는 함수들**이고, 흰색 원 $\mathcal{F}$ 는 **현재 모델 구조(architecture)로 표현 가능한 함수들의 범위**다.  
- 두 영역이 겹친다는 것은: 데이터와 맞는 함수들 중에서 모델이 실제로 표현할 수 있는 후보가 존재한다는 뜻이다.  
- **더 제한적인 아키텍처**란, 모델이 표현할 수 있는 함수의 범위를 인위적으로 줄이는 것을 의미한다.  
  - 예: 모든 가능한 복잡한 함수 대신, **특정 구조(예: CNN의 합성곱 구조, RNN의 순차 구조)** 를 강제로 적용하는 것.  
- 이렇게 하면 학습된 해답(파란 X)이 불필요하게 넓은 공간에 퍼지지 않고, 참 해답(초록 O)에 더 가까운 위치로 제한될 수 있다.  

---

## p36. 일반화(Generalization)와 아키텍처(Architectures)  

<img src="/assets/img/probstat/4/image_41.png" alt="image" width="720px">  

- — <span style="color:green">**참 해답 (True solution)**</span>  
- — <span style="color:blue">**학습된 해답 (Learned solution)**</span>  
- ● **훈련 데이터 (Training data)**  

---

### 보충 설명  

#### 1. **일반화의 의미**  
- **일반화(generalization)** 는 훈련 데이터(●)에만 맞추는 것이 아니라, 새로운 입력 구간에서도 참 해답(— <span style="color:green">**True solution**</span>)에 가깝게 예측하는 능력을 뜻한다.  
- 학습된 해답(— <span style="color:blue">**Learned solution**</span>)이 초록색 참 해답과 얼마나 잘 일치하는지가 핵심이다.  

#### 2. **데이터 양의 영향**  
- 왼쪽 그래프: 데이터가 매우 적을 때는 모델이 단순한 직선 형태에 가까운 해답을 내며, 참 해답과 큰 차이가 난다.  
- 가운데 그래프: 데이터가 조금 늘어나면, 훈련 구간 근처에서는 참 해답과 더 비슷해지지만, 그 외 구간에서는 여전히 단순한 곡선에 머무른다.  
- 오른쪽 그래프: 데이터가 충분히 많아지면, 훈련 데이터 구간에서는 참 해답과 거의 일치한다. 그러나 **구간 밖에서는 여전히 직선 형태로 빗나간다.**  

#### 3. **아키텍처의 한계**  
- 여기서 사용한 5층 ReLU 네트워크는 데이터가 늘어날수록 훈련 구간 안에서는 개선되지만, **훈련 구간 밖에서는 참 해답을 일반화하지 못한다.**  
- 이는 단순히 데이터 양만 늘린다고 해서 완벽한 일반화를 얻을 수 없으며, **아키텍처 설계의 중요성**을 시사한다.  

---

## p37. 일반화(Generalization)와 아키텍처(Architecture)

<img src="/assets/img/probstat/4/image_42.png" alt="image" width="720px">  

- — <span style="color:green">**참 해답 (True solution)**</span>  
- — <span style="color:blue">**학습된 해답 (Learned solution)**</span>  
- ● **훈련 데이터 (Training data)**  

**아키텍처(Architecture)는 우리가 훈련 데이터 분포 밖(outside the training distribution)에서도 일반화(generalize)할 수 있도록 도와준다.**

**좋은 아키텍처(good architecture)**는  
- 참 함수를 잘 표현할 수 있어야 하고,  
- 불필요하게 복잡하지 않아야 하며,  
- 경사 기반 학습(gradient-based learning)에 의해 탐색이 용이해야 하고,  
- 병렬화(parallelization)에 적합하고, GPU에서 빠르게 동작할 수 있어야 한다.  

---

### 보충 설명  

#### 1. **36페이지와의 차이점**  
- 36페이지에서는 데이터가 많아지면 훈련 구간에서는 참 해답과 일치했지만, **훈련 데이터 밖에서는 단순 직선으로 크게 빗나갔다.**  
- 이는 아키텍처가 주어진 데이터 범위에만 맞추는 데 그쳤기 때문이다.  

#### 2. **이번 페이지(37p)의 특징**  
- 여기서는 학습 데이터 외 구간에서도 학습된 해답(파란색)이 참 해답(초록색)과 거의 완벽히 일치한다.  
- 즉, **아키텍처를 적절히 선택했기 때문에 데이터가 없는 구간에서도 올바른 함수 형태를 일반화할 수 있었다.**  

#### 3. **종합적 의미**  
- 충분한 데이터만으로는 일반화가 보장되지 않는다.  
- **좋은 아키텍처 선택이 일반화 성능을 크게 좌우한다**는 점을 그림을 통해 보여준다. 

---

## p38. 일반화(Generalization)와 아키텍처(Architecture)

<img src="/assets/img/probstat/4/image_43.png" alt="image" width="720px">  

- — <span style="color:green">**참 해답 (True solution)**</span>  
- — <span style="color:blue">**학습된 해답 (Learned solution)**</span>  
- ● **훈련 데이터 (Training data)**  

---

### 보충 설명

#### 1. **SIREN의 핵심 수식(사인 활성화)**
- 일반 MLP:  $h^{(\ell)} = \sigma \left(W^{(\ell)}h^{(\ell-1)} + b^{(\ell)}\right)$  
- **SIREN**:  $h^{(\ell)} = \sin \left(W^{(\ell)}h^{(\ell-1)} + b^{(\ell)}\right)$  

<img src="/assets/img/probstat/4/sigmoid_vs_siren.png" alt="image" width="720px"> 

- 실무에서는 첫 층에 주파수 스케일 $\omega_0$를 곱해 고주파 표현력을 높인다:  
  $$
  h^{(1)} = \sin\!\left(\omega_0 W^{(1)}x + b^{(1)}\right), \qquad 
  h^{(\ell)} = \sin\!\left(W^{(\ell)}h^{(\ell-1)} + b^{(\ell)}\right), \quad (\ell \ge 2)
  $$  
- 사인 함수는 연속이고 무한히 미분 가능하여 ($\frac{d}{dz}\sin z = \cos z$), 미분 기반 최적화와 역전파에서 안정적인 신호를 제공한다.  

#### 2. **왜 사인을 쓰나?**
- **주기적/진동적 패턴**과 **세밀한 연속 변화**를 자연스럽게 표현할 수 있다.  
- ReLU(조각별 선형)는 부드럽게 진동하는 참 함수를 근사하기 어려운 경우가 많으며, 이때 SIREN이 더 유리하다.  

#### 3. **그래프 해석(이 페이지)**  
- 데이터가 늘어날수록 학습된 해답(파란색)은 참 해답(초록색)의 **주기적 형태를 점진적으로 포착**한다.  
- 그러나 모든 구간에서 완전히 일치하지는 않고, 일부 영역에서는 차이가 남는다.  

#### 4. **비교 요약**
- **ReLU 기반(36p)** 보다 SIREN이 이번 데이터의 구조를 **더 잘 반영**한다.  
- 하지만 **37p의 아키텍처만큼** 이 데이터에 **완벽히 적합한 최적의 선택**은 아니다.  

---

## p39. 완전 연결 계층(Fully-connected Layer) vs 지역 연결 계층(Locally-connected Layer)

<img src="/assets/img/probstat/4/image_44.png" alt="image" width="720px">  

- **완전 연결 계층 (Fully-connected layer, fc layer)**  
  - 입력 $x$의 모든 요소가 가중치 행렬 $W$를 통해 모든 출력 노드와 연결된다.  
  - 편향 $b$가 더해져 중간 값 $z$가 계산되고, 이후 활성화 함수 $g(\cdot)$를 거쳐 최종 출력 $g(z)$를 얻는다.  

- **지역 연결 계층 (Locally-connected layer)**  
  - 입력 $x$의 일부 지역(local region)만 가중치 $w$와 연결된다.  
  - 동일한 가중치를 전체 입력에 공유하지 않고, 위치마다 다른 가중치를 사용한다.  
  - 편향 $b$가 더해져 $z$가 계산되고, 활성화 함수 $g(\cdot)$를 거쳐 출력 $g(z)$가 얻어진다.  

---

### 보충 설명  

#### 1. **완전 연결 계층의 특징 (Fully-connected layer)**  
- 모든 입력이 모든 출력과 연결되므로 **표현력(representational power)** 이 크다.  
- 하지만 매개변수(parameter) 수가 많아 계산량(computation cost)이 증가하며, **과적합(overfitting)** 위험이 크다.  

#### 2. **지역 연결 계층의 특징 (Locally-connected layer)**  
- 입력의 특정 위치 정보에 따라 연결이 제한되므로 **지역적 특성(local feature)** 을 반영할 수 있다.  
- 합성곱 계층(convolution layer)과 달리 **가중치 공유(weight sharing)** 를 하지 않는다.  
- 따라서 위치마다 다른 패턴을 학습할 수 있다는 장점이 있지만, 파라미터 수는 합성곱보다 많을 수 있다.  

---

## p40. 완전 연결 계층(Fully-connected Layer) vs 지역 연결 계층(Locally-connected Layer)

<img src="/assets/img/probstat/4/image_45.png" alt="image" width="400px">  

- 우리는 종종 출력(output)이 입력(input)의 **지역적(local)** 함수라고 가정한다.  
- 동일한 가중치(weights)를 사용하여 각 지역 함수(local function)를 계산한다면, 이를 **가중치 공유(weight sharing)** 라고 부른다.  
- 가중치 공유를 적용한 지역 연결 계층은 **합성곱 신경망(Convolutional Neural Network, CNN)** 이 된다.  

---

### 보충 설명  

#### 1. **지역적 함수(Local function)의 의미**  
- 지역 연결 계층은 입력 전체가 아닌 **일부 구간(local region)** 의 값만 사용하여 출력을 계산한다.  
- 예를 들어 이미지에서는 특정 픽셀 주변의 값들만 고려해 특징을 추출하는 것과 같다.  

#### 2. **가중치 공유(Weight sharing)의 역할**  
- 같은 가중치 집합(weight set)을 입력 전체에 반복적으로 적용한다.  
- 이렇게 하면 파라미터 수가 크게 줄어들고, 모델은 **위치와 무관하게 동일한 패턴을 인식**할 수 있게 된다.  

#### 3. **CNN과의 관계**  
- 지역 연결(Local connectivity) + 가중치 공유(Weight sharing) = 합성곱 신경망(CNN).  
- CNN은 이미지나 시계열 데이터처럼 **공간적 또는 시간적 구조(spatial/temporal structure)** 를 가진 데이터를 처리하는 데 특히 효과적이다.  

---

## p41. 합성곱 신경망(Convolutional Neural Network)

<img src="/assets/img/probstat/4/image_46.png" alt="image" width="400px">  

- **선형, 평행 이동 불변 변환(Linear, shift-invariant transformation)**  

<img src="/assets/img/probstat/4/image_47.png" alt="image" width="600px">  

---

### 보충 설명  

#### 1. **합성곱(Convolution)과 합성곱 계층(Conv layer)**  
- 합성곱 계층은 입력 $x$에 필터(filter) $w$를 적용하여 합성곱(convolution)을 수행한다.  
- 수식 $z = w \star x + b$ 에서 $\star$ 는 합성곱 연산을 의미한다.  

#### 2. **선형, 평행 이동 불변 변환(Linear, shift-invariant transformation)**  
- 변환은 선형(linear) 연산으로 이루어지며, 입력이 평행 이동(shift)하더라도 동일한 방식으로 처리된다.  
- 따라서 물체나 패턴이 위치만 달라져도 동일하게 인식될 수 있다.  

#### 3. **출력 수식(Output formula)**  
- $x_{\text{out}}[n,m]$ 은 입력 $x_{\text{in}}$과 필터 $w$의 합성곱 결과에 편향 $b$를 더해 계산된다.  

#### 4. **필터(Filter)의 의미**  
- 필터는 이미지나 입력 데이터에서 특정한 **지역적 패턴(local pattern)** 을 추출한다.  
- 예를 들어, 에지(edge)나 모서리(corner), 질감(texture) 같은 특징을 검출할 수 있다.  

#### 5. **물고기 그림의 의미**  
- 왼쪽의 컬러 물고기 이미지는 원본 입력이다.  
- 작은 회색 사각형은 적용된 **필터(filter)** 를 나타낸다.  
- 오른쪽 회색 영상은 필터가 입력에 적용된 결과로, 물고기의 **윤곽선(edge)** 이 강조되어 나타난다.  
- 이는 합성곱 계층이 특정 특징(feature)을 학습하고 추출하는 과정을 시각적으로 보여준다.  

---

## p42. 완전 연결 계층(Fully-Connected Layer)

<img src="/assets/img/probstat/4/image_48.png" alt="image" width="720px">  

- $x_{\text{out}} = W x_{\text{in}} + b$  
- $x_{\text{in}}$: 입력 벡터 (input vector)  
- $x_{\text{out}}$: 출력 벡터 (output vector)  
- $W$: 가중치 행렬 (weight matrix)  
- $b$: 편향 (bias)  

---

### 보충 설명  

#### 1. **행렬 연산으로서의 완전 연결 계층**  
- 완전 연결 계층은 입력 $x_{\text{in}}$ 과 가중치 행렬 $W$의 곱, 그리고 편향 $b$의 합으로 출력 $x_{\text{out}}$ 을 계산한다.  
- 즉, 다수의 입력과 출력 노드 간의 모든 연결을 하나의 **선형 변환(linear transformation)** 으로 표현할 수 있다.  

#### 2. **그래프 표현과 행렬 표현의 대응**  
- 오른쪽 그림은 모든 입력 노드와 출력 노드가 서로 연결된 형태를 보여준다.  
- 왼쪽의 행렬 표현은 같은 계산을 보다 간단히 나타낸 것으로, 실제 구현에서는 행렬 곱으로 효율적으로 계산된다.  

---

## p43. 합성곱 계층(Convolutional Layer)

<img src="/assets/img/probstat/4/image_49.png" alt="image" width="720px">  

- $x_{\text{out}} = w \star x_{\text{in}} + b$  
- $x_{\text{in}}$: 입력 벡터 (input vector)  
- $x_{\text{out}}$: 출력 벡터 (output vector)  
- $w[-1], w[0], w[1]$: 합성곱 필터 계수(convolution filter coefficients)  
- $b$: 편향 (bias)  

---

### 보충 설명  

#### 1. **합성곱(Convolution)과 합성곱 계층(Conv layer)**  
- 합성곱 계층은 입력 $x_{\text{in}}$의 **지역 구간(local region)** 에 필터 $w$를 적용하여 출력을 계산한다.  
- 수식 $x_{\text{out}} = w \star x_{\text{in}} + b$ 에서 $\star$ 는 합성곱(convolution) 연산을 나타낸다.  

#### 2. **가중치 공유(Weight sharing)**  
- 동일한 필터 계수 $w[-1], w[0], w[1]$ 이 입력 전체에 반복적으로 적용된다.  
- 이로 인해 파라미터 수가 줄고, 입력이 어디에 위치하든 동일한 패턴을 인식할 수 있게 된다.  

#### 3. **행렬 표현과 그래프 표현**  
- 왼쪽 행렬 그림은 합성곱이 입력에 대해 **대각선 방향으로 같은 필터가 적용**됨을 보여준다.  
- 오른쪽 신경망 그림은 동일한 가중치 집합이 여러 입력 구간에 적용되어 출력 노드로 연결되는 모습을 나타낸다.  
