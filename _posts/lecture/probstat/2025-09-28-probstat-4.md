---
layout: post
title: "[확률과 통계] 4주차"
date: 2025-09-28 14:01:00 +0900
categories:
  - "대학원 수업"
  - "확률과 통계"
tags: []
---

> 출처: 확률과 통계 – 박성우 교수님, 고려대학교 (2025)

## p2. 신경망을 학습시키는 방법?  

- **경사하강법(gradient descent), 확률적 경사하강법(SGD) 복습**  
- **계산 그래프(computation graphs)**  
- **사슬 구조에서의 역전파(backprop through chains)**  
- **다층 퍼셉트론(MLPs)에서의 역전파(backprop through MLPs)**  
- **유향 비순환 그래프(DAGs)에서의 역전파(backprop through DAGs)**  
- **미분 가능한 프로그래밍(differentiable programming)**  

---

## p3. 기본 원리? (Basic rationale?)

<img src="/assets/img/probstat/4/image_1.png" alt="image" width="720px">

---

### 보충설명  

#### 1. **기본 원리(Basic rationale)**  
- 딥러닝의 목적은 데이터 인스턴스 $x^{(i)}$ 를 입력받아 올바른 분류 레이블 $y^{(i)}$ 를 출력하도록 모델 $f_\theta$ 를 학습하는 것이다.  
- 이를 위해 **손실 함수(loss)** $\mathcal{L}(f_\theta(x^{(i)}), y^{(i)})$ 를 정의하여, 예측과 실제 정답 사이의 차이를 수치로 측정한다.  
- 손실 함수는 보통 **거리 기반 함수**로 정의되며, “어떻게 정의하느냐”가 모델 학습 성능에 큰 영향을 준다.  

#### 2. **모델 파라미터와 학습 과정**  
- 각 레이어는 $\theta_1, \theta_2, \dots, \theta_6$ 등 **파라미터 집합**을 가진다.  
- 학습의 목표는 모든 데이터 인스턴스에 대해 손실의 합을 최소화하는 파라미터 $\theta^\ast$ 를 찾는 것이다.  

  $$
  \theta^\ast = \arg\min_{\theta} \sum_{i=1}^N \mathcal{L}\big(f_\theta(x^{(i)}), y^{(i)}\big)
  $$  

- 이 과정을 통해 모델은 데이터에 **가장 잘 맞는 매개변수**를 학습한다.  

#### 3. **Loss function과 Objective function의 관계**  
- **Loss function (손실 함수)**: 개별 데이터 인스턴스 $(x^{(i)}, y^{(i)})$ 에 대한 오차를 측정한다.  
  $$
  \mathcal{L}(f_\theta(x^{(i)}), y^{(i)})
  $$  
- **Objective function (목적 함수)**: 전체 데이터셋에 대해 손실을 합산/평균한 값으로, 실제로 최소화해야 하는 대상이다.  
  $$
  J(\theta) = \frac{1}{N}\sum_{i=1}^N \mathcal{L}(f_\theta(x^{(i)}), y^{(i)})
  $$  
- 따라서, 학습은 결국 **Objective function $J(\theta)$ 를 최소화하는 최적화 문제**로 귀결된다.  
- 손실 함수는 그 목적 함수를 구성하는 **기본 단위**라고 볼 수 있다.  

---

## p4. 최적화 (Optimization)  

<img src="/assets/img/probstat/4/image_2.png" alt="image" width="360px">

$$
\theta^\ast = \arg\min_{\theta} J(\theta)
$$  

**우리가 $J$에 대해 알고 있는 지식은 무엇인가?**  
- 우리는 $J(\theta)$ 를 계산할 수 있다.  
  → Black box optimization 
- 우리는 $J(\theta)$ 와 $\nabla_\theta J(\theta)$ 를 계산할 수 있다. ($\nabla_\theta$는 Gradient)  
  → First order optimization  
- 우리는 $J(\theta)$, $\nabla_\theta J(\theta)$, 그리고 $H_\theta(J(\theta))$ 를 계산할 수 있다. ($H_\theta$는 Hessian)  
  → Second order optimization  

---

### 보충설명  

1. **블랙박스 최적화**  
- 내부 구조를 알지 못한 채 단순히 함수값 $J(\theta)$ 만을 이용하여 최소화를 시도하는 방식이다.  
- 신경망을 처음 보면 내부 계산 과정을 명확히 알 수 없다는 점에서 **black box optimization** 으로 이해할 수 있다.  

2. **1차 최적화 (First-order optimization)**  
- 함수값과 함께 **기울기(gradient)** $\nabla_\theta J(\theta)$ 를 사용할 수 있다.  
- 대표적으로 **경사하강법(gradient descent), 확률적 경사하강법(SGD)** 등이 이 범주에 속한다.  

3. **2차 최적화 (Second-order optimization)**  
- 함수값과 기울기뿐 아니라, **헤시안(Hessian)** $H_\theta(J(\theta))$ 까지 활용한다.  
- 곡률 정보를 이용해 더 정밀한 최적화가 가능하지만, 계산량이 매우 크다는 단점이 있다.  

---

## p5. 경사하강법 (Gradient Descent)  

<img src="/assets/img/probstat/4/image_3.png" alt="image" width="600px">

---

### 보충설명  

#### 1. **경사하강법(Gradient Descent)의 기본 개념**  
- 목적 함수 $J(\theta)$ 를 최소화하기 위해 파라미터 $\theta$ 를 반복적으로 갱신하는 방법이다.  
- 현재 위치에서 기울기(gradient)의 반대 방향으로 이동하여 $J(\theta)$ 값을 줄인다.  

#### 2. **지역 최소값(Local minima)과 전역 최소값(Global minimum)**  
- 최적화 곡면은 여러 개의 골짜기를 가질 수 있다.  
- 이 중 일부는 **지역 최소값(local minima)** 으로, 더 낮은 값이 존재하더라도 그 안에 머무를 수 있다.  
- 반면 가장 낮은 지점은 **전역 최소값(global minimum)** 으로, 이론적으로는 도달해야 하는 목표점이다.  
- 실제로는 지역 최소값에 도달하는 경우가 대부분이며, 흔히 “local minima에 빠지는 게 99.9% 이상”이라고 말할 정도로 전역 최소값 도달은 드물다.  

#### 3. **파라미터 공간과 양자화(Quantization)**  
- 실제 학습에서 사용하는 파라미터 공간은 전체 범위를 다 활용하지 못하고, 비효율적으로 사용되는 경우가 많다.  
- 이런 상황에서 **양자화(quantization)** 기법을 적용하면 파라미터를 압축해 표현할 수 있으며, 계산 및 저장 효율을 크게 높일 수 있다.  

---

## p6. 확률적 경사하강법 (Stochastic Gradient Descent, SGD)  

- 전체 손실 함수 $J$ (각 샘플별 개별 손실의 합)를 최소화하고자 한다.  

- 확률적 경사하강법에서는 데이터의 부분집합(batch)에 대해 그래디언트를 계산한다.  
  - 배치 크기(batchsize)=1인 경우, 각 샘플마다 $\theta$ 가 갱신된다.  
  - 배치 크기(batchsize)=N (전체 집합)인 경우, 이는 표준 경사하강법(standard gradient descent)이 된다.  

- 그래디언트 방향은 모든 샘플에 대해 평균을 취한 경우(표준 경사하강법)에 비해 **노이즈가 많다(noisy)**.  

- **장점**  
  - 더 빠르다: 작은 샘플로 전체 그래디언트를 근사한다.  
  - 암묵적인 정규화 효과(implicit regularizer)가 있다.  

- **단점**  
  - 분산이 크고(high variance), 갱신이 불안정하다(unstable updates).  

---

### 보충설명  

#### 1. **확률적(Stochastic)이라는 명칭의 이유**  
- 확률적 경사하강법(SGD)은 전체 데이터를 한 번에 학습하는 대신, **데이터를 작은 덩어리(chunk, mini-batch)** 로 나누어 학습한다.  
- 이때 어떤 chunk(샘플 집합)를 사용할지는 **확률적으로(random)** 선택된다.  
- 따라서 학습 과정에서 매번 다른 부분 집합을 기반으로 파라미터가 갱신되며, 이러한 무작위성이 반영되어 “Stochastic”이라는 이름이 붙는다.  

#### 2. **암묵적인 정규화 효과(Implicit Regularization)**  
- SGD는 전체 데이터에 대해 정확한 그래디언트를 계산하지 않고, 무작위로 선택된 작은 샘플 집합으로 근사한다.  
- 이 과정에서 갱신 방향에 노이즈가 섞이게 되는데, 이런 노이즈가 모델이 **특정 데이터에 과적합(overfitting) 되는 것을 방지하는 역할**을 한다.  
- 즉, 명시적으로 정규화 항(regularization term)을 추가하지 않아도, 확률적 샘플링으로 인한 변동성이 일종의 **규제(regularization)** 로 작용하여 일반화 성능을 높여준다.  

---

## p7. 모멘텀 (Momentum)  

- 언덕을 굴러 내려가는 무거운 공이 속도를 얻는 것과 같다.  
- 그래디언트 스텝은 이전 업데이트 방향을 계속 따르는 쪽으로 편향된다.  

$$
\theta^{t+1} \leftarrow \theta^t - \eta \nabla f(\theta^t) + \alpha \cdot m^t
$$  

- 도움을 줄 수도 있고, 방해가 될 수도 있다.  
- 모멘텀의 강도(strength of momentum)는 **하이퍼파라미터(hyperparameter)** 이다.  

<img src="/assets/img/probstat/4/image_4.png" alt="image" width="720px">  

---

### 보충설명  

1. **모멘텀에 대한 직관적인 설명**  
- 단순한 경사하강법은 현재 기울기에만 의존해 파라미터를 이동시킨다.  
- 모멘텀을 적용하면 이전 단계의 이동 방향이 누적되어, 마치 공이 굴러가며 가속도를 얻는 것처럼 파라미터 갱신에 관성이 붙는다.  
- 이로 인해 **지역 최소값(local minima)** 에 갇히지 않고 벗어날 수 있는 가능성이 커진다.  

2. **수식 해석**  
- $\eta$ : 학습률(learning rate), 업데이트 크기를 조절한다.  
- $\nabla f(\theta^t)$ : 현재 시점에서의 그래디언트.  
- $\alpha \cdot m^t$ : 이전 업데이트에서 온 관성(모멘텀 항), $\alpha$ 가 클수록 더 큰 영향을 준다.  
- 결과적으로 이동 방향은 "현재 그래디언트"와 "과거의 업데이트"가 결합된 형태가 된다.  

3. **모멘텀의 효과**  
- **장점**: 진동(oscillation)을 줄이고, 평평한 지역을 빠르게 통과하며, 지역 최소값에서 빠져나오는 데 도움을 준다.  
- **단점**: 하이퍼파라미터 $\eta$ 와 $\alpha$ 를 적절히 조정하지 않으면 오히려 발산하거나 최적점 근처에서 크게 진동할 수 있다.  

4. **하이퍼파라미터로서의 $\alpha$의 의미**  
- 모멘텀의 강도 $\alpha$ 는 문제와 데이터셋에 따라 달라지는 값이며, 반드시 실험적으로 조정해야 한다.  
- $\alpha$는 학습률 $\eta$ 와 함께 모델의 수렴 속도와 안정성을 좌우하는 핵심 요소다.  

---

## p8. 예시: SGD의 동작  

<img src="/assets/img/probstat/4/image_5.png" alt="image" width="720px">  

- 스텝 크기 $\alpha = 0.02$  
- 모멘텀 $\beta = 0.99$  

모멘텀은 보통 진동을 억제하고 반복(iteration)의 속도를 높여 더 빠른 수렴을 가능하게 하는 수단으로 이해된다.  
하지만 모멘텀은 또 다른 흥미로운 동작을 보인다.  
더 넓은 범위의 스텝 크기를 사용할 수 있게 하고, 동시에 자체적인 진동을 만들어내기도 한다.  
그렇다면 어떤 일이 일어나고 있는 것일까?  

[참고 링크: <a href="https://distill.pub/2017/momentum/" target="_blank">https://distill.pub/2017/momentum/</a>]  

---

### 보충설명  

1. **SGD의 이동 경로**  
- 그림에서 주황색 궤적은 확률적 경사하강법(SGD)이 출발점에서 해(solution)로 이동하는 과정을 나타낸다.  
- 단순 경사하강법은 불안정하거나 수렴이 느릴 수 있는데, 모멘텀을 적용하면 궤적이 달라진다.  

2. **모멘텀의 영향**  
- 모멘텀은 진동을 줄이고 더 빠른 방향으로 이동하게 만든다.  
- 그러나 동시에, 스텝 크기와 결합될 때 새로운 진동을 만들어낼 수도 있다.  
- 따라서 모멘텀은 단순히 학습 속도를 높이는 요소를 넘어서, **최적화 경로 자체를 바꾸는 요인**이 된다.  

3. **스텝 크기와 모멘텀의 조합**  
- 학습률(스텝 크기, $\alpha$)과 모멘텀($\beta$)은 서로 긴밀히 상호작용한다.  
- $\alpha$가 지나치게 크면 발산할 수 있고, $\beta$가 너무 크면 진동이 심해진다.  
- 적절한 조합을 선택하면 빠른 수렴과 안정성을 동시에 얻을 수 있다.  

---

## p9. 목적 함수의 예시 (손실 함수)  

<img src="/assets/img/probstat/4/image_6.png" alt="image" width="720px">  

### 보충설명  

#### 1. (상단의 첫 번째 그림)  
- 목적 함수 $J(\theta)$ 가 매끄럽고 단일한 곡선 형태를 보이는 경우이다.  
- 전역 최소값(global minimum)을 안정적으로 찾을 수 있으며, 학습이 빠르고 안정적으로 수렴한다.  

#### 2. (상단의 두 번째 그림) Local minima  
- 여러 개의 골짜기를 가지는 경우, 전역 최소값이 아닌 지역 최소값(local minima)에 머무를 수 있다.  
- 이런 경우 더 좋은 해를 찾지 못하고 학습 성능이 제한될 수 있다.  

#### 3. (상단의 세 번째 그림) Vanishing gradient  
- 그래프가 상수 함수 형태를 보인다.  
- 상수 함수 $J(\theta) = c$ 를 미분하면  
  $$
  \nabla_\theta J(\theta) = 0
  $$  
  이므로 파라미터 업데이트가 전혀 일어나지 않는다.  
- 이로 인해 학습이 멈추게 된다.  

#### 4. (하단의 첫 번째 그림) Vanishing gradient (단계적)  
- 계단 함수 형태의 그래프에서는 특정 구간에서 기울기가 0이 된다.  
- 이런 경우에도 $\nabla_\theta J(\theta) = 0$ 이 되어 학습이 정체된다.  

#### 5. (하단의 두 번째 그림) Exploding gradient  
- 그래프 모양은 두 곡선이 아래쪽에서 만나면서 원점 근처에서 값이 급격히 커지는 형태이다.  
- 예를 들어 다음과 같은 함수가 있다:  
  $$
  J(\theta) = \frac{1}{|\theta|}
  $$  
- 이때 그래디언트는  
  $$
  \nabla_\theta J(\theta) 
  = \frac{d}{d\theta}\left(\frac{1}{|\theta|}\right) 
  = -\frac{\text{sgn}(\theta)}{\theta^2}
  $$  
  로 계산된다.  
- $\theta \to 0$ 으로 다가가면 분모 $\theta^2$ 가 0에 수렴하므로 그래디언트가 폭발적으로 커진다.  
- 이로 인해 파라미터 업데이트가 매우 불안정해지고, 학습이 발산하거나 최적점을 지나칠 수 있다.  

#### 6. (하단의 세 번째 그림)  
- 불연속적이거나 급격한 변화가 있는 그래프의 경우, 그래디언트 방향이 일관성을 가지지 못한다.  
- 이런 상황에서는 최적화가 불안정해지고, 수렴하기 어려워진다. 

---

## p10. 볼록 함수 (Convex function)  

<img src="/assets/img/probstat/4/image_7.png" alt="image" width="480px">   

단순한 경우:  
- 볼록(convex)  
- 단일 최소값(single minimum)  
- 모든 지점에서 그래디언트는 최소값을 향한다  
- 최소값에 가까워질수록 그래디언트가 점진적으로 0에 수렴한다  

---

### 보충설명  

1. **볼록 함수의 특징**  
- 볼록 함수는 전체 영역에서 오직 하나의 최소값만 존재한다.  
- 따라서 최적화 과정에서 여러 지역 최소값(local minima)에 갇히는 문제가 발생하지 않는다.  

2. **그래디언트의 방향성**  
- 함수의 어느 지점에서나 그래디언트는 항상 최소값 쪽을 가리킨다.  
- 이 성질 덕분에 경사하강법은 단순하면서도 안정적으로 전역 최소값에 도달할 수 있다.  

3. **그래디언트의 크기 변화**  
- 최소값 근처에 접근할수록 그래디언트의 크기가 점차 줄어든다.  
- 이는 학습이 급격하게 요동치지 않고, 점진적으로 수렴하게 만드는 중요한 성질이다.  

---

## p11. 선형 불연속 함수 (구간별 선형 함수, Piecewise linear function)  

<img src="/assets/img/probstat/4/image_8.png" alt="image" width="480px"> 

불연속적(discontinuous):  
- 하지만 한쪽에서 정의된 편도 도함수(one-sided derivatives)가 존재한다.  
- PyTorch에서는 문제가 되지 않는다.  

---

### 보충설명  

1. **구간별 선형 함수의 특징**  
- 함수가 여러 구간에서 서로 다른 직선으로 정의되어 있어, 특정 지점에서 불연속적일 수 있다.  
- 예를 들어 ReLU 계열 함수처럼 구간에 따라 기울기(그래디언트)가 달라지는 구조를 가질 수 있다.  

2. **편도 도함수의 정의 가능성**  
- 불연속 지점에서도 왼쪽과 오른쪽에서 접근하는 **편도 도함수(one-sided derivative)** 는 잘 정의된다.  
- 따라서 최적화 과정에서 해당 지점을 지나더라도 업데이트 방향을 정할 수 있다.  

3. **PyTorch와 같은 자동 미분 프레임워크의 처리 방식**  
- PyTorch는 구간별로 정의된 함수에 대해 편도 도함수를 활용해 자동으로 그래디언트를 계산한다.  
- 따라서 불연속적인 형태의 함수라도 실제 학습 과정에서는 문제가 되지 않는다.  

---

## p12. 그래디언트 폭발 (Exploding Gradient)  

<img src="/assets/img/probstat/4/image_9.png" alt="image" width="480px">  

Exploding gradient:  
- 최소점(minimizer)에 가까워질수록 그래디언트가 무한대로 발산한다.  
- 불안정한 업데이트와 overshoot(최적점을 지나쳐 버림)이 발생한다.  

**함수 $1/x$ 의 원점에서의 도함수는 무엇인가?**  

---

### 보충설명  

1. **그래프의 형태**  
- $J(\theta)$ 가 두 곡선이 아래에서 만나면서 $\theta=0$ 근처에서 매우 가파르게 꺾인다.  
- 이 지점에서 그래디언트가 급격히 커지며 발산하는 것이 특징이다.  

2. **수학적 해석**  
- 예를 들어 $J(\theta) = \frac{1}{\theta}$ 라고 하면,  
  $$
  \nabla_\theta J(\theta) = -\frac{1}{\theta^2}
  $$  
- $\theta \to 0$ 에 가까워지면 분모가 0에 수렴하므로 그래디언트가 무한대로 커진다.  
- 이로 인해 학습 과정에서 파라미터가 큰 폭으로 갱신되면서 발산하거나 최적점을 지나쳐 버리는 문제가 생긴다.  

3. **학습 과정에서의 영향**  
- 그래디언트 폭발은 특히 심층 신경망에서 층(layer)의 곱이 누적되면서 발생할 수 있다.  
- 파라미터 업데이트가 지나치게 커져 네트워크가 안정적으로 학습하지 못하게 된다.  
- 이를 막기 위해 gradient clipping, 정규화, 적절한 가중치 초기화와 같은 기법이 사용된다.  

---

## p13. 손실 함수(loss function)에 중요한 것은 무엇인가?  

- **어디에서나 연속적(continuous)** 일 것  
- **어디에서나 미분 가능(differentiable)** 할 것  
- **어디에서나 매끄럽(smooth)** 게 변할 것  

---

이러한 성질들을 가진 **“가능한 최선의” 비선형성**을 우리는 어떻게 모델링할 수 있을까?  

---

## p14. 손실 함수(loss function)에 중요한 것은 무엇인가?   

- **어디에서나 연속적(continuous)** ✔️  
- **어디에서나 미분 가능(differentiable)** ✔️  
- **어디에서나 매끄럽다(smooth)** ✔️  

<img src="/assets/img/probstat/4/image_10.png" alt="image" width="240px">  

- $\Phi$ 는 임의의 매끄러운 함수(smoothing function)이다.  

---

### 보충설명  

1. **GeLU의 정의**  
- GeLU(Gaussian Error Linear Unit)는 활성화 함수로, 입력 $z$ 에 대해 확률적 매끄러움(smoothing)을 적용한 형태이다.  
- 수식에서 $\Phi(z)$ 는 정규분포의 누적분포함수(CDF)로 주어지는 경우가 많다.  

2. **연속성, 미분 가능성, 매끄러움의 충족**  
- GeLU는 모든 구간에서 연속적이고 미분 가능하다.  
- 또한 곡선이 부드럽게 연결되어 있어 매끄러운(smooth) 성질을 만족한다.  

3. **반복 적용 시의 안정성**  
- 이러한 세 가지 성질(연속성, 미분 가능성, 매끄러움)을 동시에 만족하는 함수는, 여러 층(layer)에 반복 적용되더라도 해당 성질이 유지된다.  
- 따라서 GeLU는 딥러닝에서 안정적인 학습을 가능하게 하는 활성화 함수로 널리 활용된다.  

---

## p15. 계산 그래프 (Computation Graphs)  

<img src="/assets/img/probstat/4/image_11.png" alt="image" width="240px">  

- 함수적 변환들의 그래프, 즉 노드(□)로 이루어져 있으며, 이들이 연결되면 어떤 유용한 계산을 수행한다.  
- 딥러닝은 주로 **유향 비순환 그래프(directed acyclic graphs, DAGs)** 형태의 계산 그래프를 다루며, 각 노드는 미분 가능하다.  

---

### 보충설명  

1. **계산 그래프의 개념**  
- 계산 그래프는 입력 데이터(예: 텐서)가 여러 연산을 거쳐 출력으로 변환되는 과정을 구조적으로 표현한 것이다.  
- 각 노드는 덧셈, 곱셈, 활성화 함수 등 특정 연산을 나타낸다.  

2. **DAG 구조의 특징**  
- 사이클이 없는 방향 그래프이므로 연산의 흐름이 입력에서 출력으로 일방향으로 진행된다.  
- 이 구조 덕분에 순전파(forward pass)와 역전파(backpropagation)를 체계적으로 수행할 수 있다.  

3. **프레임워크에서의 활용**  
- PyTorch, TensorFlow 같은 딥러닝 프레임워크는 모델을 정의하면 내부적으로 자동으로 계산 그래프를 생성한다.  
- 이를 통해 복잡한 미분 과정을 자동 미분(autograd)으로 처리할 수 있으며, 사용자는 직접 도함수를 계산할 필요가 없다.  
