---
layout: post
title: "[확률과 통계] 12주차"
date: 2025-11-12 11:00:00 +0900
categories:
  - "대학원 수업"
  - "확률과 통계"
tags: []
---

> 출처: 확률과 통계 – 박성우 교수님, 고려대학교 (2025)

## p2. Review : Image Classification  

<img src="/assets/img/lecture/probstat/12/image_1.png" alt="image" width="600px">

> **강의 내용**  
> 
> 먼저 리뷰를 해봅시다.  
> 여러분들은 아마 이미지 classification이 어떤 건지에 대해서는 다 알고 계실 거라고 생각을 해요.  
> 이미지 classification 같은 경우는  
> 인풋의 이미지, 뭐 3D 이미지가 될 수도 있고 2D 이미지가 될 수도 있고  
> 이미지 형태의 데이터가 들어왔을 때 classifier가 존재해서,  
> classifier가 최종적으로 레이블을 아웃풋으로 만들어 내는 거  
> 그게 이미지 classification 모델의 정의였습니다.  
> classifier라고 하는 뉴럴 네트워크가  
> 이 주어진 이미지를 컨볼루셔널한 오퍼레이션을 통해  
> 실질적으로 레이블이 어떻게 되는지에 대해서 평가를 하는 거죠.  
> 그래서 이 레이블은 랭귀지 토큰이 될 수도 있고  
> 전통적인 classification 모델이라고 한다면 원핫 벡터라고 해서  
> 예를 들어서 클래스가 천 개가 있다고 하면 그 천 차원짜리의 벡터에  
> 각각의 해당되는 컴포넌트만 1이고 나머지 부분은 0인  
> 원핫 벡터를 통해 가지고 classification을 하곤 합니다.

---

## p3. Review : Image Generation

<img src="/assets/img/lecture/probstat/12/image_2.png" alt="image" width="600px">  

> **강의 내용**  
> 
> 그러면 이제 이미지 제네레이션이라는 게 무엇이냐 하면  
> 이전 시간에 우리가 VAE를 통해 가지고 어느 정도 맛을 봤지만 그것은 언컨디셔널한 거였죠.  
> 가우시안 랜덤 베리어블을 가지고 디코더를 통해 생성하는 게 VAE의 목적이었는데  
> 예를 들어서 컨디셔널한 제네레이션을 생각을 해보죠.  
> Fish라는 랭귀지 토큰 혹은 원핫 벡터가 존재할 때  
> 제네레이터가 그 레이블 인포메이션을 가지고 그것에 해당하는 어떤 이미지를 생성해야 되는 겁니다.  
> 그래서 우리가 하는 생성 모델, 특히 멀티모달 생성 모델들은  
> 이미지 classification의 역순에 해당하는,  
> 그 프로세스를 역으로 돌리는 것에 해당하는 거다 라고 생각하시면 될 것 같고.  
> 이제 여기에서 중요한 것은 뉴럴 네트워크 스트럭처라고 하죠.  
> classifier 같은 경우는 큰 이미지를 컨볼루션 레이어를 통해  
> 점차적으로 피쳐맵들이 작아지면서 최종적으로 레이블을 뱉게 되거든요.  
> 그런데 VAE 스트럭처를 깊게 살펴보시면 이해를 하실 거라 생각을 하는데  
> 디코더도 마찬가지로 그렇게 생겼습니다. 역순으로 생겼습니다.  
> 디코더 스트럭처에 이런 레이블이 들어가거나 레이턴트 인포메이션이 들어가면  
> 작은 정보들을 좀 더 키워나가면서 최종적으로 이미지 resolution에 맞게  
> 그렇게 뉴럴 네트워크가 구성이 되어 있었죠.  
> 그런 부분들이 classification과 제네레이션의 차이이고  
> 그렇기 때문에 이런 구조상의 특징점으로 인해서 제너레이터 뉴럴 네트워크 아키텍쳐가 구성이 됐다.  
> 이런 히스토리컬한 컨텍스트가 있다고 이해를 해주시면 될 것 같습니다.

---

## p4. Review : Image Generation

- 고차원 미관측 변수들(high-dimensional unobserved variables)의 모델  $P(\mathbf{X} \mid \mathbf{Y} = y)$  

- 무작위 이미지를 샘플링하는 것 이상의 많은 문제들에 유용하다!

---

## p5. Review : Generative Model

<img src="/assets/img/lecture/probstat/12/image_3.png" alt="image" width="600px">

---

## p6. Review : Conditional Generative Model

<img src="/assets/img/lecture/probstat/12/image_4.png" alt="image" width="530px">

---

## p7. Review : Conditional Generative Model

<img src="/assets/img/lecture/probstat/12/image_5.png" alt="image" width="600px">

---

## p8. Review : Conditional Generative Model

<img src="/assets/img/lecture/probstat/12/image_6.png" alt="image" width="570px">

---

## p9. Data Preparation in Conditional Generation

<img src="/assets/img/lecture/probstat/12/image_7.png" alt="image" width="720px">

---

## p10. Challenges?

1. 출력은 고차원(high-dimensional)이며, 구조화된 객체(structured object)이다.

    <img src="/assets/img/lecture/probstat/12/image_8.png" alt="image" width="200px">

2. 매핑(mapping)에 불확실성이 존재하며, 가능한 출력들이 많다.

    <img src="/assets/img/lecture/probstat/12/image_9.png" alt="image" width="200px">

---

## p11. Property of Generative Models?

1. 고차원적이고 구조화된 출력을 모델링한다.

    <img src="/assets/img/lecture/probstat/12/image_8.png" alt="image" width="200px">

2. 불확실성을 모델링한다; 가능한 출력들의 전체 분포를 모델링한다.  

    <img src="/assets/img/lecture/probstat/12/image_9.png" alt="image" width="200px">

---

## p12. Image-to-Image Translation

<img src="/assets/img/lecture/probstat/12/image_10.png" alt="image" width="600px">

---

## p13. Image-to-Image Translation

<img src="/assets/img/lecture/probstat/12/image_11.png" alt="image" width="540px">

---

## p14. Image-to-Image Translation : Objective Function

<img src="/assets/img/lecture/probstat/12/image_12.png" alt="image" width="720px">

---

## p15. Image-to-Image Translation : Objective Function

<img src="/assets/img/lecture/probstat/12/image_13.png" alt="image" width="720px">

---

## p16. Motivation : GANs

<img src="/assets/img/lecture/probstat/12/image_14.png" alt="image" width="720px">

---

## p17. Actor-critic Perspective

<img src="/assets/img/lecture/probstat/12/image_15.png" alt="image" width="600px">

---

## p18. Role of discriminator : critic

<img src="/assets/img/lecture/probstat/12/image_16.png" alt="image" width="600px">

---

## p19. Role of generator : actor

<img src="/assets/img/lecture/probstat/12/image_17.png" alt="image" width="600px">

---

## p20. Min-max game : Game theory

<img src="/assets/img/lecture/probstat/12/image_18.png" alt="image" width="600px">

---

## p21. Min-max game : Game theory

<img src="/assets/img/lecture/probstat/12/image_19.png" alt="image" width="600px">

---

## p22. Conditional GANs

<img src="/assets/img/lecture/probstat/12/image_20.png" alt="image" width="640px">

---

## p23. Conditional GANs

<img src="/assets/img/lecture/probstat/12/image_21.png" alt="image" width="600px">

---

## p24. Conditional GANs for Image Translation

<img src="/assets/img/lecture/probstat/12/image_22.png" alt="image" width="600px">

---

## p25. Conditional GANs

<img src="/assets/img/lecture/probstat/12/image_23.png" alt="image" width="800px">

---

## p26. Edges2Cats

<img src="/assets/img/lecture/probstat/12/image_24.png" alt="image" width="600px">

---

## p27. Pix2Pix : Labels to Facades

<img src="/assets/img/lecture/probstat/12/image_25.png" alt="image" width="600px">

---

## p28. GANs vs VAEs

<img src="/assets/img/lecture/probstat/12/image_26.png" alt="image" width="600px">

---

## p29. GANs vs VAEs

<img src="/assets/img/lecture/probstat/12/image_27.png" alt="image" width="800px">

---

## p30. Theory : Global Convergence of Critic

<img src="/assets/img/lecture/probstat/12/image_28.png" alt="image" width="720px">

---

> **명제 1.**  G가 고정되었을 때, 최적의 판별기 D는  
> 
> > $$D_G^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}$$  
> 
> **증명**  
> 
> > $$V(G, D) = \int_x p_{\text{data}}(x) \log(D(x)) \, dx + \int_z p_z(z) \log(1 - D(g(z))) \, dz$$  
> 
> > $$= \int_x p_{\text{data}}(x) \log(D(x)) + p_g(x) \log(1 - D(x)) \, dx$$  
> 
> 임의의 $(a, b) \in \mathbb{R}^2 \setminus \{0, 0\}$ 에 대해,  
> 함수 $y \mapsto a\log(y) + b\log(1-y)$ 는  
> $[0,1]$ 에서 $\frac{a}{a+b}$ 일 때 최대값을 가진다.  
> 판별기(discriminator)는  
> $\text{Supp}(p_{\text{data}}) \cup \text{Supp}(p_g)$ 밖에서 정의될 필요가 없으며,  
> 이것으로 증명이 완료된다. □
>
> ---
>
> 이 정리는 “G(Generator)를 고정했을 때, 어떤 D가 가장 좋은가?”를 알려준다.  
> 즉, 생성 모델 G가 이미 정해져 있다면,  
> 판별기 D는 어떤 값을 출력해야  
> 전체 GAN 목적함수 $V(G, D)$ 가 최대가 되는지를  
> **정확히 계산한 결과**이다.  
>
> 핵심 아이디어는 다음과 같다.  
>
> > 1) GAN의 목적함수 $V(G,D)$ 는  
> >    데이터 분포와 생성 분포가 함께 등장하는  
> >    로그 형태의 합이다.  
>
> > 2) 이 적분을 $x$별로 바라보면,  
> >    각 $x$에 대해  
> >    $p_{\text{data}}(x)\log(D(x)) + p_g(x)\log(1 - D(x))$  
> >    를 최대화하는 문제가 된다.  
>
> > 3) 이는 일반적인 형태  
> >    $a\log(y) + b\log(1-y)$  
> >    의 최대화 문제와 동일하다.  
>
> > 4) 이 함수는 $y = \frac{a}{a+b}$ 일 때 최대가 된다.  
>
> 여기서 $a = p_{\text{data}}(x)$, $b = p_g(x)$ 로 두면  
> 각 $x$마다 최적의 판별기 출력은  
>
> > $$D^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}$$  
>
> 이 된다.  
>
> 즉, **판별기의 최적 출력은  
> “이 $x$가 진짜 데이터일 확률의 정규화된 비율”** 이다.  
> 다시 말해, 데이터와 생성 분포의 상대적 확률비로  
> 진짜 여부를 판단하는 것이 최적이라는 뜻이다.  
>
> 마지막 문장에 따르면,  
> 판별기는 $p_{\text{data}}$ 또는 $p_g$ 가 0인 구간에서는  
> 정의될 필요가 없다.  
>
> > “두 분포가 실제로 존재하는 영역(지원집합)에서만 의미가 있으며,  
> > 그 바깥에서는 $V(G,D)$ 에 아무 영향도 없다.”  
>
> 이렇게 해서 최적 판별기 공식이 도출되며,  
> 이는 **GAN 이론의 가장 핵심적인 기반 공식**이다.

---

## p31. Theory : Global Convergence of Distribution

<img src="/assets/img/lecture/probstat/12/image_29.png" alt="image" width="600px">

---

## p32. Theory : Global Convergence of Distribution

<img src="/assets/img/lecture/probstat/12/image_30.png" alt="image" width="640px">

---

## p33. Theory : Global Convergence of Distribution

<img src="/assets/img/lecture/probstat/12/image_31.png" alt="image" width="600px">