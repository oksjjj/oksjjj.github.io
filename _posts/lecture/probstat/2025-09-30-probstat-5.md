---
layout: post
title: "[확률과 통계] 5주차"
date: 2025-09-30 20:00:00 +0900
categories:
  - "대학원 수업"
  - "확률과 통계"
tags: []
---

> 출처: 확률과 통계 – 박성우 교수님, 고려대학교 (2025)

## p2. 단변량 가우시안 분포 (Univariate Gaussian distribution)  

<img src="/assets/img/probstat/5/image_1.png" alt="image" width="600px">

---

### 보충 설명  

#### 1. **그래프 해석**  
- 세로축: **표본 값(Instance Value)**  
- 이 그래프는 평균이 $\mu = 1$, 표준편차가 $\sqrt{3}$ 인 단변량 가우시안 확률변수(Univariate Gaussian RV)에서 생성된 여러 표본(instance)들을 보여준다.  
- 표본들은 평균 주변에 밀집하는 경향을 가지며, 표준편차가 클수록 더 넓게 퍼져 분포한다.  

---

## p3. 다변량 가우시안 분포 (Multivariate Gaussian distribution)  

<img src="/assets/img/probstat/5/image_2.png" alt="image" width="720px">

---

### 보충 설명  

#### 1. **수식 해석**  
- 다변량 가우시안 분포는 평균 벡터 $\vec{\mu}$ 와 공분산 행렬 $\Sigma$로 정의된다.  
- 확률밀도함수는 다음과 같다.  

$$
p_{\vec{X}}(\vec{x}) = \frac{1}{(2\pi)^{D/2} |\Sigma|^{1/2}} 
e^{-\tfrac{1}{2} (\vec{x} - \vec{\mu})^T \Sigma^{-1} (\vec{x} - \vec{\mu})}
$$  

#### 2. **가우시안 랜덤 벡터 (Gaussian random vector)**  
- $\vec{x}$ 는 $D$차원 확률변수로, 각 성분이 $x_0, \dots, x_{D-1}$ 로 이루어진다.  
- 평균 벡터 $\vec{\mu} = E[\vec{x}]$ 는 각 차원별 평균 $\mu_0, \dots, \mu_{D-1}$ 로 구성된다.  

#### 3. **그림 해석**  
- 왼쪽 그림의 파란 점들은 평균이 $\mu = [1,1]$ 인 2차원 가우시안 분포에서 생성된 표본(instance)들이다.  
- 빨간 십자가는 평균 벡터 $\mu$를 나타내며, 점들이 그 주변에 퍼져 있다.  
- 점들이 분포하는 방향과 퍼짐 정도는 공분산 행렬 $\Sigma$에 의해 결정된다.  

---

## p4. 다변량 가우시안 분포 (Multivariate Gaussian distribution)  

<img src="/assets/img/probstat/5/image_3.png" alt="image" width="720px">

---

### 보충 설명  

#### 1. **그림 해석**  
- 왼쪽 그림의 파란 점들은 평균이 $\mu = [1,1]$ 인 2차원 가우시안 분포에서 추출된 표본(instance)들이다.  
- 빨간 십자가는 평균 벡터 $\mu$를 나타내며, 점들이 그 주변에 퍼져 있다.  

#### 2. **공분산 행렬 $\Sigma$**  
- $\Sigma$는 각 차원의 분산과 서로 다른 차원 간의 공분산을 포함하는 행렬이다.  
- 대각 원소 $\sigma_i^2$ 는 차원 $x_i$의 분산을 의미한다.  
- 비대각 원소 $\rho_{ij}$ 는 $x_i$와 $x_j$ 사이의 공분산을 의미한다.  

#### 3. **수식의 의미**  
- $\rho_{ij} = E[(x_i - \mu_i)(x_j - \mu_j)]$ : 두 변수 $x_i, x_j$의 공분산을 정의한다.  
- $\sigma_i^2 = E[(x_i - \mu_i)^2]$ : 각 변수 $x_i$의 분산을 정의한다.  

---

## p5. 최대우도추정 (Maximum Likelihood Estimation, MLE)  

**최대우도 매개변수 추정 (Maximum Likelihood Parameter Estimation)**  

- 실제 세계에서는 우리는 $\vec{\mu}$ 와 $\Sigma$를 알지 못한다.  
- 만약 학습 데이터베이스 $$D = \{\vec{x}_0, \dots, \vec{x}_{M-1}\}$$ 가 있다면,  
  $\vec{\mu}$ 와 $\Sigma$를 다음과 같이 추정할 수 있다.  

$$
\{\hat{\mu}_{ML}, \hat{\Sigma}_{ML}\} 
= \arg\max \prod_{m=0}^{M-1} p(\vec{x}_m | \vec{\mu}, \Sigma)
$$  

$$
= \arg\max \sum_{m=0}^{M-1} \ln p(\vec{x}_m | \vec{\mu}, \Sigma)
$$  

---

<img src="/assets/img/probstat/5/image_4.png" alt="image" width="500px">

---

### 보충 설명  

#### 1. **수식의 의미**  
- 위 수식은 주어진 데이터 집합 $D$가 발생할 가능성을 최대로 하는 평균 $\vec{\mu}$와 공분산 $\Sigma$를 찾는 문제를 나타낸다.  
- 첫 번째 식은 모든 표본 확률의 곱(우도, likelihood)을 최대로 만드는 파라미터를 구한다는 의미이다.  
- 두 번째 식은 로그를 취해 계산을 단순화한 것으로, **로그-우도(log-likelihood)** 를 최대로 하는 파라미터를 구한다.  

#### 2. **그림 해석**  
- 오른쪽 그림에서 파란 점들은 표본(instance) $\vec{x}_m$ 을 의미한다.  
- 각 화살표는 $\vec{x}_m - \vec{\mu}$ 로, 이는 각 표본과 평균 벡터 사이의 차이 벡터를 나타낸다.  
- MLE는 이 차이 벡터들의 분포를 가장 잘 설명할 수 있는 평균 $\vec{\mu}$와 공분산 $\Sigma$를 찾는 방식으로 작동한다.  
- 즉, 화살표들이 모이는 중심이 평균 $\vec{\mu}$이고, 화살표들의 퍼짐과 방향성이 공분산 $\Sigma$에 반영된다.  

---

## p6. 대수의 법칙 / 통계적 추정량 (LLN / Statistical Estimator)  

**최대우도 매개변수 추정 (Maximum Likelihood Parameter Estimation)**  

- 이전 슬라이드의 오른쪽 항(RHS)을 미분하고 이를 0으로 두면,  
  최대우도 해는 다음과 같이 된다.  

$$
\hat{\mu}_{ML} = \frac{1}{M} \sum_{m=0}^{M-1} \vec{x}_m
$$  

$$
\hat{\Sigma}_{ML} = \frac{1}{M} \sum_{m=0}^{M-1} (\vec{x}_m - \vec{\mu})(\vec{x}_m - \vec{\mu})^T
$$  

---

<img src="/assets/img/probstat/5/image_4.png" alt="image" width="500px">

---

### 보충 설명  

#### 1. **로그-우도(log-likelihood)의 전개**  
- $M$개의 표본 $$\{\vec{x}_m\}_{m=0}^{M-1}$$ 이 다변량 정규분포 $$\mathcal{N}(\vec{\mu}, \Sigma)$$ 에서 독립적으로 추출되었다고 하자.  
- 한 표본의 확률밀도는  

$$
p(\vec{x}_m|\vec{\mu},\Sigma) 
= \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} 
  \exp\!\left(-\tfrac{1}{2}(\vec{x}_m-\vec{\mu})^T \Sigma^{-1} (\vec{x}_m-\vec{\mu})\right).
$$  

- 전체 데이터셋의 우도(likelihood)는  

$$
L(\vec{\mu},\Sigma) = \prod_{m=0}^{M-1} p(\vec{x}_m|\vec{\mu},\Sigma).
$$  

- 로그를 취하면 (곱이 합으로 바뀜):  

$$
\ell(\vec{\mu},\Sigma) = \ln L(\vec{\mu},\Sigma) 
= -\frac{M}{2}\ln|2\pi\Sigma| 
-\frac{1}{2}\sum_{m=0}^{M-1} (\vec{x}_m-\vec{\mu})^T \Sigma^{-1} (\vec{x}_m-\vec{\mu}).
$$  

- 여기서 $-\tfrac{MD}{2}\ln(2\pi)$ 항은 상수이므로, **최적화와 무관하여 const 로 묶는다.**  

$$
\ell(\vec{\mu},\Sigma) 
= -\frac{M}{2}\ln|\Sigma| 
-\frac{1}{2}\sum_{m=0}^{M-1} (\vec{x}_m-\vec{\mu})^T \Sigma^{-1} (\vec{x}_m-\vec{\mu}) + \text{const}.
$$  

#### 2. **평균 추정치의 도출**  
- $\vec{\mu}$에 대해 미분:  

$$
\frac{\partial \ell}{\partial \vec{\mu}}
= \Sigma^{-1}\!\left(\sum_{m=0}^{M-1}\vec{x}_m - M\vec{\mu}\right).
$$  

이를 0으로 두면  

$$
\hat{\mu}_{ML} = \frac{1}{M}\sum_{m=0}^{M-1}\vec{x}_m.
$$  

#### 3. **공분산 추정치의 도출**  
- 산포행렬 $S = \sum_{m=0}^{M-1} (\vec{x}_m-\vec{\mu})(\vec{x}_m-\vec{\mu})^T$ 로 두면  

$$
\ell(\vec{\mu},\Sigma) = -\frac{M}{2}\ln|\Sigma| - \frac{1}{2}\mathrm{tr}(\Sigma^{-1}S) + \text{const}.
$$  

- $\Sigma$에 대해 미분 후 0으로 두면  

$$
S = M\Sigma \quad \Rightarrow \quad
\hat{\Sigma}_{ML} = \frac{1}{M} \sum_{m=0}^{M-1} (\vec{x}_m-\hat{\mu}_{ML})(\vec{x}_m-\hat{\mu}_{ML})^T.
$$  

#### 4. **그림 해석 (MLE와의 관련성)**  
- 그림에서 각 파란 점은 표본 $$\vec{x}_m$$ 을 의미하고, 빨간 점은 평균 추정치 $$\hat{\mu}_{ML}$$ 이다.  
- 각 화살표 $$(\vec{x}_m - \hat{\mu}_{ML})$$ 는 표본과 평균 간의 차이를 나타낸다.  
- MLE는 **이 차이 벡터들의 분포를 가장 잘 설명하는 평균과 공분산을 찾는 과정**이다.  
  - 평균 $$\hat{\mu}_{ML}$$ 은 모든 화살표 길이의 합이 0이 되도록 하는 중심점이다.  
  - 공분산 $$\hat{\Sigma}_{ML}$$ 은 화살표들이 퍼지는 방향과 크기를 반영하여, 분포의 타원 모양(등확률 곡선)을 결정한다.  
- 따라서 그림 속에서 빨간 점이 중심으로 잡히고, 화살표들이 보여주는 퍼짐 정도가 $$\hat{\Sigma}_{ML}$$ 로 표현되는 것이 바로 MLE의 결과이다.  

---

## p7. 대수의 법칙 / 통계적 추정량 (LLN / Statistical Estimator)  

**표본평균 (Sample Mean), 표본공분산 (Sample Covariance)**  

- $\Sigma$의 최대우도추정(MLE)은 보통 실제보다 작게 나온다.  
- 따라서 이를 약간 조정하는 것이 더 낫다.  
- 다음은 $\vec{\mu}$ 와 $\Sigma$의 **불편추정량(unbiased estimators)** 으로,  
  각각 **표본평균(sample mean)** 과 **표본공분산(sample covariance)** 이라고 부른다.  

$$
\vec{\mu} = \frac{1}{M} \sum_{m=0}^{M-1} \vec{x}_m
$$  

$$
\Sigma = \frac{1}{M-1} \sum_{m=0}^{M-1} (\vec{x}_m - \vec{\mu})(\vec{x}_m - \vec{\mu})^T
$$  

---

<img src="/assets/img/probstat/5/image_4.png" alt="image" width="500px">

---

### 보충 설명  

#### 1. **왜 분모가 $M-1$ 이어야 하는가?**  

- MLE 방식의 표본분산은  

$$
s^2_{MLE} = \frac{1}{M}\sum_{i=1}^M (x_i - \bar{x})^2
$$  

이고, 이를 전개하면  

$$
\sum_{i=1}^M (x_i - \bar{x})^2 
= \sum_{i=1}^M (x_i - \mu)^2 - M(\bar{x}-\mu)^2.
$$  

따라서  

$$
s^2_{MLE} = \frac{1}{M}\Bigg(\sum_{i=1}^M (x_i - \mu)^2 - M(\bar{x}-\mu)^2\Bigg).
$$  

이제 기대값을 취하면  

$$
E[s^2_{MLE}] 
= \frac{1}{M}\Bigg(\sum_{i=1}^M E[(x_i-\mu)^2] - M E[(\bar{x}-\mu)^2]\Bigg).
$$  

여기서 $E[(x_i-\mu)^2] = \sigma^2$ 이므로,  
결국 $E[(\bar{x}-\mu)^2]$ 의 계산이 핵심이다.  

---

##### **보충 설명: 왜 $E[(\bar{x}-\mu)^2] = \dfrac{\sigma^2}{M}$ 인가?**  

- 표본평균의 정의  

$$
\bar{x} = \frac{1}{M}\sum_{i=1}^M x_i
$$  

- 분산의 기본 성질  

  - $\mathrm{Var}(aX) = a^2 \mathrm{Var}(X)$  
  - 독립이면 $\mathrm{Var}(X+Y) = \mathrm{Var}(X)+\mathrm{Var}(Y)$  

- 표본평균의 분산  

$$
\mathrm{Var}(\bar{x})
= \mathrm{Var}\left(\frac{1}{M}\sum_{i=1}^M x_i\right)
= \frac{1}{M^2}\mathrm{Var}\Big(\sum_{i=1}^M x_i\Big)
= \frac{1}{M^2}(M\sigma^2)
= \frac{\sigma^2}{M}.
$$  

- 분산의 정의에 따라  

$$
E[(\bar{x}-\mu)^2] = \mathrm{Var}(\bar{x}) = \frac{\sigma^2}{M}.
$$  

즉, 표본평균은 모집단 평균 $\mu$를 중심으로 분산이 $\sigma^2/M$ 인 분포를 가진다.  

---

따라서,  

$$
E[s^2_{MLE}] = \frac{1}{M}\left(M\sigma^2 - M\cdot \frac{\sigma^2}{M}\right)
= \frac{M-1}{M}\sigma^2.
$$  

이는 항상 $\sigma^2$ 보다 작다.  

이를 보정하기 위해 $M$ 대신 $M-1$ 로 나누면,  

$$
s^2 = \frac{1}{M-1}\sum_{i=1}^M (x_i-\bar{x})^2
$$  

이다.  

앞에서 얻은 결과에 따르면  

$$
E\!\left[\sum_{i=1}^M (x_i-\bar{x})^2\right] = (M-1)\sigma^2
$$  

임을 알 수 있다.  

따라서 분모를 $M-1$ 로 두면  

$$
E[s^2] = E\!\left[\frac{1}{M-1}\sum_{i=1}^M (x_i-\bar{x})^2\right]
= \frac{1}{M-1}(M-1)\sigma^2
= \sigma^2
$$  

가 되어, 표본분산 $s^2$ 는 실제 분산 $\sigma^2$ 와 같은 기대값을 갖는 **불편(unbiased) 추정량**이 된다.   

#### 2. **그림 해석**  
- 파란 점들은 표본(instance)이고, 빨간 점은 표본평균 $\vec{\mu}$ 를 나타낸다.  
- 각 화살표는 차이 벡터 $(\vec{x}_m - \vec{\mu})$ 이다.  

- 이 벡터를 자기 자신과 곱하면 (외적, outer product)  

$$
(\vec{x}_m - \vec{\mu})(\vec{x}_m - \vec{\mu})^T
$$  

라는 **행렬**이 된다.  

- 이 행렬은 단일 표본이 분산과 공분산에 얼마나 기여하는지를 나타낸다.  
  - 대각 원소는 각 좌표축 방향의 분산(흩어짐 정도)을 의미한다.  
  - 비대각 원소는 서로 다른 좌표축 간의 공분산(함께 움직이는 정도)을 의미한다.  

- 모든 표본에 대해 이 행렬을 구한 후 평균을 내면  

$$
\Sigma = \frac{1}{M-1}\sum_{m=0}^{M-1} (\vec{x}_m - \vec{\mu})(\vec{x}_m - \vec{\mu})^T
$$  

이 되고, 이것이 **표본공분산 행렬(sample covariance matrix)** 이다.  

즉, 그림 속 화살표들의 퍼짐과 방향성을 수학적으로 집계한 결과가 $\Sigma$ 로 나타난다.  

---

## p8. 대수의 법칙 / 통계적 추정량 (LLN / Statistical Estimator)  

**표본평균 (Sample Mean), 표본공분산 (Sample Covariance)**  

$$
\vec{\mu} = \frac{1}{M}\sum_{m=0}^{M-1} \vec{x}_m
$$  

$$
\Sigma = \frac{1}{M-1}\sum_{m=0}^{M-1} (\vec{x}_m - \vec{\mu})(\vec{x}_m - \vec{\mu})^T
$$  

- 표본평균과 표본공분산은 실제 평균(real mean)과 실제 공분산(real covariance)과 동일하지 않다.  
- 그러나 특별히 구분할 필요가 없는 경우, 같은 기호 $\vec{\mu}, \Sigma$ 를 사용한다.  

---

<img src="/assets/img/probstat/5/image_4.png" alt="image" width="500px">

---

### 보충 설명  

#### 1. **표본 추정량과 모집단 모수의 차이**  
- $\vec{\mu}$ 와 $\Sigma$ 는 실제 모집단의 모수(parameter)가 아니라, 주어진 표본을 기반으로 한 추정치(estimator)이다.  
- 표본의 수가 커질수록 (즉, $M \to \infty$) 표본평균과 표본공분산은 실제 평균과 공분산에 수렴한다.  

#### 2. **그림 해석**  
- 파란 점들은 표본(instance)이고, 빨간 점은 표본평균 $\vec{\mu}$ 를 나타낸다.  
- 화살표는 각 표본과 평균의 차이 벡터 $(\vec{x}_m - \vec{\mu})$ 이며,  
  이들의 외적을 평균 낸 것이 표본공분산 $\Sigma$ 를 형성한다.  

---

## p9. 고유벡터와 고유값 (Eigenvector and Eigenvalue)  

- $D \times D$ 정방행렬 $A$의 오른쪽 고유벡터(right eigenvector) $\vec{v}$ 는 다음을 만족하는 벡터이다.  

$$
A \vec{v} = \lambda \vec{v} \tag{1}
$$  

- 이때 $\lambda$ 를 **고유값(eigenvalue)** 이라고 한다.  
- 식 (1)이 해(solution)를 가지기 위한 필요충분조건은  

$$
|A - \lambda I| = 0 \tag{2}
$$  

이다.  

---

### 보충 설명  

#### 1. **고유값 방정식의 의미**  
- $A \vec{v} = \lambda \vec{v}$ 는 행렬 $A$가 벡터 $\vec{v}$ 를 단순히 크기만 $\lambda$ 배 확장하거나 축소하고, 방향은 바꾸지 않는다는 의미이다.  
- 즉, $\vec{v}$ 는 행렬 $A$의 선형변환에 대해 **불변한 방향**을 가진 벡터이다.  

#### 2. **행렬식 조건**  
- $(A - \lambda I)\vec{v} = 0$ 의 비자명 해(non-trivial solution)를 가지려면,  
  $(A - \lambda I)$ 가 가역적이지 않아야 한다.  
- 이는 곧 행렬식이 0이 되는 조건, 즉  

$$
|A - \lambda I| = 0
$$  

을 만족해야 한다.  
- 이 식을 **특성방정식(characteristic equation)** 이라고 하며, 이를 풀어 $\lambda$ 값을 구할 수 있다.  

---

## p10. 고유벡터와 고유값 (Eigenvector and Eigenvalue)  

- 지금까지 우리는 오른쪽 고유벡터(right eigenvector)와 오른쪽 고유값(right eigenvalue)을 다루어 왔다.  

$$
A \vec{v}_d = \lambda_d \vec{v}_d
$$  

- 또한 왼쪽 고유벡터(left eigenvector)도 존재할 수 있는데, 이는 행벡터(row vector) $\vec{u}_d$ 이며, 그에 대응하는 왼쪽 고유값(left eigenvalue) $\kappa_d$ 는 다음을 만족한다.  

$$
\vec{u}_d^T A = \kappa_d \vec{u}_d^T
$$  

---

### 보충 설명 (열 관점 vs 행 관점)  

#### 1. **열 관점 (오른쪽 고유벡터)**  

행렬 $A$와 열벡터 $\vec{x}$의 곱은,  
$A$의 **각 열(column)** 이 $\vec{x}$의 성분에 의해 가중합되는 것으로 볼 수 있다.  

예:  

$$
A = \begin{bmatrix}
2 & 1 \\
0 & 3
\end{bmatrix}, \quad 
\vec{x} = \begin{bmatrix}x_1 \\ x_2\end{bmatrix}
$$  

곱을 계산하면,  

$$
A\vec{x} = 
\begin{bmatrix}
2 & 1 \\
0 & 3
\end{bmatrix}
\begin{bmatrix}x_1 \\ x_2\end{bmatrix}
= x_1 \begin{bmatrix}2 \\ 0\end{bmatrix} 
+ x_2 \begin{bmatrix}1 \\ 3\end{bmatrix}
$$  

즉, $A\vec{x}$ 는 $A$의 **열(column)** 들의 선형결합으로 표현된다.  
따라서 오른쪽 고유벡터 $\vec{v}$ 는 “$A$의 열 조합이 $\lambda \vec{v}$로 딱 맞게 떨어지는 방향”이다.  

---

#### 2. **행 관점 (왼쪽 고유벡터)**  

이번에는 행벡터 $\vec{u}^T$와 $A$의 곱을 보자.  

$$
\vec{u}^T A =
\begin{bmatrix}u_1 & u_2\end{bmatrix}
\begin{bmatrix}
2 & 1 \\
0 & 3
\end{bmatrix}
$$  

행렬 곱의 정의에 따라,  

$$
\vec{u}^T A =
u_1 \cdot \begin{bmatrix}2 & 1\end{bmatrix}
+ u_2 \cdot \begin{bmatrix}0 & 3\end{bmatrix}
$$  

즉, $A$의 **첫 번째 행** $\begin{bmatrix}2 & 1\end{bmatrix}$ 은 $u_1$로,  
**두 번째 행** $\begin{bmatrix}0 & 3\end{bmatrix}$ 은 $u_2$로 가중합된다.  

---

이를 다시 계산하면,  

$$
\vec{u}^T A =
\begin{bmatrix}
2u_1 + 0 \cdot u_2 , \; 1\cdot u_1 + 3u_2
\end{bmatrix}
=
\begin{bmatrix}
2u_1 , \; u_1 + 3u_2
\end{bmatrix}
$$  

---

#### 직관  
- $\vec{u}^T A$는 **$A$의 행(row) 벡터들의 선형결합**이다.  
- 계수는 $\vec{u}^T = (u_1, u_2)$가 제공한다.  
- 따라서 왼쪽 고유벡터 $\vec{u}^T$ 는  
  $A$의 행들의 조합이 자기 자신 방향(배율 $\kappa$)을 유지하는 경우를 찾아주는 것이다.  

---

#### 3. **종합**  

- **오른쪽 고유벡터 (열 관점)**  
  - $A\vec{v} = \lambda \vec{v}$  
  - $\vec{v}$ 는 $A$의 **열들의 조합**을 통해 자기 자신 방향을 유지한다.  

- **왼쪽 고유벡터 (행 관점)**  
  - $\vec{u}^T A = \kappa \vec{u}^T$  
  - $\vec{u}^T$ 는 $A$의 **행들의 조합**을 통해 자기 자신 방향을 유지한다.  

즉,  
- 오른쪽 고유벡터: $A$가 “데이터를 **곱하는 방향**(열 공간)”을 설명  
- 왼쪽 고유벡터: $A$의 각 행이 “데이터를 **관찰하는 방향**(행 공간)”을 설명  
한다.  


