---
layout: post
title: "[확률과 통계] 13주차"
date: 2025-11-25 14:00:00 +0900
categories:
  - "대학원 수업"
  - "확률과 통계"
tags: []
---

> 출처: 확률과 통계 – 박성우 교수님, 고려대학교 (2025)

## p2. Motivation  

<img src="/assets/img/lecture/probstat/13/image_1.png" alt="image" width="720px">

---

## p3. Motivation  

<img src="/assets/img/lecture/probstat/13/image_2.png" alt="image" width="800px">

---

## p4. Change of Variables

- **변수 변경 (1차원 경우):**  
  만약 $X = f(Z)$ 이고 $f(\cdot)$ 가 단조이며 역함수 $Z = f^{-1}(X) = h(X)$ 가 존재하면:

  $$
  p_X(x) = p_Z(h(x))\, \mid h'(x) \mid
  $$

- 이전 예시:  
  $X = f(Z) = 4Z$, $Z \sim \mathcal{U}[0,2]$ 일 때 $p_X(4)$ 는 무엇인가?

  - $h(X) = X/4$
  - $p_X(4) = p_Z(1)\, h'(4) = \frac{1}{2} \times \mid 1/4 \mid = \frac{1}{8}$

- 더 흥미로운 예시:  
  $X = f(Z) = \exp(Z)$, $Z \sim \mathcal{U}[0,2]$ 일 때 $p_X(x)$ 는 무엇인가?

  - $h(X) = \ln(X)$
  - $p_X(x) = p_Z(\ln(x))\, \mid h'(x) \mid = \frac{1}{2x}$ for $x \in [\exp(0), \exp(2)]$

- $p_X(x)$ 의 “모양(shape)” 은 prior $p_Z(z)$ 보다 더 복잡할 수 있음에 유의하자.

---

> **수식 도출 과정**
>
> **1단계. 확률은 동일 — 변수만 다르다**
>
> 변수변환의 출발점은 매우 단순하다.
>
> > $X$가 어떤 작은 구간에 있을 확률 = $Z$가 그 구간으로 변환된 구간에 있을 확률
>
> 즉,
>
> $$
> P(X \in [x, x+dx]) = P(Z \in [z, z+dz])
> $$
>
> 여기서  
>
> $$z = h(x)$$
>
> ---
>
> **2단계. 양쪽을 확률밀도로 표현**
>
> 왼쪽(X 쪽):
>
> $$
> P(X \in [x, x+dx]) = p_X(x)\, dx
> $$
>
> 오른쪽(Z 쪽):
>
> $$
> P(Z \in [z, z+dz]) = p_Z(z)\, dz
> $$
>
> 두 확률이 같으므로:
>
> $$
> p_X(x)\, dx = p_Z(z)\, dz
> $$
>
> ---
>
> **3단계. $Z = h(X)$ 를 미분하여 $dz$와 $dx$ 관계 구하기**
>
> $$
> z = h(x)
> $$
>
> 양변 미분하면:
>
> $$
> dz = h'(x)\, dx
> $$
>
> ---
>
> **4단계. $dz = h'(x)\, dx$ 를 확률식에 대입**
>
> 확률식:
>
> $$
> p_X(x)\, dx = p_Z(z)\, dz
> $$
>
> 여기에 $z = h(x)$, $dz = h'(x)\, dx$를 대입하면:
>
> $$
> p_X(x)\, dx = p_Z(h(x))\, h'(x)\, dx
> $$
>
> 양변에서 $dx$ 제거:
>
> $$
> p_X(x) = p_Z(h(x))\, h'(x)
> $$
>
> ---
>
> **5단계. 왜 절댓값이 필요한가?**
>
> 단조 감소일 때:
>
> $$
> h'(x) < 0
> $$
>
> 그러면
>
> $$
> p_Z(h(x))\, h'(x)
> $$
>
> 이 음수가 되어버린다 → 확률밀도는 음수가 될 수 없음.
>
> 또한 변수 변환에서 필요한 것은
>
> > 방향이 아니라 “길이의 변화량(스케일)”
>
> 이며, 이는 항상 양수여야 한다.
>
> 따라서 절댓값을 붙여주면:
>
> $$
> p_X(x) = p_Z(h(x))\, |h'(x)|
> $$

---

## p5. Change of Variables

- $Z$를 $[0,1]^n$ 구간에의 균등(uniform) 확률벡터라고 하자.
- $X = A Z$ 이고, $A$는 역행렬 $W = A^{-1}$을 갖는 정방행렬이라고 하자.  
  이때 $X$는 어떻게 분포하는가?
- 기하학적으로, 행렬 $A$는 단위 하이퍼큐브 $[0,1]^n$을 하나의 parallelotope(평행다포체)로 사상한다.
- 하이퍼큐브와 parallelotope는 정사각형/정육면체와 평행사변형/평행육면체를 고차원으로 일반화한 개념이다.

<img src="/assets/img/lecture/probstat/13/image_3.png" alt="image" width="600px">

그림: 행렬
$$A =
\begin{pmatrix}
a & c \\
b & d
\end{pmatrix}
$$ 는 단위 정사각형을 평행사변형으로 사상한다.

---

## p6. Change of Variables

- parallelotope의 부피(volume)는 행렬 $A$의 행렬식(determinant)의 절댓값과 동일하다.

  $$
  \det(A)
  =
  \det
  \begin{pmatrix}
  a & c \\
  b & d
  \end{pmatrix}
  =
  ad - bc
  $$

  <img src="/assets/img/lecture/probstat/13/image_4.png" alt="image" width="480px">

- $X = A Z$라고 하자. 여기서 $A$는 정방의 가역 행렬이며, 그 역행렬은 $W = A^{-1}$이다.  
  $X$는 면적이 $|\det(A)|$인 parallelotope 상에서 균일하게 분포한다.  
  따라서 다음을 얻는다:

  $$
  p_X(x) = p_Z(Wx)\,/\,|\det(A)|
  $$

  $$
  = p_Z(Wx)\,|\det(W)|
  $$

- 왜냐하면 $W = A^{-1}$이면  

  $$
  \det(W) = \frac{1}{\det(A)}
  $$

- 1차원 경우의 공식과의 유사성에 주목하라.

---

> **1차원 공식과의 유사성 설명**
>
> 다변량 선형변환에서도 밀도변환 공식의 핵심 구조는  
> **“원래 밀도를 역변환한 지점에서 평가하고, 스케일 변화량의 절댓값으로 나눈다”**는 점에서  
> 1차원 변수변환 공식과 완전히 동일하다.
>
> 1차원에서는  
> $$
> p_X(x) = p_Z(h(x))\,|h'(x)|
> $$
> 로서 스케일 변화량이 도함수 $h'(x)$ 하나였다.
>
> 다변량에서는  
> $$
> p_X(x) = p_Z(Wx)\,|\det(W)|,
> $$
> 여기서 스케일 변화량이 **Jacobian의 절댓값 = $\det(W)$** 로 일반화되었을 뿐이다.
>
> 즉,  
> **1D: 길이(scale) 변화 → $|h'(x)|$**  
> **nD: 부피(volume) 변화 → $|\det(W)|$**
>
> 따라서 다변량 공식은 1차원 변수변환의 자연스러운 확장(extension)이다.

---

## p7. Change of Variables

- $A$를 통한 선형변환의 경우, 부피의 변화는 행렬 $A$의 행렬식(determinant)로 주어진다.

- 비선형 변환 $f(\cdot)$의 경우, 선형화된(linearized) 부피 변화는  
  $f(\cdot)$의 Jacobian의 행렬식(determinant)으로 주어진다.

- **변수변환(일반 경우)**:  
  $f : \mathbb{R}^n \to \mathbb{R}^n$ 이 $X = f(Z)$ 와 $Z = f^{-1}(X)$ 를 만족하도록 가역일 때,  
  $Z$와 $X$의 대응(mapping)은 다음과 같다:

  $$
  p_X(x)
  =
  p_Z\!\bigl(f^{-1}(x)\bigr)
  \left|
    \det\!\left(
      \frac{\partial f^{-1}(x)}{\partial x}
    \right)
  \right|
  $$

- **Note 0**:  
  이는 이전 1차원 경우  
  $p_X(x) = p_Z(h(x))\,|h'(x)|$  
  를 일반화한 것이다.

- **Note 1**:  
  VAE와 달리, $x, z$는 연속적이어야 하며 같은 차원을 가져야 한다.  
  예를 들어, $x \in \mathbb{R}^n$ 이면 $z \in \mathbb{R}^n$ 이다.

- **Note 2**:  
  어떤 가역행렬 $A$에 대해서도  
  $\det(A^{-1}) = \det(A)^{-1}$ 이다.

  따라서,

  $$
  p_X(x)
  =
  p_Z(z)
  \left|
    \det\!\left(
      \frac{\partial f(z)}{\partial z}
    \right)
  \right|^{-1}
  $$

---

## p8. Example: Two-dimensional Change of Variables

- $Z_1$과 $Z_2$를 결합밀도 $p_{Z_1,Z_2}$를 갖는 연속 확률변수라고 하자.

- $u : \mathbb{R}^2 \to \mathbb{R}^2$ 를 가역변환이라고 하자.  
  두 입력과 두 출력으로 이루어져 있으며, 이를 $u = (u_1, u_2)$로 표기한다.

- $v = (v_1, v_2)$ 를 그 역변환이라고 하자.

- $X_1 = u_1(Z_1, Z_2)$ 이고 $X_2 = u_2(Z_1, Z_2)$ 라고 하자.  
  그러면 $Z_1 = v_1(X_1, X_2)$ 이고 $Z_2 = v_2(X_1, X_2)$ 이다.

  $$p_{X_1,X_2}(x_1,x_2)$$

  $$=
  p_{Z_1,Z_2}\bigl(v_1(x_1,x_2),\, v_2(x_1,x_2)\bigr)
  \left|
  \det
  \begin{pmatrix}
  \dfrac{\partial v_1(x_1,x_2)}{\partial x_1} &
  \dfrac{\partial v_1(x_1,x_2)}{\partial x_2} \\[6pt]
  \dfrac{\partial v_2(x_1,x_2)}{\partial x_1} &
  \dfrac{\partial v_2(x_1,x_2)}{\partial x_2}
  \end{pmatrix}
  \right|
  \quad \text{(inverse)}
  $$

  $$=
  p_{Z_1,Z_2}(z_1,z_2)
  \left|
  \det
  \begin{pmatrix}
  \dfrac{\partial u_1(z_1,z_2)}{\partial z_1} &
  \dfrac{\partial u_1(z_1,z_2)}{\partial z_2} \\[6pt]
  \dfrac{\partial u_2(z_1,z_2)}{\partial z_1} &
  \dfrac{\partial u_2(z_1,z_2)}{\partial z_2}
  \end{pmatrix}
  \right|^{-1}
  \quad \text{(forward)}
  $$

---

> **inverse의 의미**  
> inverse는 **역변환 Jacobian**을 사용했다는 뜻이다.  
> 즉, $x \mapsto z$ 로 가는 역함수 $v(x)$를 미분하여  
> $ \frac{\partial v(x)}{\partial x} $ 의 Jacobian determinant를 쓰는 방식이다.  
> 이는  
> $$
> p_X(x) = p_Z(v(x))\,\left|\det\!\left(\frac{\partial v(x)}{\partial x}\right)\right|
> $$  
> 형태로 표현된다.
> 
> **forward의 의미**  
> forward는 **정방향 변환 Jacobian**을 사용했다는 뜻이다.  
> 즉, $z \mapsto x$ 로 가는 원래 함수 $u(z)$를 미분하여  
> $ \frac{\partial u(z)}{\partial z} $ 의 Jacobian determinant를 쓰고,  
> 그 역수를 취하는 방식이다.  
> 이는  
> $$
> p_X(x)
> =
> p_Z(z)\,
> \left|
> \det\!\left(\frac{\partial u(z)}{\partial z}\right)
> \right|^{-1}
> $$  
> 형태로 표현된다.
> 
> **두 방식의 관계**  
> 역함수 정리(inverse function theorem)에 의해  
> $$
> \det\!\left(\frac{\partial v(x)}{\partial x}\right)
> =
> \left[\det\!\left(\frac{\partial u(z)}{\partial z}\right)\right]^{-1}
> $$  
> 이므로, inverse 방식과 forward 방식은 완전히 동일한 결과를 준다.

---

## p9. Motivation: Normalizing Flows

- 관측 변수 $X$와 잠재 변수 $Z$ 위의  
  유향(directed) 잠재변수(latent-variable) 모델을 고려하자.

- **normalizing flow 모델**에서,  
  $f_\theta : \mathbb{R}^n \to \mathbb{R}^n$ 로 주어지는 $Z$와 $X$ 사이의 사상(mapping)은  
  결정적(deterministic)이며 가역적(invertible)이다.  
  따라서  
  $$
  X = f_\theta(Z), \quad Z = f_\theta^{-1}(X)
  $$

  <img src="/assets/img/lecture/probstat/13/image_5.png" alt="image" width="200px">

- 변수변환(change of variables)를 사용하면, 주변우도(marginal likelihood) $p(x)$는 다음과 같이 주어진다:

  $$
  p_X(x;\theta)
  =
  p_Z\!\left(f_\theta^{-1}(x)\right)
  \left|
  \det\!\left(
    \frac{\partial f_\theta^{-1}(x)}{\partial x}
  \right)
  \right|
  $$

- 참고: $x, z$는 연속적이어야 하며 동일한 차원을 가져야 한다.

---

## p10. Flow of Transformations

**Normalizing:**  
가역 변환을 적용한 후, 변수변환(change of variables)은 정규화된(normalized) 밀도를 제공한다.

**Flow:**  
가역 변환들은 서로 합성될 수 있다.

$$
z_m
=
f_\theta^{m}
\circ \cdots \circ
f_\theta^{1}(z_0)
=
f_\theta^{m}\!\bigl(f_\theta^{m-1}\!(\cdots(f_\theta^{1}(z_0))\bigr)
\;\triangleq\;
f_\theta(z_0)
$$

- $z_0$에 대해 단순한(simple) 분포로 시작한다 (예: 가우시안).
- 최종적으로 $x = z_M$ 을 얻기 위해, $M$개의 가역 변환을 순차적으로 적용한다.
- 변수변환 공식에 의해,

  $$
  p_X(x;\theta)
  =
  p_Z\!\left(f_\theta^{-1}(x)\right)
  \prod_{m=1}^{M}
  \left|
  \det
  \left(
  \frac{\partial (f_\theta^{m})^{-1}(z_m)}
      {\partial z_m}
  \right)
  \right|
  $$

  (참고: 행렬곱의 determinant 는 determinant 들의 곱과 동일하다)

---

## p11. Maximum Likelihood Estimation

- 데이터셋 $\mathcal{D}$ 위에서 maximum likelihood를 이용한 학습:

$$
\max_{\theta} \log p_X(\mathcal{D};\theta)
=
\sum_{x \in \mathcal{D}}
\log p_Z\!\left(f_\theta^{-1}(x)\right)
+
\log
\left|
\det\!\left(
\frac{\partial f_\theta^{-1}(x)}{\partial x}
\right)
\right|
$$

- inverse 변환 $x \mapsto z$ 와 변수변환 공식(change of variables)을 통한  
  **정확한 우도 계산(exact likelihood evaluation)**

- forward 변환 $z \mapsto x$ 를 통한 **샘플링(sampling)**

$$
z \sim p_Z(z),
\quad
x = f_\theta(z)
$$

- inverse 변환을 통해 잠재 표현(latent representations) 추론  
  (inference network가 필요 없음!)

$$
z = f_\theta^{-1}(x)
$$
