---
layout: post
title: "[확률과 통계] 13주차"
date: 2025-11-25 14:00:00 +0900
categories:
  - "대학원 수업"
  - "확률과 통계"
tags: []
---

> 출처: 확률과 통계 – 박성우 교수님, 고려대학교 (2025)

## p2. Motivation  

<img src="/assets/img/lecture/probstat/13/image_1.png" alt="image" width="720px">

---

## p3. Motivation  

<img src="/assets/img/lecture/probstat/13/image_2.png" alt="image" width="800px">

---

## p4. Change of Variables

- **변수 변경 (1차원 경우):**  
  만약 $X = f(Z)$ 이고 $f(\cdot)$ 가 단조이며 역함수 $Z = f^{-1}(X) = h(X)$ 가 존재하면:

  $$
  p_X(x) = p_Z(h(x))\, \mid h'(x) \mid
  $$

- 이전 예시:  
  $X = f(Z) = 4Z$, $Z \sim \mathcal{U}[0,2]$ 일 때 $p_X(4)$ 는 무엇인가?

  - $h(X) = X/4$
  - $p_X(4) = p_Z(1)\, h'(4) = \frac{1}{2} \times \mid 1/4 \mid = \frac{1}{8}$

- 더 흥미로운 예시:  
  $X = f(Z) = \exp(Z)$, $Z \sim \mathcal{U}[0,2]$ 일 때 $p_X(x)$ 는 무엇인가?

  - $h(X) = \ln(X)$
  - $p_X(x) = p_Z(\ln(x))\, \mid h'(x) \mid = \frac{1}{2x}$ for $x \in [\exp(0), \exp(2)]$

- $p_X(x)$ 의 “모양(shape)” 은 prior $p_Z(z)$ 보다 더 복잡할 수 있음에 유의하자.

---

> **수식 도출 과정**
>
> **1단계. 확률은 동일 — 변수만 다르다**
>
> 변수변환의 출발점은 매우 단순하다.
>
> > $X$가 어떤 작은 구간에 있을 확률 = $Z$가 그 구간으로 변환된 구간에 있을 확률
>
> 즉,
>
> $$
> P(X \in [x, x+dx]) = P(Z \in [z, z+dz])
> $$
>
> 여기서  
>
> $$z = h(x)$$
>
> ---
>
> **2단계. 양쪽을 확률밀도로 표현**
>
> 왼쪽(X 쪽):
>
> $$
> P(X \in [x, x+dx]) = p_X(x)\, dx
> $$
>
> 오른쪽(Z 쪽):
>
> $$
> P(Z \in [z, z+dz]) = p_Z(z)\, dz
> $$
>
> 두 확률이 같으므로:
>
> $$
> p_X(x)\, dx = p_Z(z)\, dz
> $$
>
> ---
>
> **3단계. $Z = h(X)$ 를 미분하여 $dz$와 $dx$ 관계 구하기**
>
> $$
> z = h(x)
> $$
>
> 양변 미분하면:
>
> $$
> dz = h'(x)\, dx
> $$
>
> ---
>
> **4단계. $dz = h'(x)\, dx$ 를 확률식에 대입**
>
> 확률식:
>
> $$
> p_X(x)\, dx = p_Z(z)\, dz
> $$
>
> 여기에 $z = h(x)$, $dz = h'(x)\, dx$를 대입하면:
>
> $$
> p_X(x)\, dx = p_Z(h(x))\, h'(x)\, dx
> $$
>
> 양변에서 $dx$ 제거:
>
> $$
> p_X(x) = p_Z(h(x))\, h'(x)
> $$
>
> ---
>
> **5단계. 왜 절댓값이 필요한가?**
>
> 단조 감소일 때:
>
> $$
> h'(x) < 0
> $$
>
> 그러면
>
> $$
> p_Z(h(x))\, h'(x)
> $$
>
> 이 음수가 되어버린다 → 확률밀도는 음수가 될 수 없음.
>
> 또한 변수 변환에서 필요한 것은
>
> > 방향이 아니라 “길이의 변화량(스케일)”
>
> 이며, 이는 항상 양수여야 한다.
>
> 따라서 절댓값을 붙여주면:
>
> $$
> p_X(x) = p_Z(h(x))\, |h'(x)|
> $$

---

## p5. Change of Variables

- $Z$를 $[0,1]^n$ 구간에의 균등(uniform) 확률벡터라고 하자.
- $X = A Z$ 이고, $A$는 역행렬 $W = A^{-1}$을 갖는 정방행렬이라고 하자.  
  이때 $X$는 어떻게 분포하는가?
- 기하학적으로, 행렬 $A$는 단위 하이퍼큐브 $[0,1]^n$을 하나의 parallelotope(평행다포체)로 사상한다.
- 하이퍼큐브와 parallelotope는 정사각형/정육면체와 평행사변형/평행육면체를 고차원으로 일반화한 개념이다.

<img src="/assets/img/lecture/probstat/13/image_3.png" alt="image" width="600px">

그림: 행렬
$$A =
\begin{pmatrix}
a & c \\
b & d
\end{pmatrix}
$$ 는 단위 정사각형을 평행사변형으로 사상한다.

---

## p6. Change of Variables

- parallelotope의 부피(volume)는 행렬 $A$의 행렬식(determinant)의 절댓값과 동일하다.

  $$
  \det(A)
  =
  \det
  \begin{pmatrix}
  a & c \\
  b & d
  \end{pmatrix}
  =
  ad - bc
  $$

  <img src="/assets/img/lecture/probstat/13/image_4.png" alt="image" width="480px">

- $X = A Z$라고 하자. 여기서 $A$는 정방의 가역 행렬이며, 그 역행렬은 $W = A^{-1}$이다.  
  $X$는 면적이 $|\det(A)|$인 parallelotope 상에서 균일하게 분포한다.  
  따라서 다음을 얻는다:

  $$
  p_X(x) = p_Z(Wx)\,/\,|\det(A)|
  $$

  $$
  = p_Z(Wx)\,|\det(W)|
  $$

- 왜냐하면 $W = A^{-1}$이면  

  $$
  \det(W) = \frac{1}{\det(A)}
  $$

- 1차원 경우의 공식과의 유사성에 주목하라.

---

> **1차원 공식과의 유사성 설명**
>
> 다변량 선형변환에서도 밀도변환 공식의 핵심 구조는  
> **“원래 밀도를 역변환한 지점에서 평가하고, 스케일 변화량의 절댓값으로 나눈다”**는 점에서  
> 1차원 변수변환 공식과 완전히 동일하다.
>
> 1차원에서는  
> $$
> p_X(x) = p_Z(h(x))\,|h'(x)|
> $$
> 로서 스케일 변화량이 도함수 $h'(x)$ 하나였다.
>
> 다변량에서는  
> $$
> p_X(x) = p_Z(Wx)\,|\det(W)|,
> $$
> 여기서 스케일 변화량이 **Jacobian의 절댓값 = $\det(W)$** 로 일반화되었을 뿐이다.
>
> 즉,  
> **1D: 길이(scale) 변화 → $|h'(x)|$**  
> **nD: 부피(volume) 변화 → $|\det(W)|$**
>
> 따라서 다변량 공식은 1차원 변수변환의 자연스러운 확장(extension)이다.

---

## p7. Change of Variables

- $A$를 통한 선형변환의 경우, 부피의 변화는 행렬 $A$의 행렬식(determinant)로 주어진다.

- 비선형 변환 $f(\cdot)$의 경우, 선형화된(linearized) 부피 변화는  
  $f(\cdot)$의 Jacobian의 행렬식(determinant)으로 주어진다.

- **변수변환(일반 경우)**:  
  $f : \mathbb{R}^n \to \mathbb{R}^n$ 이 $X = f(Z)$ 와 $Z = f^{-1}(X)$ 를 만족하도록 가역일 때,  
  $Z$와 $X$의 대응(mapping)은 다음과 같다:

  $$
  p_X(x)
  =
  p_Z\!\bigl(f^{-1}(x)\bigr)
  \left|
    \det\!\left(
      \frac{\partial f^{-1}(x)}{\partial x}
    \right)
  \right|
  $$

- **Note 0**:  
  이는 이전 1차원 경우  
  $p_X(x) = p_Z(h(x))\,|h'(x)|$  
  를 일반화한 것이다.

- **Note 1**:  
  VAE와 달리, $x, z$는 연속적이어야 하며 같은 차원을 가져야 한다.  
  예를 들어, $x \in \mathbb{R}^n$ 이면 $z \in \mathbb{R}^n$ 이다.

- **Note 2**:  
  어떤 가역행렬 $A$에 대해서도  
  $\det(A^{-1}) = \det(A)^{-1}$ 이다.

  따라서,

  $$
  p_X(x)
  =
  p_Z(z)
  \left|
    \det\!\left(
      \frac{\partial f(z)}{\partial z}
    \right)
  \right|^{-1}
  $$

---

## p8. Example: Two-dimensional Change of Variables

- $Z_1$과 $Z_2$를 결합밀도 $p_{Z_1,Z_2}$를 갖는 연속 확률변수라고 하자.

- $u : \mathbb{R}^2 \to \mathbb{R}^2$ 를 가역변환이라고 하자.  
  두 입력과 두 출력으로 이루어져 있으며, 이를 $u = (u_1, u_2)$로 표기한다.

- $v = (v_1, v_2)$ 를 그 역변환이라고 하자.

- $X_1 = u_1(Z_1, Z_2)$ 이고 $X_2 = u_2(Z_1, Z_2)$ 라고 하자.  
  그러면 $Z_1 = v_1(X_1, X_2)$ 이고 $Z_2 = v_2(X_1, X_2)$ 이다.

  $$p_{X_1,X_2}(x_1,x_2)$$

  $$=
  p_{Z_1,Z_2}\bigl(v_1(x_1,x_2),\, v_2(x_1,x_2)\bigr)
  \left|
  \det
  \begin{pmatrix}
  \dfrac{\partial v_1(x_1,x_2)}{\partial x_1} &
  \dfrac{\partial v_1(x_1,x_2)}{\partial x_2} \\[6pt]
  \dfrac{\partial v_2(x_1,x_2)}{\partial x_1} &
  \dfrac{\partial v_2(x_1,x_2)}{\partial x_2}
  \end{pmatrix}
  \right|
  \quad \text{(inverse)}
  $$

  $$=
  p_{Z_1,Z_2}(z_1,z_2)
  \left|
  \det
  \begin{pmatrix}
  \dfrac{\partial u_1(z_1,z_2)}{\partial z_1} &
  \dfrac{\partial u_1(z_1,z_2)}{\partial z_2} \\[6pt]
  \dfrac{\partial u_2(z_1,z_2)}{\partial z_1} &
  \dfrac{\partial u_2(z_1,z_2)}{\partial z_2}
  \end{pmatrix}
  \right|^{-1}
  \quad \text{(forward)}
  $$

---

> **inverse의 의미**  
> inverse는 **역변환 Jacobian**을 사용했다는 뜻이다.  
> 즉, $x \mapsto z$ 로 가는 역함수 $v(x)$를 미분하여  
> $ \frac{\partial v(x)}{\partial x} $ 의 Jacobian determinant를 쓰는 방식이다.  
> 이는  
> $$
> p_X(x) = p_Z(v(x))\,\left|\det\!\left(\frac{\partial v(x)}{\partial x}\right)\right|
> $$  
> 형태로 표현된다.
> 
> **forward의 의미**  
> forward는 **정방향 변환 Jacobian**을 사용했다는 뜻이다.  
> 즉, $z \mapsto x$ 로 가는 원래 함수 $u(z)$를 미분하여  
> $ \frac{\partial u(z)}{\partial z} $ 의 Jacobian determinant를 쓰고,  
> 그 역수를 취하는 방식이다.  
> 이는  
> $$
> p_X(x)
> =
> p_Z(z)\,
> \left|
> \det\!\left(\frac{\partial u(z)}{\partial z}\right)
> \right|^{-1}
> $$  
> 형태로 표현된다.
> 
> **두 방식의 관계**  
> 역함수 정리(inverse function theorem)에 의해  
> $$
> \det\!\left(\frac{\partial v(x)}{\partial x}\right)
> =
> \left[\det\!\left(\frac{\partial u(z)}{\partial z}\right)\right]^{-1}
> $$  
> 이므로, inverse 방식과 forward 방식은 완전히 동일한 결과를 준다.

---

## p9. Motivation: Normalizing Flows

- 관측 변수 $X$와 잠재 변수 $Z$ 위의  
  유향(directed) 잠재변수(latent-variable) 모델을 고려하자.

- **normalizing flow 모델**에서,  
  $f_\theta : \mathbb{R}^n \to \mathbb{R}^n$ 로 주어지는 $Z$와 $X$ 사이의 사상(mapping)은  
  결정적(deterministic)이며 가역적(invertible)이다.  
  따라서  
  $$
  X = f_\theta(Z), \quad Z = f_\theta^{-1}(X)
  $$

  <img src="/assets/img/lecture/probstat/13/image_5.png" alt="image" width="200px">

- 변수변환(change of variables)를 사용하면, 주변우도(marginal likelihood) $p(x)$는 다음과 같이 주어진다:

  $$
  p_X(x;\theta)
  =
  p_Z\!\left(f_\theta^{-1}(x)\right)
  \left|
  \det\!\left(
    \frac{\partial f_\theta^{-1}(x)}{\partial x}
  \right)
  \right|
  $$

- 참고: $x, z$는 연속적이어야 하며 동일한 차원을 가져야 한다.

---

## p10. Flow of Transformations

**Normalizing:**  
가역 변환을 적용한 후, 변수변환(change of variables)은 정규화된(normalized) 밀도를 제공한다.

**Flow:**  
가역 변환들은 서로 합성될 수 있다.

$$
z_m
=
f_\theta^{m}
\circ \cdots \circ
f_\theta^{1}(z_0)
=
f_\theta^{m}\!\bigl(f_\theta^{m-1}\!(\cdots(f_\theta^{1}(z_0))\bigr)
\;\triangleq\;
f_\theta(z_0)
$$

- $z_0$에 대해 단순한(simple) 분포로 시작한다 (예: 가우시안).
- 최종적으로 $x = z_M$ 을 얻기 위해, $M$개의 가역 변환을 순차적으로 적용한다.
- 변수변환 공식에 의해,

  $$
  p_X(x;\theta)
  =
  p_Z\!\left(f_\theta^{-1}(x)\right)
  \prod_{m=1}^{M}
  \left|
  \det
  \left(
  \frac{\partial (f_\theta^{m})^{-1}(z_m)}
      {\partial z_m}
  \right)
  \right|
  $$

  (참고: 행렬곱의 determinant 는 determinant 들의 곱과 동일하다)

---

## p11. Maximum Likelihood Estimation

- 데이터셋 $\mathcal{D}$ 위에서 maximum likelihood를 이용한 학습:

$$
\max_{\theta} \log p_X(\mathcal{D};\theta)
=
\sum_{x \in \mathcal{D}}
\log p_Z\!\left(f_\theta^{-1}(x)\right)
+
\log
\left|
\det\!\left(
\frac{\partial f_\theta^{-1}(x)}{\partial x}
\right)
\right|
$$

- inverse 변환 $x \mapsto z$ 와 변수변환 공식(change of variables)을 통한  
  **정확한 우도 계산(exact likelihood evaluation)**

- forward 변환 $z \mapsto x$ 를 통한 **샘플링(sampling)**

$$
z \sim p_Z(z),
\quad
x = f_\theta(z)
$$

- inverse 변환을 통해 잠재 표현(latent representations) 추론  
  (inference network가 필요 없음!)

$$
z = f_\theta^{-1}(x)
$$

---

## p12. Flow of Transformations

- 기본 분포(base distribution): Gaussian

  <img src="/assets/img/lecture/probstat/13/image_6.png" alt="image" width="800px">

- 기본 분포(base distribution): Uniform

  <img src="/assets/img/lecture/probstat/13/image_7.png" alt="image" width="800px">

- 10개의 planar(평면) 변환은 단순한 분포들을 더 복잡한 분포로 변환할 수 있다.

---

## p13. Flow of Transformations

- 효율적인 샘플링과 계산 가능한(tractable) 우도 평가(likelihood evaluation)를  
  가능하게 하는 단순한 prior $p_Z(z)$.  
  예: 등방성 가우시안(isotropic Gaussian)

- 계산 가능한 평가를 갖는 가역 변환들:
  - 우도 계산(likelihood evaluation)은  
    $x \mapsto z$ 매핑을 효율적으로 계산할 수 있어야 한다.
  - 샘플링은  
    $z \mapsto x$ 매핑을 효율적으로 계산할 수 있어야 한다.

- 우도를 계산하는 것은 또한  
  $n \times n$ Jacobian 행렬들의 행렬식(determinant) 계산을 필요로 한다.  
  여기서 $n$은 데이터의 차원이다.
  - $n \times n$ 행렬의 행렬식(determinant) 계산은 $O(n^3)$으로,  
    학습 루프 내에서 사용하기에는 지나치게 비싸다.

- **핵심 아이디어(Key idea):**  
  Jacobian 행렬이 특별한 구조를 갖도록 변환을 선택한다.  
  예를 들어, 삼각행렬(triangular matrix)의 행렬식(determinant)은  
  대각 원소들의 곱이며, 이는 $O(n)$ 연산이다.

---

## p14. Triangular Jacobian

$$
\mathbf{x} = (x_1, \ldots, x_n)
= f(\mathbf{z})
= (f_1(\mathbf{z}), \ldots, f_n(\mathbf{z}))
$$

$$
J
=
\frac{\partial f}{\partial \mathbf{z}}
=
\begin{pmatrix}
\frac{\partial f_1}{\partial z_1} & \cdots & \frac{\partial f_1}{\partial z_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial z_1} & \cdots & \frac{\partial f_n}{\partial z_n}
\end{pmatrix}
$$

$x_i = f_i(\mathbf{z})$가 $\mathbf{z}_{\le i}$에만 의존한다고 하자. 그러면

$$
J
=
\frac{\partial f}{\partial \mathbf{z}}
=
\begin{pmatrix}
\frac{\partial f_1}{\partial z_1} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial z_1} & \cdots & \frac{\partial f_n}{\partial z_n}
\end{pmatrix}
$$

는 하삼각(lower triangular) 구조를 갖는다.  
행렬식(determinant)은 **선형 시간(linear time)** 안에 계산될 수 있다.

마찬가지로, $x_i$가 $\mathbf{z}_{\ge i}$에만 의존하는 경우  
Jacobian은 상삼각(upper triangular)이다.

---

## p15. Normalizing Flows: Theoretical Side

**표기(conventions)**

- $\mu$와 $\nu$를 $\mathbb{R}^d$ 위의 확률측도(probability measures)라고 하고,  
  각각 르베그(Lebesgue) 밀도 $p$와 $q$를 갖는다고 하자.

- 우리는 $\mu$와 $p$를 **목표(target) 측도/밀도**라고 부르고,  
  $\nu$와 $q$를 **기준(reference) 측도/밀도**라고 부른다.

<img src="/assets/img/lecture/probstat/13/image_8.png" alt="image" width="480px">

그림: $\mu$는 왼쪽, $\nu$는 오른쪽

---

> **르베그 밀도(Lebesgue density)란?**  
>
> 르베그 밀도는 $\mathbb{R}^d$ 공간에서 정의된 확률측도가  
> **르베그 측도(일반적인 유클리드 공간의 길이·면적·부피 개념)** 에 대해  
> 절대연속일 때 존재하는 확률밀도 함수이다.
>
> 즉, 확률측도 $\mu$가 르베그 측도 $\lambda$에 대해 절대연속이면  
> 라돈-니코딤 정리에 의해 다음을 만족하는 함수 $p(x)$가 존재한다:
>
> $$
> d\mu(x) = p(x)\, d\lambda(x)
> $$
>
> 여기서의 $p(x)$가 바로 **르베그 밀도(Lebesgue density)** 이다.
>
> 직관적으로 말하면,  
> “일반적인 연속 공간에서의 확률밀도 함수(pdf)”가  
> 바로 르베그 밀도라고 이해할 수 있다.

---

## p16. Normalizing Flows: Theoretical Side

**normalizing flow**는  
$p$에서 $q$로 가는 사상(mapping) $T : \mathbb{R}^d \to \mathbb{R}^d$로서,  
다음 조건들을 만족한다:

(i) $T$는 거의 모든 곳에서(a.e.) 미분 가능하며,  
    거의 모든 $z$에 대해 $\det J_T(z) \neq 0$ 이다.

(ii) $$T_{\#}\mu = \nu$$, 혹은 밀도 관점에서 쓰면 다음과 같다:

$$
q(y)
=
\frac{p\!\left(T^{-1}(y)\right)}
     {\left|\det J_T\!\left(T^{-1}(y)\right)\right|}
\qquad \Longleftrightarrow \qquad
p(x)
=
q(T(x)) \,\left|\det J_T(x)\right|
$$

여기서 $J_T$는 $T$의 Jacobian이다.  
이 식을 다음과 같이 쓸 수도 있다:  
$$q = T_{\#}p$$.

식 (1)에 나타난 양의 로그(log)를 자주 고려하게 된다:

$$
\log p(x)
=
\log q \circ T(x)
+
\log\!\left|\det J_T(x)\right|
$$

---

## p17. Normalizing Flows: Theoretical Side

$T_1, T_2, \ldots, T_n$이 normalizing flow라고 하고,  
$T = T_n \circ T_{n-1} \circ \cdots \circ T_1$이 $q$에서 $p$로 가는 normalizing flow라고 하자.  

그러면,

$$
\log p(x)
=
\log(q \circ T(x)) + \log\left|\det J_T(x)\right|
$$

또는,

$$
\log p(x)
=
\log(q \circ T(x))
+
\sum_{j=1}^{n}
\log\left|\det J_{T_j}(z_{j-1})\right|
$$

여기서  
$z_0 = x$,  
$z_1 = T_1(z_0)$,  
$z_2 = T_2(z_1)$,  
$\ldots,$  
$z_{n-1} = T_{n-1}(z_{n-2})$ 이다.

---

## p18. Recap : KL divergence

KL-발산의 성질은 Jensen 부등식으로부터 얻어진다:

$$
\begin{aligned}
- D_{\mathrm{KL}}(\pi_1 \mid \pi_2)
&= - \mathbb{E}_{\pi_1}\!\left( \log \frac{\pi_1}{\pi_2} \right) \\[6pt]
&= \mathbb{E}_{\pi_1}\!\left( \log \frac{\pi_2}{\pi_1} \right) \\[6pt]
&\le
\log\!\left(
        \mathbb{E}_{\pi_1}\!\left( \frac{\pi_2}{\pi_1} \right)
     \right)
\qquad\qquad\qquad\text{since log is concave} \\[6pt]
&=
\log\!\left(
        \int_{\{\pi_1(x) > 0\}}
        \pi_1(x)\, \frac{\pi_2(x)}{\pi_1(x)}\, dx
     \right) \\[6pt]
&\le \log(1)
\qquad\qquad\qquad\quad\quad\quad\quad\quad\text{as log increasing} \\[6pt]
&= 0.
\end{aligned}
$$

---

> **왜 ‘since log is concave’가 등장하는가?**  
> Jensen 부등식에 따르면, 어떤 함수가 **concave(오목)** 하면  
> $$\mathbb{E}[f(X)] \le f(\mathbb{E}[X])$$  
> 이 성립한다.  
> 여기서 로그 함수 $\log(\cdot)$는 concave이므로  
> $$\mathbb{E}_{\pi_1}\!\left( \log \frac{\pi_2}{\pi_1} \right)
> \le
> \log\!\left(
> \mathbb{E}_{\pi_1}\!\left( \frac{\pi_2}{\pi_1} \right)
> \right)$$  
> 이 부등식이 바로 Jensen 부등식의 직접적인 적용이다.
> 
> ---
> 
> **왜 ‘as log increasing’이 등장하는가?**  
> 로그 함수 $\log(\cdot)$는 **증가함수(increasing function)** 이고,  
> $$ \int \pi_1(x)\, \frac{\pi_2(x)}{\pi_1(x)} dx = 1 $$  
> 이므로  
> $$ \log\!\left(\int \cdots \right) \le \log(1) $$  
> 이 성립한다.  
> 즉, 내부 값이 1 이하라면 $\log$를 취해도 그 관계가 유지되며,  
> 결국 최종적으로 $0$ 이하가 되고 KL 발산의 음수 형태가 0 이하임을 보여준다.

---

## p19. Maximum Likelihood Estimation

**최적화 기반 $T$ 학습(Learning $T$ via Optimization)**

$$
\min D_{\mathrm{KL}}(T_{\#}p \mid q)
\quad\text{s.t.}\quad
\det \nabla T > 0 \;\;\text{and}\;\; T \in \mathcal{F}.
$$

$$D_{\mathrm{KL}}(T_{\#}p \mid q)
= D_{\mathrm{KL}}(p \mid T_{\#}^{-1} q)$$ 이므로,  
$\{x_i\}_{i=1}^n$이 $\mu$로부터의 i.i.d. 표본일 때, 표본 평균 근사는 다음과 같다:

$$
\min
\; -\frac{1}{n}
\sum_{i=1}^{n}
\left[
\log\, q \circ T(x_i)
+
\log \left| \det \nabla T(x_i) \right|
\right]
\quad\text{s.t.}\quad
\det \nabla T > 0
\;\;\text{and}\;\;
T \in \mathcal{F}.
$$

---

> 우리는 변환 $T$ 를 학습하고 싶다.  
> 즉, **복잡한 데이터 분포($x$ 의 분포 = $p$)**를 **단순한 기준 분포($z$ 의 분포 = $q$)**로  
> 최대한 잘 밀어(push) 넣도록 하는 변환 $T$ 를 찾는다는 뜻이다.
> 
>> $$T_{\#}p$$ 는 **푸시포워드(pushforward) 분포**로,  
>> “$x$-공간의 분포 $p$를 변환 $T$에 통과시켜  
>> $z$-공간의 분포로 옮겼을 때 얻어지는 분포”를 의미한다.  
>> 즉,  
>> $x \sim p$ 이면  
>> $$z = T(x)$$ 의 분포가 바로 $$T_{\#}p$$ 이다.  
>> 그리고 $x \mapsto T(x) = z$ 이다.
> 
> ---
> 
> 제한식 $\det \nabla T > 0$ 은  
> $T$ 가 거의 어디서나 가역이며 Jacobian의 행렬식이 0이 되지 않도록 보장한다는 뜻이다.
> 
> ---
> 
> 조건 $T \in \mathcal{F}$ 는  
> $T$ 가 미리 정한 함수 클래스(예: 특정 신경망 구조) 안에 있어야 한다는 뜻이다.
> 
> ---
> 
> 항등식  
> 
> $$
> D_{\mathrm{KL}}(T_{\#}p \mid q)
> =
> D_{\mathrm{KL}}(p \mid T_{\#}^{-1} q)
> $$
> 
> 는 KL 발산이 변수변환 아래에서 형태를 유지한다는 뜻이다.
>
>> KL 발산은 확률밀도에 대한 적분 형태로 정의되는데,  
>> 변수변환(예: $x \mapsto T(x)$ 또는 $z \mapsto T^{-1}(z)$)을 적용해도  
>> Jacobian 항이 나타나면서 양쪽 식이 서로 정확히 보정된다.  
>>
>> 즉,  
>> 분포를 $T$ 로 밀어(push) 옮기느냐,  
>> 혹은 $T^{-1}$ 로 끌어오느냐의 표현만 달라질 뿐  
>> “두 분포 사이의 정보 차이” 자체는 동일하게 유지되기 때문에  
>> 위 항등식이 성립한다는 뜻이다.
> 
> ---
>
> 표본 $\{x_i\}_{i=1}^n$ 을 이용해 기대값을  
> 표본 평균으로 근사할 수 있다는 뜻이다.
> 
> ---
>
> 최종 최적화식  
> 
> $$
> \min
> -\frac{1}{n}
> \sum_{i=1}^{n}
> \left[
> \log(q(T(x_i)))
> +
> \log \left| \det \nabla T(x_i) \right|
> \right]
> $$
> 
> 는 normalizing flow에서 사용되는 **로그-우도의 최대화**와 동등하다는 뜻이다.

---

## p20. Maximum Likelihood Estimation

<img src="/assets/img/lecture/probstat/13/image_9.png" alt="image" width="800px">

---

> **(1) 첫 번째 줄**
> 
> $$
> D_{\mathrm{KL}}(T_{\#}p \,\|\, q)
> =
> \int_{\mathbb{R}^d} (T_{\#}p)(y)\,\log\frac{(T_{\#}p)(y)}{q(y)}\,dy
> $$
> 
> KL 발산의 정의를 그대로 쓴 것이다.
>
>
> **(2) 두 번째 줄 — pushforward 밀도 $$(T_{\#}p)(y)$$ 를 전개한 단계**
> 
> $$
> \int_{\mathbb{R}^d}
> p(T^{-1}(y))\,\det\nabla T^{-1}(y)\,
> \log
> \frac{
> p(T^{-1}(y))\,\det\nabla T^{-1}(y)
> }{
> q(y)
> }
> dy
> $$
> 
> >
> > 이유:
> > pushforward 밀도는 정의상
> > 
> > $$
> > (T_{\#}p)(y)
> > =
> > p(T^{-1}(y))\,
> > \left|\det\nabla T^{-1}(y)\right|
> > $$
> > 
> > KL 식 안의 모든 $$(T_{\#}p)(y)$$ 를 이 정의로 치환했기 때문에 이렇게 바뀐다.
>
>
> **(3) 세 번째 줄**
> 
> 아래 줄은 앞 단계의 적분을  
> **변수변환 $y = T(x)$** 로 바꾸고 난 뒤의 결과이다:
>
> $$
> \int_{\mathbb{R}^d}
> p(x)\,
> \log
> \frac{
> p(x)
> }{
> q(T(x))\,\det\nabla T(x)
> }
> dx
> $$
>
> ---
>
> > **(a) 먼저, 치환을 적용한다:  $y = T(x)$**
> >
> > 이제 적분 변수는 $y$ 대신 $x$ 가 된다.  
> > 이때 **항등적으로**  
> > $$
> > T^{-1}(y) = x
> > $$
> > 이므로,
> > 기존 적분에 나타났던  
> > $p(T^{-1}(y))$ 는 단순히  
> > $$
> > p(x)
> > $$  
> > 로 바뀐다.
>
> ---
>
> > **(b) 미분 형식 변화:  $dy = \mid \det\nabla T(x) \mid \,dx$**
> >
> > 적분에서 변수 치환을 하면  
> > $dy$ 를 $dx$ 로 바꿔야 하는데,  
> > 이때 “부피 변화율”이 Jacobian determinant 이다.
> >
> > 즉,
> > $$
> > dy = \left|\det\nabla T(x)\right|\,dx.
> > $$
> >
> > 따라서 이전 적분에 있던 $dy$ 자리는  
> > $$
> > \left|\det\nabla T(x)\right|\,dx
> > $$  
> > 로 바뀐다.
>
> ---
>
> > **(c) 역함수 Jacobian 관계를 적용한다**
> >
> > inverse function theorem 에 의해,
> >
> > $$
> > \det\nabla T^{-1}(y)
> > =
> > \frac{1}{\det\nabla T(x)}.
> > $$
> >
> > 이것이 아주 중요하다.  
> > 왜냐하면 이전 단계의 integrand 에는 $p(T^{-1}(y))\det\nabla T^{-1}(y)$  
> > 그리고 measure $dy$ 안에는 $|\det\nabla T(x)|$  
> > 라는 두 개의 Jacobian 이 **곱 형태**로 등장하기 때문이다.
>
> ---
>
> > **(d) Jacobian 항들이 정확히 상쇄되는 과정**
> >
> > 치환 전 integrand 의 Jacobian 파트:
> > 
> > $$
> > \det\nabla T^{-1}(y)
> > $$
> >
> > 치환으로 생긴 measure 변화:
> > $$
> > dy = (\det\nabla T(x))\,dx
> > $$
> >
> > 둘을 곱하면:
> > 
> > $$
> > \det\nabla T^{-1}(y)\; \det\nabla T(x)
> > =
> > \frac{1}{\det\nabla T(x)}\; \det\nabla T(x)
> > =
> > 1.
> > $$
> >
> > 즉, **완전히 사라진다.**
>
> ---
>
> > **(e) log 안에서 일어나는 변화**
> >
> > 기존 식에서는  
> > 
> > $$
> > \log\frac{
> > p(T^{-1}(y))\,\det\nabla T^{-1}(y)
> > }{
> > q(y)
> > }
> > $$
> >
> > 이제 각각을 $x$ 기준으로 바꾸면:
> >
> > - $p(T^{-1}(y)) \rightarrow p(x)$  
> > - $q(y) \rightarrow q(T(x))$  
> > - $\det\nabla T^{-1}(y) \rightarrow 1/\det\nabla T(x)$
> >
> > 따라서 log 내부는:
> > 
> > $$
> > \log\frac{
> > p(x)
> > }{
> > q(T(x))\,\det\nabla T(x)
> > }.
> > $$
>
> ---
>
> > **(f) 최종 정리**
> >
> > 적분 변수는 전부 $x$ 기준,
> > Jacobian 은 log 안과 measure 에서 정확히 정리되고,
> > 전체 결과는 다음과 같이 깔끔해진다:
> >
> > $$
> > \int_{\mathbb{R}^d}
> > p(x)\,
> > \log
> > \frac{
> > p(x)
> > }{
> > q(T(x))\,\det\nabla T(x)
> > }
> > dx
> > $$
>
> **(4) 네 번째 줄 — 로그 분리 및 적분 분리**
> >
> 로그 성질 적용:
> 
> $$
> \log \frac{p(x)}{q(T(x))\,\det\nabla T(x)}
> =
> \log p(x)
> -
> \log q(T(x))
> -
> \log \det\nabla T(x)
> $$
> 
> 적분의 선형성 때문에 세 항을 분리 가능:
>  
> $$=
> \int_{\mathbb{R}^d} p(x)\log p(x)\,dx
> -
> \int_{\mathbb{R}^d} p(x)\log q(T(x))\,dx
> -
> \int_{\mathbb{R}^d} p(x)\log\det\nabla T(x)\,dx
> $$

---

## p21. Maximum Likelihood Estimation

<img src="/assets/img/lecture/probstat/13/image_10.png" alt="image" width="800px">

---

## p22. Normalizing Flows의 예시

- 비선형성 함수 $h : \mathbb{R} \to \mathbb{R}$

- Jacobian 계산에는 “행렬 행렬식 보조정리(matrix determinant lemma)”가 필요하다:

  $$
  \det(A + u v^{T})
  =
  \left( 1 + v^{T} A^{-1} u \right)\det(A)
  $$

- 그림: $h$의 예시들

<img src="/assets/img/lecture/probstat/13/image_11.png" alt="image" width="600px">

---

> **행렬 행렬식 보조정리(matrix determinant lemma)**란?
>
> 이 보조정리는  
> 행렬 $A$ 에 **랭크 1(rank-1) 행렬** $u v^{T}$ 를 더한 행렬  
> $A + u v^{T}$ 의 행렬식을  
>
> $$
> \det(A + u v^{T})
> =
> \left( 1 + v^{T} A^{-1} u \right)\det(A)
> $$
>
> 형태로 매우 간단하게 계산할 수 있게 해주는 정리이다.
>
> 즉,  
> **큰 행렬의 행렬식을 직접 계산하지 않고**,  
> **$A^{-1}$과 $u, v$의 내적만으로**  
> 새로운 행렬의 행렬식을 얻을 수 있게 해주는 도구이다.
>
> 이 성질은 normalizing flow에서 Jacobian 행렬이  
> “기본 행렬 + rank-1 보정” 형태로 나올 때  
> 연산을 크게 줄여주는 핵심 트릭이다.
>
> ---
>
> **그림의 의미 (Examples of $h$)**  
>
> 그림은 normalizing flow에서 사용되는  
> **비선형성 함수 $h$** 의 여러 예시를 보여준다.
>
> - 파란색: 완만한 S-curve 형태의 비선형 함수  
> - 빨간색: 단순 선형 함수  
> - 초록색: sigmoid-like 형태의 비선형 함수  
>
> 이러한 $h$들은 flow 모델에서  
> **복잡한 분포를 만들기 위한 비선형 변형의 기본 요소**로 사용된다.
>
> 즉,  
> *간단한 분포(z)를 복잡한 분포(x)로 만들기 위해  
> 어떤 모양의 비선형 함수를 사용할 수 있는지 시각적으로 보여주는 그림*이다.

---

## p23. Planar Flows (Single Layer SVM)

- $u, w \in \mathbb{R}^d$, $b \in \mathbb{R}$, 그리고 비선형성 $h : \mathbb{R} \to \mathbb{R}$가 주어졌을 때,  
  planar flow $T_{pf} : \mathbb{R}^d \to \mathbb{R}^d$ 를 다음과 같이 정의한다:

  $$
  T_{pf}(z)
  =
  z + u\, h(w^{T} z + b).
  $$

- 우리는 다음을 계산한다:

  $$
  J_{T_{pf}}(z)
  =
  I_d
  +
  (u w^{T})\, h'(w^{T} z + b)
  \;\;\Longrightarrow\;\;
  \det J_{T_{pf}}(z)
  =
  1 + w^{T} u\, h'(w^{T} z + b).
  $$

- 그림: Planar flow에서의 변형 예시

<img src="/assets/img/lecture/probstat/13/image_12.png" alt="image" width="250px">

---

> (1) 변환 정의식  
> 
>   $$T_{pf}(z) = z + u\,h(w^{T}z + b)$$  
> 
> - 기본 입력 $z$ 에 작은 변화량을 더해 주는 구조이다.  
> - 이 변화량은 **스칼라 값** $h(w^{T}z + b)$ 를 계산한 뒤,  
>   이를 **벡터** $u$ 방향으로 늘려주어 적용하는 형태이다.  
> - 즉, “$z$ 를 $u$ 방향으로 밀어주는(push) 비선형 변형”이라고 볼 수 있다.  
> - $w^{T}z + b$ 는 하이퍼플레인(평면)을 정의하며, $h(\cdot)$ 는 그 평면을 기준으로  
>   어느 정도 변형을 줄지 결정한다.
>
> ---
>
> (2) Jacobian 계산식  
> 
>   $$
>   J_{T_{pf}}(z)
>   =
>   I_d + (u w^{T})\, h'(w^{T}z + b)
>   $$
> 
> - $T_{pf}$ 는 항등변환(identity) $I_d$ 에  
>   랭크-1(rank-1) 행렬 $u w^{T}$ 를 더한 구조이다.  
> - $h'(w^{T}z + b)$ 는 스칼라이므로, 전체로는 “랭크-1 업데이트(Jacobian)”가 된다.  
> - 이 구조 덕분에 행렬식(det)을 매우 효율적으로 계산할 수 있다.
>
> ---
>
> (3) 행렬식(det) 계산식  
> 
>   $$
>   \det J_{T_{pf}}(z)
>   =
>   1 + w^{T}u\, h'(w^{T} z + b)
>   $$
> 
> - 이는 **matrix determinant lemma**  
>   $\det(A + uv^{T}) = (1 + v^{T} A^{-1} u)\det(A)$  
>   를 그대로 적용한 결과이다.  
> - 여기서는 $A = I_d$ 이므로 $\det(I_d)=1$, $A^{-1} = I_d$ 가 되어  
>   계산이 매우 단순해진다.  
> - 따라서 planar flow는 “가벼운 비용으로” Jacobian determinant 를  
>   계산할 수 있게 설계된 구조이다.
>
> ---
>
> (4) 그림의 의미  
>
> - 그림의 파란 직선은 벡터 $w$ 가 정의하는 **하이퍼플레인(직선)** 을 나타낸다.  
> - 이 직선에 수직 방향이 $w$ 의 방향이며,  
>   planar flow는 이 평면 주변에서만 변형이 세게 일어난다.  
> - 점선으로 표시된 두 개의 선은 $w^{T} z + b = \text{constant}$ 형태의 레벨셋(level set)이며,  
>   flow가 해당 영역 근처에서 공간을 **뒤틀고(push)** 있다는 것을 보여준다.  
> - 화살표들은 실제로 입력 공간이 planar flow에 의해  
>   “어떤 방향으로 변형되는지” 시각적으로 표현한 것이다.  
> - 요약하면,  
>   **planar flow는 어떤 하나의 평면에 기반하여 데이터를 u 방향으로 비선형적으로 밀어서  
>   분포를 더 복잡하게 만드는 변환**이다.

---

## p24. Radial Flows (Physically-informed NN)

- $$a \in \mathbb{R}_{>0}$$, $$b \in \mathbb{R}$$, 그리고 $$z_0 \in \mathbb{R}^d$$ 가 주어졌을 때,  
  우리는 radial flow $$T_{rf} : \mathbb{R}^d \rightarrow \mathbb{R}^d$$를 다음과 같이 정의한다:

  $$
  T_{rf}(z)
  =
  z
  +
  \frac{b}{\,a + \|z - z_0\|_2\,}
  (z - z_0).
  $$

<img src="/assets/img/lecture/probstat/13/image_13.png" alt="image" width="250px">

---

## p25. Sylvester Flows (Single Layer MLP)

- 양의 정수 $m < d$,  
  $A \in \mathbb{R}^{d \times m}$,  
  $B \in \mathbb{R}^{d \times m}$,  
  $b \in \mathbb{R}^m$,  
  $h : \mathbb{R} \rightarrow \mathbb{R}$ 가 주어졌을 때,  
  Sylvester flow $T_{\text{syl}} : \mathbb{R}^d \rightarrow \mathbb{R}^d$ 를 다음과 같이 정의한다:

  $$
  T_{\text{syl}}(z)
  =
  z + A\,h(B^T z + b).
  $$

- 계산하면,

  $$
  J_{T_{\text{syl}}}(z)
  =
  I_d
  +
  A\,\mathrm{diag}\!\big(h'(B^T z + b)\big)\,B^T.
  $$

- Sylvester의 determinant 항등식(Kobyzev et al. (2020))에 의해, 다음을 얻는다:

  $$
  \det J_{T_{\text{syl}}}(z)
  =
  \det\!\left(
      I_d
      +
      A\,\mathrm{diag}\!\big(h'(B^T z + b)\big)\,B^T
  \right)
  $$
  
  $$=
  \det\!\left(
      I_m
      +
      \mathrm{diag}\!\big(h'(B^T z + b)\big)\,B^T A
  \right).
  $$

- 참고 문헌:  
  Kobyzev, I., Prince, S. J., and Brubaker, M. A. (2020).  
  *Normalizing flows: An introduction and review of current methods.*  
  IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):3964–3979.

---

> Sylvester flow는 다층 퍼셉트론(MLP)의 단일 레이어 형태를 이용해  
> 입력 벡터 $z \in \mathbb{R}^d$ 를 변환하는 normalizing flow의 한 종류이다.
>
> ---
>
> 먼저, $m < d$ 인 양의 정수와  
> 행렬 $A \in \mathbb{R}^{d \times m}$, $B \in \mathbb{R}^{d \times m}$,  
> 그리고 편향 $b \in \mathbb{R}^m$,  
> 비선형 함수 $h : \mathbb{R} \to \mathbb{R}$ 를 정의한다.
>
> ---
>
> 그런 다음 변환
>
> $$
> T_{\text{syl}}(z) = z + A\,h(B^T z + b)
> $$
>
> 를 사용해 입력 $z$ 를 변형한다.  
> 이는 선형 변환 $B^T z + b$에 비선형 $h$를 취하고,  
> 이를 다시 행렬 $A$ 를 통해 원래 공간 $\mathbb{R}^d$로 올려서  
> 잔차 형태(residual form)로 더해주는 구조이다.
>
> ---
>
> Jacobian은
>
> $$
> J_{T_{\text{syl}}}(z)
> =
> I_d + A\,\mathrm{diag}(h'(B^T z + b))\,B^T
> $$
>
> 와 같이 계산된다.  
> 여기에서 $\mathrm{diag}(h'(\cdot))$ 는 비선형 함수의 도함수를  
> 각 성분에 대해 대각행렬로 만든 것이다.
>
> ---
>
> 그러나 일반적으로 $d \times d$ Jacobian의 행렬식을 직접 계산하는 것은 어렵다.  
> 따라서 **Sylvester의 행렬식 항등식**을 이용해 계산을 크게 단순화한다.
>
> ---
>
> Sylvester의 항등식에 따르면
>
> $$
> \det(I_d + A D B^T)
> =
> \det(I_m + D B^T A)
> $$
>
> 로 바꿀 수 있다. (여기에서 $D = \mathrm{diag}(h'(B^T z + b))$)
>
> 즉, $d \times d$ 행렬식을  
> **더 작은 크기인 $m \times m$ 행렬식**으로 변환할 수 있다.  
> ($m < d$ 이므로 계산량이 매우 감소한다.)
>
> ---
>
> 따라서 최종적으로
>
> $$
> \det J_{T_{\text{syl}}}(z)
> =
> \det\!\big(
> I_m
> +
> \mathrm{diag}(h'(B^T z + b))\,B^T A
> \big)
> $$
>
> 와 같이 계산된다.
>
> ---
>
> 이 구조 덕분에 Sylvester flow는  
> **표현력은 충분히 유지하면서도 Jacobian 행렬식 계산을 효율적으로 수행**할 수 있어  
> normalizing flow 모델에서 매우 유용하게 쓰인다.

---

## p26. Generic Neural Networks

- 정규화 흐름(normalizing flows)의 결합 레이어(coupling layers)는  
  신경망(neural networks)을 사용하여 모델링될 수 있다.

- 일반적으로, 신경망은 가역(invertible)이 아니다.  
  그러나, 가역성은 종종 네트워크가 전단사(bijective)임을 보임으로써 보장된다.

**Lemma (보조정리)**

만약 $NN(\cdot) : \mathbb{R} \rightarrow \mathbb{R}$이,  
다층 퍼셉트론(multilayer perceptron)이고,  
모든 가중치가 양수이며,  
모든 활성화 함수가 엄격히 단조(strictly monotone)라면,  
$NN(\cdot)$ 은 엄격히 단조 함수이다.

**가역성을 강제로 만족시키는 다른 방법들:**

- $NN(\cdot) : \mathbb{R} \rightarrow \mathbb{R}_{>0}$이 되도록 강제하고  
  그것을 적분(integrate)한다.

- $NN(\cdot) : \mathbb{R} \rightarrow \mathbb{R}$이 볼록(convex)이 되도록 강제하고  
  그것을 미분(differentiate)한다  
  (입력에 대해 볼록한 신경망 - input convex neural networks).

---

> 첫 번째 방법:  
> $NN(\cdot) : \mathbb{R} \rightarrow \mathbb{R}_{>0}$ 이 되도록 강제하고, 그것을 적분한다.  
>  
> ---
>
> > 핵심 아이디어:  
> > NN의 출력이 항상 양수라는 것은 NN이 “입력이 증가하면 기울기가 절대로 음수가 되지 않는”  
> > 형태의 함수를 만들 수 있다는 뜻이다.  
> >
> > NN(x) > 0 이므로, 다음과 같은 함수를 정의할 수 있다:  
> >  
> > $$
> > F(x) = \int NN(t)\, dt
> > $$
> >  
> > 이때 F(x)는 입력이 증가할수록 항상 증가하는 **단조 증가(strictly monotone)** 함수가 된다.  
> >  
> > 단조 증가 함수는 서로 다른 입력이 같은 출력을 만들 수 없으므로 가역적이다.  
> >
> > 즉,  
> > **양수 출력 NN → 적분 → 단조 증가 함수 → 가역성 확보**.  
>
> ---
>
> 두 번째 방법:  
> $NN(\cdot) : \mathbb{R} \rightarrow \mathbb{R}$ 이 볼록(convex)이 되도록 강제하고, 그것을 미분한다.  
>
> ---
>
> > 핵심 아이디어:  
> > 볼록 함수 $\phi(x)$ 는 미분했을 때 $\phi'(x)$ 가 항상 **단조 증가**한다.  
> > 즉, 입력 x가 증가하면 기울기 또한 절대로 감소하지 않는다.  
> >
> > 그래서 NN을 “입력에 대해 볼록한 신경망(input convex neural network)”으로 만들었을 때,  
> > NN을 $\phi(x)$ 라고 보고,  
> > 그 도함수  
> >  
> > $$
> > \phi'(x)
> > $$
> >  
> > 를 사용하면 자동으로 단조 증가하는 함수가 된다.
> >
> > 단조 증가 함수는 가역성을 만족한다.
> >
> > 즉,
> > **볼록 NN → 미분 → 단조 증가 함수 → 가역성 확보**.

---

## p27. Input Convex Neural Network

- PICNN $$f(x, y; \theta)$$는 두 개의 은닉층 집합 $$\{u_i\}_{i=1}^k$$와 $$\{z_i\}_{i=1}^k$$에 의해  
  기술되며, 각각 $x$-경로(x-path)와 $y$-경로(y-path)에 대응한다.  

- $K$-층 PICNN의 구조는 다음과 같은 반복되는 은닉 유닛들에 의해 주어진다:  

  $$
  \begin{aligned}
  u_{i+1}
  &=
  \tilde{g}_i\!\left(\tilde{W}_i u_i + \tilde{b}_i\right)
  \\[10pt]
  z_{i+1}
  &=
  g_i\Big(
        W_i^{(z)}
        \big(
            z_i \circ [ W_i^{(zu)} u_i + b_i^{(zu)} ]
        \big)
        +
        W_i^{(y)}
        \big(
            y \circ [ W_i^{(yu)} u_i + b_i^{(yu)} ]
        \big)
        +
        W_i^{(u)} u_i
        +
        b_i
    \Big)
  \end{aligned}
  $$

  여기서 $\tilde{g}_i$ 와 $g_i$ 는 비선형 활성화 함수들이다.  
  PICNN의 최종 스칼라 값 출력은  

  $$
  f(x, y; \theta) = z_K
  $$

  이다.  

---

> PICNN이 $x$에 대해 볼록(convex)이 되는 핵심 이유는  
> 네트워크 구조 전체가 “볼록성을 보존하는 연산들”로만 구성되기 때문이다.  
>   
> 첫 번째로, $u$-경로는  
> $$u_{i+1} = \tilde{g}_i(\tilde{W}_i u_i + \tilde{b}_i)$$  
> 의 형태인데, 여기서 $$\tilde{W}_i$$ 는 비음수(nonnegative) 행렬로 제한된다.  
> 비음수 행렬과 단조 증가 활성화 함수 $$\tilde{g}_i$$ 는  
> 볼록성을 깨지 않는 연산이다.  
>   
> 또한 매우 중요한 점은, 비록 위 식이 $u_i$만을 포함하는 것처럼 보이지만  
> 실제로는 $$u_0 = x$$ 에서 시작해  
> $$u_1 = \tilde{g}_0(\tilde{W}_0 x + \tilde{b}_0), \quad u_2 = \tilde{g}_1(\tilde{W}_1 u_1 + \tilde{b}_1)$$  
> 와 같이 모든 $u_i$ 가 본질적으로 $x$의 함수라는 것이다.  
> 즉, $u_i = u_i(x)$ 이므로 $u$-경로는 $x$와 직접적 관계가 있으며,  
> 그 업데이트 규칙이 볼록성을 보존하는 구조이기 때문에  
> $u_{i+1}$ 역시 $x$에 대해 볼록이 된다.  
> 이는  
> “볼록 함수에 비음수 조합 + 단조 활성화 = 여전히 볼록”  
> 이라는 볼록성 보존 법칙에서 비롯된다.  
>   
> 두 번째로, $z$-경로는  
> $$
> z_{i+1}
> =
> g_i\!\Big(
>       W_i^{(z)}(z_i \circ [ W_i^{(zu)}u_i + b_i^{(zu)} ])
>       +
>       W_i^{(y)}(y \circ [ W_i^{(yu)}u_i + b_i^{(yu)} ])
>       +
>       W_i^{(u)}u_i
>       +
>       b_i
> \Big)
> $$  
> 와 같은 구조를 갖는다.  
>   
> 여기서 중요한 점은 $W_i^{(u)}$, $W_i^{(zu)}$, $W_i^{(yu)}$ 등이  
> 모두 **비음수 행렬로 제한**된다는 사실이다.  
> 비음수 조합은 볼록성을 보존하며,  
> $u_i$ 자체가 이미 $x$에 대해 볼록이므로  
> $u_i$ 를 포함하는 모든 항들이 여전히 $x$에 대해 볼록이다.  
>   
> 또한 Hadamard 곱(원소별 곱) $a \circ b$ 는  
> 두 항 중 하나가 비음수일 경우 볼록성을 보존하는 연산이며,  
> 이 조건 또한 네트워크 설계에서 보장된다.  
> 따라서  
> $z_i \circ [ W_i^{(zu)} u_i + b_i^{(zu)} ]$  
> 역시 볼록성을 유지한다.  
>   
> 세 번째로, 마지막에는 단조 증가 활성화 함수 $g_i$ 를 적용한다.  
> 단조 증가 함수는 볼록함수의 볼록성을 절대 깨지 않는다.  
> 즉, $g_i(\cdot)$ 의 출력도 볼록이다.  
>   
> 이런 식으로 각 계층에서  
> “볼록성 보존 → 볼록성 보존 → 볼록성 보존”  
> 구조가 반복되므로,  
> $z_K$ 는 $x$에 대해 확실하게 볼록(convex)하게 된다.  
>   
> PICNN이 본질적으로 convexity를 강제할 수 있는 이유는  
> **모든 경로가 볼록성을 파괴할 수 있는 연산(음수 가중치, 비단조 활성화 등)을 제거하고,  
> 볼록성을 보존하는 연산만으로 전체 네트워크를 구성했기 때문**이다.  
>   
> 요약하면:  
>   
> - 비음수 가중치 → 볼록 조합  
> - 단조 증가 활성화 → 볼록성 유지  
> - Hadamard 곱의 구조적 제약 → 볼록성 유지  
> - $u$ 경로가 실제로는 $x$로부터 시작하는 연쇄 구조 → $x$에 대한 볼록성 유지  
> - $z$ 경로 역시 볼록성을 보존하는 연산으로 구성  
>   
> 결과적으로  
> $$f(x, y; \theta) = z_K$$  
> 는 $x$에 대해 볼록함수(convex function)가 된다.  
 
---

## p28. Input Convex Neural Network

- **명제(Proposition) 1, Amos et al. (2017)**    
  모든 $\{ W_i^{(z)} \}_{i=1}^{k-1}$ 이 비음수(non-negative)이고,  
  모든 함수 $\tilde{g}_i$ 가 볼록(convex)이며  
  단조 증가(non-decreasing)일 때,  
  함수 $f$ 는 $y$ 에 대해 볼록(convex)이다.

- 증명은  
  볼록 함수들의 **볼록 조합(convex combinations)** 과  
  볼록 함수들의 **합성(compositions)** 역시 볼록이라는  
  단순한 아이디어를 따른다.

- $f^j$ 가 $x_j$ 에 대한 볼록 함수가 되도록 보장하려면,  
  $W^{(z)}$ 의 모든 항목이 비음수(non-negative)가 되도록 제약해야 한다.

- 학습 과정은 (3)번 식의 $KL(\theta)$가 최소화되도록  
  파라미터 $\theta$ 를 최적화한다.

- 정규화 항(regularization term)은  
  모든 층(layer)의 모든 $W^{(z)}$ 가중치 행렬 항목이  
  양수(positive)가 되도록 보장하기 위해 추가되며,  
  다음과 같은 식을 갖는다:

  $$
  -\frac{1}{n}
  \sum_{i=1}^{n}
  \Bigg(
      \log g \circ T(\theta)(x_i)
      +
      \sum_{j=1}^{d}
      \log(\nabla T(\theta))_j(x_i)
  \Bigg)
  +
  \lambda
  \sum_{j=1}^{d}
  \sum_{k=1}^{K}
  \left\|
      \max\!\big(-(W_j^{(z)})_k,\, 0\big)
  \right\|_{F}^{2}.
  $$

---

> 위 식은 두 부분으로 구성되어 있다.  
>   
> 첫 번째 부분  
> $$-\frac{1}{n}\sum_{i=1}^{n}\Big(\log g\!\circ\!T(\theta)(x_i)+\sum_{j=1}^{d}\log(\nabla T(\theta))_j(x_i)\Big)$$  
> 는 normalizing flow 학습에서 사용하는 **기본 로그-우도(log-likelihood) 최대화 항**이다.  
> 이는 변환 $T(\theta)$ 를 적용한 뒤 얻어지는 분포가  
> 데이터에 잘 맞도록 만드는 역할을 한다.  
>   
> 두 번째 부분  
> $$\lambda\sum_{j=1}^{d}\sum_{k=1}^{K}\left\|\max(-(W_j^{(z)})_k,0)\right\|_F^2$$  
> 는 **정규화 항(regularization term)** 으로,  
> $W^{(z)}$ 의 항목이 **음수가 되지 않도록 강제**하는 역할을 한다.  
>   
> 여기서 $\max(-(W_j^{(z)})_k, 0)$ 는  
> $W^{(z)}$ 의 요소가 음수일 경우 그 절댓값을 반환하고,  
> 양수일 경우 0을 반환한다.  
> 따라서 이 항을 제곱해 패널티로 더하면  
> **음수 가중치가 등장할 때에만 비용이 증가**하게 된다.  
>   
> 즉, 이 항은  
> “$W^{(z)}$ 의 모든 요소가 비음수(non-negative)가 되도록 만드는 페널티”  
> 로 작동하며, 이것이 PICNN의 **볼록성(convexity) 보장을 위한 핵심 제약**이다.  
>   
> 최종적으로 이 정규화 항을 포함시키면  
> 학습 과정 동안 $W^{(z)}$ 의 값이 음수가 되지 않도록 유지되어  
> 네트워크의 볼록성과 가역성(invertibility) 조건을 지속적으로 만족하게 된다.  