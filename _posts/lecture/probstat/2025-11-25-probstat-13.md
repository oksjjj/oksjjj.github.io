---
layout: post
title: "[확률과 통계] 13주차"
date: 2025-11-25 14:00:00 +0900
categories:
  - "대학원 수업"
  - "확률과 통계"
tags: []
---

> 출처: 확률과 통계 – 박성우 교수님, 고려대학교 (2025)

## p2. Motivation  

<img src="/assets/img/lecture/probstat/13/image_1.png" alt="image" width="720px">

---

## p3. Motivation  

<img src="/assets/img/lecture/probstat/13/image_2.png" alt="image" width="800px">

---

## p4. Change of Variables

- **변수 변경 (1차원 경우):**  
  만약 $X = f(Z)$ 이고 $f(\cdot)$ 가 단조이며 역함수 $Z = f^{-1}(X) = h(X)$ 가 존재하면:

  $$
  p_X(x) = p_Z(h(x))\, \mid h'(x) \mid
  $$

- 이전 예시:  
  $X = f(Z) = 4Z$, $Z \sim \mathcal{U}[0,2]$ 일 때 $p_X(4)$ 는 무엇인가?

  - $h(X) = X/4$
  - $p_X(4) = p_Z(1)\, h'(4) = \frac{1}{2} \times \mid 1/4 \mid = \frac{1}{8}$

- 더 흥미로운 예시:  
  $X = f(Z) = \exp(Z)$, $Z \sim \mathcal{U}[0,2]$ 일 때 $p_X(x)$ 는 무엇인가?

  - $h(X) = \ln(X)$
  - $p_X(x) = p_Z(\ln(x))\, \mid h'(x) \mid = \frac{1}{2x}$ for $x \in [\exp(0), \exp(2)]$

- $p_X(x)$ 의 “모양(shape)” 은 prior $p_Z(z)$ 보다 더 복잡할 수 있음에 유의하자.

---

> **수식 도출 과정**
>
> **1단계. 확률은 동일 — 변수만 다르다**
>
> 변수변환의 출발점은 매우 단순하다.
>
> > $X$가 어떤 작은 구간에 있을 확률 = $Z$가 그 구간으로 변환된 구간에 있을 확률
>
> 즉,
>
> $$
> P(X \in [x, x+dx]) = P(Z \in [z, z+dz])
> $$
>
> 여기서  
>
> $$z = h(x)$$
>
> ---
>
> **2단계. 양쪽을 확률밀도로 표현**
>
> 왼쪽(X 쪽):
>
> $$
> P(X \in [x, x+dx]) = p_X(x)\, dx
> $$
>
> 오른쪽(Z 쪽):
>
> $$
> P(Z \in [z, z+dz]) = p_Z(z)\, dz
> $$
>
> 두 확률이 같으므로:
>
> $$
> p_X(x)\, dx = p_Z(z)\, dz
> $$
>
> ---
>
> **3단계. $Z = h(X)$ 를 미분하여 $dz$와 $dx$ 관계 구하기**
>
> $$
> z = h(x)
> $$
>
> 양변 미분하면:
>
> $$
> dz = h'(x)\, dx
> $$
>
> ---
>
> **4단계. $dz = h'(x)\, dx$ 를 확률식에 대입**
>
> 확률식:
>
> $$
> p_X(x)\, dx = p_Z(z)\, dz
> $$
>
> 여기에 $z = h(x)$, $dz = h'(x)\, dx$를 대입하면:
>
> $$
> p_X(x)\, dx = p_Z(h(x))\, h'(x)\, dx
> $$
>
> 양변에서 $dx$ 제거:
>
> $$
> p_X(x) = p_Z(h(x))\, h'(x)
> $$
>
> ---
>
> **5단계. 왜 절댓값이 필요한가?**
>
> 단조 감소일 때:
>
> $$
> h'(x) < 0
> $$
>
> 그러면
>
> $$
> p_Z(h(x))\, h'(x)
> $$
>
> 이 음수가 되어버린다 → 확률밀도는 음수가 될 수 없음.
>
> 또한 변수 변환에서 필요한 것은
>
> > 방향이 아니라 “길이의 변화량(스케일)”
>
> 이며, 이는 항상 양수여야 한다.
>
> 따라서 절댓값을 붙여주면:
>
> $$
> p_X(x) = p_Z(h(x))\, |h'(x)|
> $$

---

## p5. Change of Variables

- $Z$를 $[0,1]^n$ 구간에의 균등(uniform) 확률벡터라고 하자.
- $X = A Z$ 이고, $A$는 역행렬 $W = A^{-1}$을 갖는 정방행렬이라고 하자.  
  이때 $X$는 어떻게 분포하는가?
- 기하학적으로, 행렬 $A$는 단위 하이퍼큐브 $[0,1]^n$을 하나의 parallelotope(평행다포체)로 사상한다.
- 하이퍼큐브와 parallelotope는 정사각형/정육면체와 평행사변형/평행육면체를 고차원으로 일반화한 개념이다.

<img src="/assets/img/lecture/probstat/13/image_3.png" alt="image" width="600px">

그림: 행렬
$$A =
\begin{pmatrix}
a & c \\
b & d
\end{pmatrix}
$$ 는 단위 정사각형을 평행사변형으로 사상한다.

---

## p6. Change of Variables

- parallelotope의 부피(volume)는 행렬 $A$의 행렬식(determinant)의 절댓값과 동일하다.

  $$
  \det(A)
  =
  \det
  \begin{pmatrix}
  a & c \\
  b & d
  \end{pmatrix}
  =
  ad - bc
  $$

  <img src="/assets/img/lecture/probstat/13/image_4.png" alt="image" width="480px">

- $X = A Z$라고 하자. 여기서 $A$는 정방의 가역 행렬이며, 그 역행렬은 $W = A^{-1}$이다.  
  $X$는 면적이 $|\det(A)|$인 parallelotope 상에서 균일하게 분포한다.  
  따라서 다음을 얻는다:

  $$
  p_X(x) = p_Z(Wx)\,/\,|\det(A)|
  $$

  $$
  = p_Z(Wx)\,|\det(W)|
  $$

- 왜냐하면 $W = A^{-1}$이면  

  $$
  \det(W) = \frac{1}{\det(A)}
  $$

- 1차원 경우의 공식과의 유사성에 주목하라.

---

> **1차원 공식과의 유사성 설명**
>
> 다변량 선형변환에서도 밀도변환 공식의 핵심 구조는  
> **“원래 밀도를 역변환한 지점에서 평가하고, 스케일 변화량의 절댓값으로 나눈다”**는 점에서  
> 1차원 변수변환 공식과 완전히 동일하다.
>
> 1차원에서는  
> $$
> p_X(x) = p_Z(h(x))\,|h'(x)|
> $$
> 로서 스케일 변화량이 도함수 $h'(x)$ 하나였다.
>
> 다변량에서는  
> $$
> p_X(x) = p_Z(Wx)\,|\det(W)|,
> $$
> 여기서 스케일 변화량이 **Jacobian의 절댓값 = $\det(W)$** 로 일반화되었을 뿐이다.
>
> 즉,  
> **1D: 길이(scale) 변화 → $|h'(x)|$**  
> **nD: 부피(volume) 변화 → $|\det(W)|$**
>
> 따라서 다변량 공식은 1차원 변수변환의 자연스러운 확장(extension)이다.

---

## p7. Change of Variables

- $A$를 통한 선형변환의 경우, 부피의 변화는 행렬 $A$의 행렬식(determinant)로 주어진다.

- 비선형 변환 $f(\cdot)$의 경우, 선형화된(linearized) 부피 변화는  
  $f(\cdot)$의 Jacobian의 행렬식(determinant)으로 주어진다.

- **변수변환(일반 경우)**:  
  $f : \mathbb{R}^n \to \mathbb{R}^n$ 이 $X = f(Z)$ 와 $Z = f^{-1}(X)$ 를 만족하도록 가역일 때,  
  $Z$와 $X$의 대응(mapping)은 다음과 같다:

  $$
  p_X(x)
  =
  p_Z\!\bigl(f^{-1}(x)\bigr)
  \left|
    \det\!\left(
      \frac{\partial f^{-1}(x)}{\partial x}
    \right)
  \right|
  $$

- **Note 0**:  
  이는 이전 1차원 경우  
  $p_X(x) = p_Z(h(x))\,|h'(x)|$  
  를 일반화한 것이다.

- **Note 1**:  
  VAE와 달리, $x, z$는 연속적이어야 하며 같은 차원을 가져야 한다.  
  예를 들어, $x \in \mathbb{R}^n$ 이면 $z \in \mathbb{R}^n$ 이다.

- **Note 2**:  
  어떤 가역행렬 $A$에 대해서도  
  $\det(A^{-1}) = \det(A)^{-1}$ 이다.

  따라서,

  $$
  p_X(x)
  =
  p_Z(z)
  \left|
    \det\!\left(
      \frac{\partial f(z)}{\partial z}
    \right)
  \right|^{-1}
  $$

---

## p8. Example: Two-dimensional Change of Variables

- $Z_1$과 $Z_2$를 결합밀도 $p_{Z_1,Z_2}$를 갖는 연속 확률변수라고 하자.

- $u : \mathbb{R}^2 \to \mathbb{R}^2$ 를 가역변환이라고 하자.  
  두 입력과 두 출력으로 이루어져 있으며, 이를 $u = (u_1, u_2)$로 표기한다.

- $v = (v_1, v_2)$ 를 그 역변환이라고 하자.

- $X_1 = u_1(Z_1, Z_2)$ 이고 $X_2 = u_2(Z_1, Z_2)$ 라고 하자.  
  그러면 $Z_1 = v_1(X_1, X_2)$ 이고 $Z_2 = v_2(X_1, X_2)$ 이다.

  $$p_{X_1,X_2}(x_1,x_2)$$

  $$=
  p_{Z_1,Z_2}\bigl(v_1(x_1,x_2),\, v_2(x_1,x_2)\bigr)
  \left|
  \det
  \begin{pmatrix}
  \dfrac{\partial v_1(x_1,x_2)}{\partial x_1} &
  \dfrac{\partial v_1(x_1,x_2)}{\partial x_2} \\[6pt]
  \dfrac{\partial v_2(x_1,x_2)}{\partial x_1} &
  \dfrac{\partial v_2(x_1,x_2)}{\partial x_2}
  \end{pmatrix}
  \right|
  \quad \text{(inverse)}
  $$

  $$=
  p_{Z_1,Z_2}(z_1,z_2)
  \left|
  \det
  \begin{pmatrix}
  \dfrac{\partial u_1(z_1,z_2)}{\partial z_1} &
  \dfrac{\partial u_1(z_1,z_2)}{\partial z_2} \\[6pt]
  \dfrac{\partial u_2(z_1,z_2)}{\partial z_1} &
  \dfrac{\partial u_2(z_1,z_2)}{\partial z_2}
  \end{pmatrix}
  \right|^{-1}
  \quad \text{(forward)}
  $$

---

> **inverse의 의미**  
> inverse는 **역변환 Jacobian**을 사용했다는 뜻이다.  
> 즉, $x \mapsto z$ 로 가는 역함수 $v(x)$를 미분하여  
> $ \frac{\partial v(x)}{\partial x} $ 의 Jacobian determinant를 쓰는 방식이다.  
> 이는  
> $$
> p_X(x) = p_Z(v(x))\,\left|\det\!\left(\frac{\partial v(x)}{\partial x}\right)\right|
> $$  
> 형태로 표현된다.
> 
> **forward의 의미**  
> forward는 **정방향 변환 Jacobian**을 사용했다는 뜻이다.  
> 즉, $z \mapsto x$ 로 가는 원래 함수 $u(z)$를 미분하여  
> $ \frac{\partial u(z)}{\partial z} $ 의 Jacobian determinant를 쓰고,  
> 그 역수를 취하는 방식이다.  
> 이는  
> $$
> p_X(x)
> =
> p_Z(z)\,
> \left|
> \det\!\left(\frac{\partial u(z)}{\partial z}\right)
> \right|^{-1}
> $$  
> 형태로 표현된다.
> 
> **두 방식의 관계**  
> 역함수 정리(inverse function theorem)에 의해  
> $$
> \det\!\left(\frac{\partial v(x)}{\partial x}\right)
> =
> \left[\det\!\left(\frac{\partial u(z)}{\partial z}\right)\right]^{-1}
> $$  
> 이므로, inverse 방식과 forward 방식은 완전히 동일한 결과를 준다.

---

## p9. Motivation: Normalizing Flows

- 관측 변수 $X$와 잠재 변수 $Z$ 위의  
  유향(directed) 잠재변수(latent-variable) 모델을 고려하자.

- **normalizing flow 모델**에서,  
  $f_\theta : \mathbb{R}^n \to \mathbb{R}^n$ 로 주어지는 $Z$와 $X$ 사이의 사상(mapping)은  
  결정적(deterministic)이며 가역적(invertible)이다.  
  따라서  
  $$
  X = f_\theta(Z), \quad Z = f_\theta^{-1}(X)
  $$

  <img src="/assets/img/lecture/probstat/13/image_5.png" alt="image" width="200px">

- 변수변환(change of variables)를 사용하면, 주변우도(marginal likelihood) $p(x)$는 다음과 같이 주어진다:

  $$
  p_X(x;\theta)
  =
  p_Z\!\left(f_\theta^{-1}(x)\right)
  \left|
  \det\!\left(
    \frac{\partial f_\theta^{-1}(x)}{\partial x}
  \right)
  \right|
  $$

- 참고: $x, z$는 연속적이어야 하며 동일한 차원을 가져야 한다.

---

## p10. Flow of Transformations

**Normalizing:**  
가역 변환을 적용한 후, 변수변환(change of variables)은 정규화된(normalized) 밀도를 제공한다.

**Flow:**  
가역 변환들은 서로 합성될 수 있다.

$$
z_m
=
f_\theta^{m}
\circ \cdots \circ
f_\theta^{1}(z_0)
=
f_\theta^{m}\!\bigl(f_\theta^{m-1}\!(\cdots(f_\theta^{1}(z_0))\bigr)
\;\triangleq\;
f_\theta(z_0)
$$

- $z_0$에 대해 단순한(simple) 분포로 시작한다 (예: 가우시안).
- 최종적으로 $x = z_M$ 을 얻기 위해, $M$개의 가역 변환을 순차적으로 적용한다.
- 변수변환 공식에 의해,

  $$
  p_X(x;\theta)
  =
  p_Z\!\left(f_\theta^{-1}(x)\right)
  \prod_{m=1}^{M}
  \left|
  \det
  \left(
  \frac{\partial (f_\theta^{m})^{-1}(z_m)}
      {\partial z_m}
  \right)
  \right|
  $$

  (참고: 행렬곱의 determinant 는 determinant 들의 곱과 동일하다)

---

## p11. Maximum Likelihood Estimation

- 데이터셋 $\mathcal{D}$ 위에서 maximum likelihood를 이용한 학습:

$$
\max_{\theta} \log p_X(\mathcal{D};\theta)
=
\sum_{x \in \mathcal{D}}
\log p_Z\!\left(f_\theta^{-1}(x)\right)
+
\log
\left|
\det\!\left(
\frac{\partial f_\theta^{-1}(x)}{\partial x}
\right)
\right|
$$

- inverse 변환 $x \mapsto z$ 와 변수변환 공식(change of variables)을 통한  
  **정확한 우도 계산(exact likelihood evaluation)**

- forward 변환 $z \mapsto x$ 를 통한 **샘플링(sampling)**

$$
z \sim p_Z(z),
\quad
x = f_\theta(z)
$$

- inverse 변환을 통해 잠재 표현(latent representations) 추론  
  (inference network가 필요 없음!)

$$
z = f_\theta^{-1}(x)
$$

---

## p12. Flow of Transformations

- 기본 분포(base distribution): Gaussian

  <img src="/assets/img/lecture/probstat/13/image_6.png" alt="image" width="800px">

- 기본 분포(base distribution): Uniform

  <img src="/assets/img/lecture/probstat/13/image_7.png" alt="image" width="800px">

- 10개의 planar(평면) 변환은 단순한 분포들을 더 복잡한 분포로 변환할 수 있다.

---

## p13. Flow of Transformations

- 효율적인 샘플링과 계산 가능한(tractable) 우도 평가(likelihood evaluation)를  
  가능하게 하는 단순한 prior $p_Z(z)$.  
  예: 등방성 가우시안(isotropic Gaussian)

- 계산 가능한 평가를 갖는 가역 변환들:
  - 우도 계산(likelihood evaluation)은  
    $x \mapsto z$ 매핑을 효율적으로 계산할 수 있어야 한다.
  - 샘플링은  
    $z \mapsto x$ 매핑을 효율적으로 계산할 수 있어야 한다.

- 우도를 계산하는 것은 또한  
  $n \times n$ Jacobian 행렬들의 행렬식(determinant) 계산을 필요로 한다.  
  여기서 $n$은 데이터의 차원이다.
  - $n \times n$ 행렬의 행렬식(determinant) 계산은 $O(n^3)$으로,  
    학습 루프 내에서 사용하기에는 지나치게 비싸다.

- **핵심 아이디어(Key idea):**  
  Jacobian 행렬이 특별한 구조를 갖도록 변환을 선택한다.  
  예를 들어, 삼각행렬(triangular matrix)의 행렬식(determinant)은  
  대각 원소들의 곱이며, 이는 $O(n)$ 연산이다.

---

## p14. Triangular Jacobian

$$
\mathbf{x} = (x_1, \ldots, x_n)
= f(\mathbf{z})
= (f_1(\mathbf{z}), \ldots, f_n(\mathbf{z}))
$$

$$
J
=
\frac{\partial f}{\partial \mathbf{z}}
=
\begin{pmatrix}
\frac{\partial f_1}{\partial z_1} & \cdots & \frac{\partial f_1}{\partial z_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial z_1} & \cdots & \frac{\partial f_n}{\partial z_n}
\end{pmatrix}
$$

$x_i = f_i(\mathbf{z})$가 $\mathbf{z}_{\le i}$에만 의존한다고 하자. 그러면

$$
J
=
\frac{\partial f}{\partial \mathbf{z}}
=
\begin{pmatrix}
\frac{\partial f_1}{\partial z_1} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial z_1} & \cdots & \frac{\partial f_n}{\partial z_n}
\end{pmatrix}
$$

는 하삼각(lower triangular) 구조를 갖는다.  
행렬식(determinant)은 **선형 시간(linear time)** 안에 계산될 수 있다.

마찬가지로, $x_i$가 $\mathbf{z}_{\ge i}$에만 의존하는 경우  
Jacobian은 상삼각(upper triangular)이다.

---

## p15. Normalizing Flows: Theoretical Side

**표기(conventions)**

- $\mu$와 $\nu$를 $\mathbb{R}^d$ 위의 확률측도(probability measures)라고 하고,  
  각각 르베그(Lebesgue) 밀도 $p$와 $q$를 갖는다고 하자.

- 우리는 $\mu$와 $p$를 **목표(target) 측도/밀도**라고 부르고,  
  $\nu$와 $q$를 **기준(reference) 측도/밀도**라고 부른다.

<img src="/assets/img/lecture/probstat/13/image_8.png" alt="image" width="480px">

그림: $\mu$는 왼쪽, $\nu$는 오른쪽

---

> **르베그 밀도(Lebesgue density)란?**  
>
> 르베그 밀도는 $\mathbb{R}^d$ 공간에서 정의된 확률측도가  
> **르베그 측도(일반적인 유클리드 공간의 길이·면적·부피 개념)** 에 대해  
> 절대연속일 때 존재하는 확률밀도 함수이다.
>
> 즉, 확률측도 $\mu$가 르베그 측도 $\lambda$에 대해 절대연속이면  
> 라돈-니코딤 정리에 의해 다음을 만족하는 함수 $p(x)$가 존재한다:
>
> $$
> d\mu(x) = p(x)\, d\lambda(x)
> $$
>
> 여기서의 $p(x)$가 바로 **르베그 밀도(Lebesgue density)** 이다.
>
> 직관적으로 말하면,  
> “일반적인 연속 공간에서의 확률밀도 함수(pdf)”가  
> 바로 르베그 밀도라고 이해할 수 있다.

---

## p16. Normalizing Flows: Theoretical Side

**normalizing flow**는  
$p$에서 $q$로 가는 사상(mapping) $T : \mathbb{R}^d \to \mathbb{R}^d$로서,  
다음 조건들을 만족한다:

(i) $T$는 거의 모든 곳에서(a.e.) 미분 가능하며,  
    거의 모든 $z$에 대해 $\det J_T(z) \neq 0$ 이다.

(ii) $$T_{\#}\mu = \nu$$, 혹은 밀도 관점에서 쓰면 다음과 같다:

$$
q(y)
=
\frac{p\!\left(T^{-1}(y)\right)}
     {\left|\det J_T\!\left(T^{-1}(y)\right)\right|}
\qquad \Longleftrightarrow \qquad
p(x)
=
q(T(x)) \,\left|\det J_T(x)\right|
$$

여기서 $J_T$는 $T$의 Jacobian이다.  
이 식을 다음과 같이 쓸 수도 있다:  
$$q = T_{\#}p$$.

식 (1)에 나타난 양의 로그(log)를 자주 고려하게 된다:

$$
\log p(x)
=
\log q \circ T(x)
+
\log\!\left|\det J_T(x)\right|
$$

---

## p17. Normalizing Flows: Theoretical Side

$T_1, T_2, \ldots, T_n$이 normalizing flow라고 하고,  
$T = T_n \circ T_{n-1} \circ \cdots \circ T_1$이 $q$에서 $p$로 가는 normalizing flow라고 하자.  

그러면,

$$
\log p(x)
=
\log(q \circ T(x)) + \log\left|\det J_T(x)\right|
$$

또는,

$$
\log p(x)
=
\log(q \circ T(x))
+
\sum_{j=1}^{n}
\log\left|\det J_{T_j}(z_{j-1})\right|
$$

여기서  
$z_0 = x$,  
$z_1 = T_1(z_0)$,  
$z_2 = T_2(z_1)$,  
$\ldots,$  
$z_{n-1} = T_{n-1}(z_{n-2})$ 이다.

---

## p18. Recap : KL divergence

KL-발산의 성질은 Jensen 부등식으로부터 얻어진다:

$$
\begin{aligned}
- D_{\mathrm{KL}}(\pi_1 \mid \pi_2)
&= - \mathbb{E}_{\pi_1}\!\left( \log \frac{\pi_1}{\pi_2} \right) \\[6pt]
&= \mathbb{E}_{\pi_1}\!\left( \log \frac{\pi_2}{\pi_1} \right) \\[6pt]
&\le
\log\!\left(
        \mathbb{E}_{\pi_1}\!\left( \frac{\pi_2}{\pi_1} \right)
     \right)
\qquad\qquad\qquad\text{since log is concave} \\[6pt]
&=
\log\!\left(
        \int_{\{\pi_1(x) > 0\}}
        \pi_1(x)\, \frac{\pi_2(x)}{\pi_1(x)}\, dx
     \right) \\[6pt]
&\le \log(1)
\qquad\qquad\qquad\quad\quad\quad\quad\quad\text{as log increasing} \\[6pt]
&= 0.
\end{aligned}
$$

---

> **왜 ‘since log is concave’가 등장하는가?**  
> Jensen 부등식에 따르면, 어떤 함수가 **concave(오목)** 하면  
> $$\mathbb{E}[f(X)] \le f(\mathbb{E}[X])$$  
> 이 성립한다.  
> 여기서 로그 함수 $\log(\cdot)$는 concave이므로  
> $$\mathbb{E}_{\pi_1}\!\left( \log \frac{\pi_2}{\pi_1} \right)
> \le
> \log\!\left(
> \mathbb{E}_{\pi_1}\!\left( \frac{\pi_2}{\pi_1} \right)
> \right)$$  
> 이 부등식이 바로 Jensen 부등식의 직접적인 적용이다.
> 
> ---
> 
> **왜 ‘as log increasing’이 등장하는가?**  
> 로그 함수 $\log(\cdot)$는 **증가함수(increasing function)** 이고,  
> $$ \int \pi_1(x)\, \frac{\pi_2(x)}{\pi_1(x)} dx = 1 $$  
> 이므로  
> $$ \log\!\left(\int \cdots \right) \le \log(1) $$  
> 이 성립한다.  
> 즉, 내부 값이 1 이하라면 $\log$를 취해도 그 관계가 유지되며,  
> 결국 최종적으로 $0$ 이하가 되고 KL 발산의 음수 형태가 0 이하임을 보여준다.

---

## p19. Maximum Likelihood Estimation

**최적화 기반 $T$ 학습(Learning $T$ via Optimization)**

$$
\min D_{\mathrm{KL}}(T_{\#}p \mid q)
\quad\text{s.t.}\quad
\det \nabla T > 0 \;\;\text{and}\;\; T \in \mathcal{F}.
$$

$$D_{\mathrm{KL}}(T_{\#}p \mid q)
= D_{\mathrm{KL}}(p \mid T_{\#}^{-1} q)$$ 이므로,  
$\{x_i\}_{i=1}^n$이 $\mu$로부터의 i.i.d. 표본일 때, 표본 평균 근사는 다음과 같다:

$$
\min
\; -\frac{1}{n}
\sum_{i=1}^{n}
\left[
\log\, q \circ T(x_i)
+
\log \left| \det \nabla T(x_i) \right|
\right]
\quad\text{s.t.}\quad
\det \nabla T > 0
\;\;\text{and}\;\;
T \in \mathcal{F}.
$$

---

> 우리는 변환 $T$ 를 학습하고 싶다.  
> 즉, **복잡한 데이터 분포($x$ 의 분포 = $p$)**를 **단순한 기준 분포($z$ 의 분포 = $q$)**로  
> 최대한 잘 밀어(push) 넣도록 하는 변환 $T$ 를 찾는다는 뜻이다.
> 
>> $$T_{\#}p$$ 는 **푸시포워드(pushforward) 분포**로,  
>> “$x$-공간의 분포 $p$를 변환 $T$에 통과시켜  
>> $z$-공간의 분포로 옮겼을 때 얻어지는 분포”를 의미한다.  
>> 즉,  
>> $x \sim p$ 이면  
>> $$z = T(x)$$ 의 분포가 바로 $$T_{\#}p$$ 이다.  
>> 그리고 $x \mapsto T(x) = z$ 이다.
> 
> ---
> 
> 제한식 $\det \nabla T > 0$ 은  
> $T$ 가 거의 어디서나 가역이며 Jacobian의 행렬식이 0이 되지 않도록 보장한다는 뜻이다.
> 
> ---
> 
> 조건 $T \in \mathcal{F}$ 는  
> $T$ 가 미리 정한 함수 클래스(예: 특정 신경망 구조) 안에 있어야 한다는 뜻이다.
> 
> ---
> 
> 항등식  
> 
> $$
> D_{\mathrm{KL}}(T_{\#}p \mid q)
> =
> D_{\mathrm{KL}}(p \mid T_{\#}^{-1} q)
> $$
> 
> 는 KL 발산이 변수변환 아래에서 형태를 유지한다는 뜻이다.
>
>> KL 발산은 확률밀도에 대한 적분 형태로 정의되는데,  
>> 변수변환(예: $x \mapsto T(x)$ 또는 $z \mapsto T^{-1}(z)$)을 적용해도  
>> Jacobian 항이 나타나면서 양쪽 식이 서로 정확히 보정된다.  
>>
>> 즉,  
>> 분포를 $T$ 로 밀어(push) 옮기느냐,  
>> 혹은 $T^{-1}$ 로 끌어오느냐의 표현만 달라질 뿐  
>> “두 분포 사이의 정보 차이” 자체는 동일하게 유지되기 때문에  
>> 위 항등식이 성립한다는 뜻이다.
> 
> ---
>
> 표본 $\{x_i\}_{i=1}^n$ 을 이용해 기대값을  
> 표본 평균으로 근사할 수 있다는 뜻이다.
> 
> ---
>
> 최종 최적화식  
> 
> $$
> \min
> -\frac{1}{n}
> \sum_{i=1}^{n}
> \left[
> \log(q(T(x_i)))
> +
> \log \left| \det \nabla T(x_i) \right|
> \right]
> $$
> 
> 는 normalizing flow에서 사용되는 **로그-우도의 최대화**와 동등하다는 뜻이다.

---

## p20. Maximum Likelihood Estimation

<img src="/assets/img/lecture/probstat/13/image_9.png" alt="image" width="800px">

---

> **(1) 첫 번째 줄**
> 
> $$
> D_{\mathrm{KL}}(T_{\#}p \,\|\, q)
> =
> \int_{\mathbb{R}^d} (T_{\#}p)(y)\,\log\frac{(T_{\#}p)(y)}{q(y)}\,dy
> $$
> 
> KL 발산의 정의를 그대로 쓴 것이다.
>
>
> **(2) 두 번째 줄 — pushforward 밀도 $$(T_{\#}p)(y)$$ 를 전개한 단계**
> 
> $$
> \int_{\mathbb{R}^d}
> p(T^{-1}(y))\,\det\nabla T^{-1}(y)\,
> \log
> \frac{
> p(T^{-1}(y))\,\det\nabla T^{-1}(y)
> }{
> q(y)
> }
> dy
> $$
> 
> >
> > 이유:
> > pushforward 밀도는 정의상
> > 
> > $$
> > (T_{\#}p)(y)
> > =
> > p(T^{-1}(y))\,
> > \left|\det\nabla T^{-1}(y)\right|
> > $$
> > 
> > KL 식 안의 모든 $$(T_{\#}p)(y)$$ 를 이 정의로 치환했기 때문에 이렇게 바뀐다.
>
>
> **(3) 세 번째 줄**
> 
> 아래 줄은 앞 단계의 적분을  
> **변수변환 $y = T(x)$** 로 바꾸고 난 뒤의 결과이다:
>
> $$
> \int_{\mathbb{R}^d}
> p(x)\,
> \log
> \frac{
> p(x)
> }{
> q(T(x))\,\det\nabla T(x)
> }
> dx
> $$
>
> ---
>
> > **(a) 먼저, 치환을 적용한다:  $y = T(x)$**
> >
> > 이제 적분 변수는 $y$ 대신 $x$ 가 된다.  
> > 이때 **항등적으로**  
> > $$
> > T^{-1}(y) = x
> > $$
> > 이므로,
> > 기존 적분에 나타났던  
> > $p(T^{-1}(y))$ 는 단순히  
> > $$
> > p(x)
> > $$  
> > 로 바뀐다.
>
> ---
>
> > **(b) 미분 형식 변화:  $dy = \mid \det\nabla T(x) \mid \,dx$**
> >
> > 적분에서 변수 치환을 하면  
> > $dy$ 를 $dx$ 로 바꿔야 하는데,  
> > 이때 “부피 변화율”이 Jacobian determinant 이다.
> >
> > 즉,
> > $$
> > dy = \left|\det\nabla T(x)\right|\,dx.
> > $$
> >
> > 따라서 이전 적분에 있던 $dy$ 자리는  
> > $$
> > \left|\det\nabla T(x)\right|\,dx
> > $$  
> > 로 바뀐다.
>
> ---
>
> > **(c) 역함수 Jacobian 관계를 적용한다**
> >
> > inverse function theorem 에 의해,
> >
> > $$
> > \det\nabla T^{-1}(y)
> > =
> > \frac{1}{\det\nabla T(x)}.
> > $$
> >
> > 이것이 아주 중요하다.  
> > 왜냐하면 이전 단계의 integrand 에는 $p(T^{-1}(y))\det\nabla T^{-1}(y)$  
> > 그리고 measure $dy$ 안에는 $|\det\nabla T(x)|$  
> > 라는 두 개의 Jacobian 이 **곱 형태**로 등장하기 때문이다.
>
> ---
>
> > **(d) Jacobian 항들이 정확히 상쇄되는 과정**
> >
> > 치환 전 integrand 의 Jacobian 파트:
> > 
> > $$
> > \det\nabla T^{-1}(y)
> > $$
> >
> > 치환으로 생긴 measure 변화:
> > $$
> > dy = (\det\nabla T(x))\,dx
> > $$
> >
> > 둘을 곱하면:
> > 
> > $$
> > \det\nabla T^{-1}(y)\; \det\nabla T(x)
> > =
> > \frac{1}{\det\nabla T(x)}\; \det\nabla T(x)
> > =
> > 1.
> > $$
> >
> > 즉, **완전히 사라진다.**
>
> ---
>
> > **(e) log 안에서 일어나는 변화**
> >
> > 기존 식에서는  
> > 
> > $$
> > \log\frac{
> > p(T^{-1}(y))\,\det\nabla T^{-1}(y)
> > }{
> > q(y)
> > }
> > $$
> >
> > 이제 각각을 $x$ 기준으로 바꾸면:
> >
> > - $p(T^{-1}(y)) \rightarrow p(x)$  
> > - $q(y) \rightarrow q(T(x))$  
> > - $\det\nabla T^{-1}(y) \rightarrow 1/\det\nabla T(x)$
> >
> > 따라서 log 내부는:
> > 
> > $$
> > \log\frac{
> > p(x)
> > }{
> > q(T(x))\,\det\nabla T(x)
> > }.
> > $$
>
> ---
>
> > **(f) 최종 정리**
> >
> > 적분 변수는 전부 $x$ 기준,
> > Jacobian 은 log 안과 measure 에서 정확히 정리되고,
> > 전체 결과는 다음과 같이 깔끔해진다:
> >
> > $$
> > \int_{\mathbb{R}^d}
> > p(x)\,
> > \log
> > \frac{
> > p(x)
> > }{
> > q(T(x))\,\det\nabla T(x)
> > }
> > dx
> > $$
>
> **(4) 네 번째 줄 — 로그 분리 및 적분 분리**
> >
> 로그 성질 적용:
> 
> $$
> \log \frac{p(x)}{q(T(x))\,\det\nabla T(x)}
> =
> \log p(x)
> -
> \log q(T(x))
> -
> \log \det\nabla T(x)
> $$
> 
> 적분의 선형성 때문에 세 항을 분리 가능:
>  
> $$=
> \int_{\mathbb{R}^d} p(x)\log p(x)\,dx
> -
> \int_{\mathbb{R}^d} p(x)\log q(T(x))\,dx
> -
> \int_{\mathbb{R}^d} p(x)\log\det\nabla T(x)\,dx
> $$

---

## p21. Maximum Likelihood Estimation

<img src="/assets/img/lecture/probstat/13/image_10.png" alt="image" width="800px">

---

## p22. Normalizing Flows의 예시

- 비선형성 함수 $h : \mathbb{R} \to \mathbb{R}$

- Jacobian 계산에는 “행렬 행렬식 보조정리(matrix determinant lemma)”가 필요하다:

  $$
  \det(A + u v^{T})
  =
  \left( 1 + v^{T} A^{-1} u \right)\det(A)
  $$

- 그림: $h$의 예시들

<img src="/assets/img/lecture/probstat/13/image_11.png" alt="image" width="600px">

---

> **행렬 행렬식 보조정리(matrix determinant lemma)**란?
>
> 이 보조정리는  
> 행렬 $A$ 에 **랭크 1(rank-1) 행렬** $u v^{T}$ 를 더한 행렬  
> $A + u v^{T}$ 의 행렬식을  
>
> $$
> \det(A + u v^{T})
> =
> \left( 1 + v^{T} A^{-1} u \right)\det(A)
> $$
>
> 형태로 매우 간단하게 계산할 수 있게 해주는 정리이다.
>
> 즉,  
> **큰 행렬의 행렬식을 직접 계산하지 않고**,  
> **$A^{-1}$과 $u, v$의 내적만으로**  
> 새로운 행렬의 행렬식을 얻을 수 있게 해주는 도구이다.
>
> 이 성질은 normalizing flow에서 Jacobian 행렬이  
> “기본 행렬 + rank-1 보정” 형태로 나올 때  
> 연산을 크게 줄여주는 핵심 트릭이다.
>
> ---
>
> **그림의 의미 (Examples of $h$)**  
>
> 그림은 normalizing flow에서 사용되는  
> **비선형성 함수 $h$** 의 여러 예시를 보여준다.
>
> - 파란색: 완만한 S-curve 형태의 비선형 함수  
> - 빨간색: 단순 선형 함수  
> - 초록색: sigmoid-like 형태의 비선형 함수  
>
> 이러한 $h$들은 flow 모델에서  
> **복잡한 분포를 만들기 위한 비선형 변형의 기본 요소**로 사용된다.
>
> 즉,  
> *간단한 분포(z)를 복잡한 분포(x)로 만들기 위해  
> 어떤 모양의 비선형 함수를 사용할 수 있는지 시각적으로 보여주는 그림*이다.

---

## p23. Planar Flows (Single Layer SVM)

- $u, w \in \mathbb{R}^d$, $b \in \mathbb{R}$, 그리고 비선형성 $h : \mathbb{R} \to \mathbb{R}$가 주어졌을 때,  
  planar flow $T_{pf} : \mathbb{R}^d \to \mathbb{R}^d$ 를 다음과 같이 정의한다:

  $$
  T_{pf}(z)
  =
  z + u\, h(w^{T} z + b).
  $$

- 우리는 다음을 계산한다:

  $$
  J_{T_{pf}}(z)
  =
  I_d
  +
  (u w^{T})\, h'(w^{T} z + b)
  \;\;\Longrightarrow\;\;
  \det J_{T_{pf}}(z)
  =
  1 + w^{T} u\, h'(w^{T} z + b).
  $$

- 그림: Planar flow에서의 변형 예시

<img src="/assets/img/lecture/probstat/13/image_12.png" alt="image" width="360px">

---

> (1) 변환 정의식  
> 
>   $$T_{pf}(z) = z + u\,h(w^{T}z + b)$$  
> 
> - 기본 입력 $z$ 에 작은 변화량을 더해 주는 구조이다.  
> - 이 변화량은 **스칼라 값** $h(w^{T}z + b)$ 를 계산한 뒤,  
>   이를 **벡터** $u$ 방향으로 늘려주어 적용하는 형태이다.  
> - 즉, “$z$ 를 $u$ 방향으로 밀어주는(push) 비선형 변형”이라고 볼 수 있다.  
> - $w^{T}z + b$ 는 하이퍼플레인(평면)을 정의하며, $h(\cdot)$ 는 그 평면을 기준으로  
>   어느 정도 변형을 줄지 결정한다.
>
> ---
>
> (2) Jacobian 계산식  
> 
>   $$
>   J_{T_{pf}}(z)
>   =
>   I_d + (u w^{T})\, h'(w^{T}z + b)
>   $$
> 
> - $T_{pf}$ 는 항등변환(identity) $I_d$ 에  
>   랭크-1(rank-1) 행렬 $u w^{T}$ 를 더한 구조이다.  
> - $h'(w^{T}z + b)$ 는 스칼라이므로, 전체로는 “랭크-1 업데이트(Jacobian)”가 된다.  
> - 이 구조 덕분에 행렬식(det)을 매우 효율적으로 계산할 수 있다.
>
> ---
>
> (3) 행렬식(det) 계산식  
> 
>   $$
>   \det J_{T_{pf}}(z)
>   =
>   1 + w^{T}u\, h'(w^{T} z + b)
>   $$
> 
> - 이는 **matrix determinant lemma**  
>   $\det(A + uv^{T}) = (1 + v^{T} A^{-1} u)\det(A)$  
>   를 그대로 적용한 결과이다.  
> - 여기서는 $A = I_d$ 이므로 $\det(I_d)=1$, $A^{-1} = I_d$ 가 되어  
>   계산이 매우 단순해진다.  
> - 따라서 planar flow는 “가벼운 비용으로” Jacobian determinant 를  
>   계산할 수 있게 설계된 구조이다.
>
> ---
>
> (4) 그림의 의미  
>
> - 그림의 파란 직선은 벡터 $w$ 가 정의하는 **하이퍼플레인(직선)** 을 나타낸다.  
> - 이 직선에 수직 방향이 $w$ 의 방향이며,  
>   planar flow는 이 평면 주변에서만 변형이 세게 일어난다.  
> - 점선으로 표시된 두 개의 선은 $w^{T} z + b = \text{constant}$ 형태의 레벨셋(level set)이며,  
>   flow가 해당 영역 근처에서 공간을 **뒤틀고(push)** 있다는 것을 보여준다.  
> - 화살표들은 실제로 입력 공간이 planar flow에 의해  
>   “어떤 방향으로 변형되는지” 시각적으로 표현한 것이다.  
> - 요약하면,  
>   **planar flow는 어떤 하나의 평면에 기반하여 데이터를 u 방향으로 비선형적으로 밀어서  
>   분포를 더 복잡하게 만드는 변환**이다.
