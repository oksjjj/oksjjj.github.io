---
layout: post
title: "[확률과 통계] 13주차"
date: 2025-11-25 14:00:00 +0900
categories:
  - "대학원 수업"
  - "확률과 통계"
tags: []
---

> 출처: 확률과 통계 – 박성우 교수님, 고려대학교 (2025)

## p2. Motivation  

<img src="/assets/img/lecture/probstat/13/image_1.png" alt="image" width="720px">

> **강의 내용**  
> 
> 오늘은 저번주에 이어서 GAN 모델을 공부했었는데  
> 바로 이제 Normalizing Flow라는 새로운 종류의 생성 모델을 공부하도록 하겠습니다.  
>
> GAN 같은 경우는 저희가 generator와 discriminator가 있어서  
> 서로 속이고 속이는 게임을 한다.  
> 그래서 zero-sum, 게임이론에 의거해서 훈련을 했었고,  
> 그거에 따라 파티클들이 점점점 샘플 퀄리티와 가깝게 되면서 진화를 하면서  
> 최종적으로 원하는 데이터 디스트리뷰션에 근접하는 결과를 낸다.  
> 이런 것들을 공부를 했었는데  
> 오늘은 좀 되게 재밌는 관점으로 갈 것 같아요.  
>
> 그래서 generator도 그렇고 VAE도 그렇고  
> 어떤 데이터를 뉴럴 네트워크으로 인풋으로 받고  
> 아웃풋으로 한 번에 내는 그런 모델들을 공부를 했었잖아요.  
>
> Normalizing Flow는 조금 다릅니다.  
> 그래서 데이터를 처리하기 위해서  
> 예를 들어서 우리가 GAN 같은 경우는 레이턴트 코드, 레이턴트 가우시안 디스트리뷰션을  
> generator에다 인풋을 넣으면 아웃풋이 그냥 데이터 생성되는 샘플들이 나왔었는데  
> 그게 아니라 Normalizing Flow는 그 단계를 여러 단계로 해서  
> 그걸 이제 수학이나 확률통계에서는 variational하게 한다.  
> 이런 얘기를 하는데  
> 여러 스텝을 통해 가지고 가우시안 디스트리뷰션으로부터  
> 뉴럴 네트워크 여러 번 피드포워드를 하면서 점진적으로 샘플이 구성되게 하는  
> 그래서 앞에서 GAN에서는 그게 트레이닝이었죠.  
>
> 네트워크에 들어가고 나오는 건 한 번씩 하되  
> discriminator와 generator가 번갈아가면서 학습하는 게 여러 번이었습니다.  
>
> 근데 이거 같은 경우는 그런 관점은 아니고  
> 이제 샘플을 가우시안으로부터 $T^{-1}$이라는 걸 통해서  
> 점점점 샘플에 가깝게 순차적으로 절차적으로 생성을 하는 게 목적인  
> 그런 모델이라고 생각하면 될 것 같아요.  
>
> 그래서 Normalizing Flow는 크게 큰 공리가 있습니다.  
> 데이터 디스트리뷰션이 있다고 가정을 했을 때  
> 거기서 이제 가우시안 디스트리뷰션으로 가는 어떤 맵핑 $T$라는 게 존재한다.  
>
> 그리고 $T$는 역함수가 있어야 됩니다.  
> 여러분들 이제 역함수에 대한 개념은 아실 거라고 생각을 해요.  
>
> 근데 역함수가 사실 되게 축복받은,  
> 정말 수학에서는 특정한 어떤 function만 그런 역함수가 존재하는 형태를 띄거든요.  
>
> 예를 들어서 일반 뉴럴 네트워크가 역함수가 존재하냐?라고 하면 존재하지 않습니다.  
> 일반적으로는.  
>
> 그래서 우리가 뉴럴 네트워크를 통해서  
> 결국 이 $T$를 혹은 $T^{-1}$를 모델링을 해야 되는데  
> 이제 뉴럴 네트워크는 방금 말씀드렸던 것처럼 일반적으로 역함수가 존재를 하지 않다 보니까  
> 그럼 그걸 어떻게 어떤 수학적인 처리를 해가지고 만들어 나갈 것이냐.  
>
> 이게 이제 Normalizing Flow의 정말 핵심 포인트라고 생각하시면 될 것 같습니다.  
>
> 그래서 앞에서 우리가 중간고사 시험 보기 전에 배웠었던 내용부터  
> 시작을 해서 한번 얘기를 해보죠.  

---

## p3. Motivation  

<img src="/assets/img/lecture/probstat/13/image_2.png" alt="image" width="800px">

> **강의 내용**  
> 
> 그래서 이제 제가 VAE를 배웠었고, GAN은 이제 VAE 같은 파이프라인으로 표현할 수 있었고  
> 그럼 이제 Flow-based 모델은 아래와 같이 표현한다는 겁니다.  
>
> 이제 $T$라는 function이 있고, $T^{-1}$라고 있는데  
> 그걸 이제 $F$라고 그냥 할게요.  
> 뭐 notation은 계속 변해도 되니까.  
>
> 그래서 $x$라는 게 존재해서  
> Flow라는 펑션 $F$라는 걸 통해가지고, $z$까지 이제 맵핑을 하고  
> 이건 이제 사실 그림에서는, 한 번에 나오는 걸로 되어 있는데  
> 이건 그냥 개략적으로 표현한 거고, 순차적으로 $x$에서 $z$로 만들고요.  
>
> 그리고 이렇게 순차적으로 만들었는데  
> 인버스 function이 이미지를 그대로 복원해야 되잖아요.  
> 그래서 인버스 function을 통해서, 또 순차적으로 역연산을 통해가지고 원래 $x'$을 만들고  
> 이때 우리가 원하는 건 $x$랑 $x'$이 최대한 닮은 거였죠.  
>
> 그래서 Normalizing Flow는, 뒤에 디퓨전 모델에서의 어떤 그런 테크닉들  
> 그리고 우리가 어떤 새로운 철학의 초기 시점이라고 생각하시면 될 것 같아요.  
> 이렇게 순차적 생성이라는 입장에서.  
>
> 그래서 GAN이랑 VAE가 이렇게 고려가 됐고  
> 이제 Likelihood Estimation도 되게 비슷한데  
> 결국 우리가 또 KL-divergence Minimization을 합니다.  
> 모든 생성 모델에 있다시피.  
>
> 그런 걸 이제 다르게 좀 다양한 모습으로 보여지는 모습을 볼 수 있을 것 같고.  

---

## p4. Change of Variables

- **변수 변경 (1차원 경우):**  
  만약 $X = f(Z)$ 이고 $f(\cdot)$ 가 단조이며 역함수 $Z = f^{-1}(X) = h(X)$ 가 존재하면:

  $$
  p_X(x) = p_Z(h(x))\, \mid h'(x) \mid
  $$

- 이전 예시:  
  $X = f(Z) = 4Z$, $Z \sim \mathcal{U}[0,2]$ 일 때 $p_X(4)$ 는 무엇인가?

  - $h(X) = X/4$
  - $p_X(4) = p_Z(1)\, h'(4) = \frac{1}{2} \times \mid 1/4 \mid = \frac{1}{8}$

- 더 흥미로운 예시:  
  $X = f(Z) = \exp(Z)$, $Z \sim \mathcal{U}[0,2]$ 일 때 $p_X(x)$ 는 무엇인가?

  - $h(X) = \ln(X)$
  - $p_X(x) = p_Z(\ln(x))\, \mid h'(x) \mid = \frac{1}{2x}$ for $x \in [\exp(0), \exp(2)]$

- $p_X(x)$ 의 “모양(shape)” 은 prior $p_Z(z)$ 보다 더 복잡할 수 있음에 유의하자.

> **강의 내용**  
> 
> 그 본격적인 내용에 들어가기 앞서  
> 이제 우리가 되게 중요한 수학적 개념을 알아야 됩니다.  
> Change of variables라는 개념을 알아야 되는데요.  
> 그래서 이거에 대해서 한번 설명을 한번 드려보겠습니다.  
>
> 이게 되게 어떻게 보면 예전에 학부 때 배우셨기도 했고  
> 중간고사 이전에도 이거 되게 간략하게 얘기를 했고 했는데  
> 어떻게 보면 확률 변수를 바꾸는 그런 툴 중에  
> 굉장히 중요하고 굉장히 제너럴한  
> 굉장히 잘 알려져 있는 그런 결과라고 보시면 될 것 같아요.  
>
> 그래서 $F$라는 function이 존재하고  
> 결국에 이게 이제 우리가 최종적으로 봤을 때는  
> 우리가 원하는 neural network로 이걸 표현하고 싶다는 거죠.  
>
> $Z$라는 random variable이 있을 때  
> 저희가 $Z$는 뭐였었죠? 거의 가우시안이었죠.  
> latent information이었죠.  
>
> 그랬을 때 $f(Z)$를 $X$라고 우리가 표현을 했을 때  
> $f$의 어떤 특이한, 이건 새로 나오는 어떤 개념인데  
> Monotone이라는 개념이 여기 나옵니다.  
>
> Monotone이라고 $f$를 가정을 했을 때  
> $f$의 inverse, 그러니까 $X$라는 거를 역으로 이제 inverse를 해서  
> imaging을 했을 때 나오는 걸 $Z$라고 하고  
> 이 inverse function을 $h$라고 우리가 표현을 해봅시다.  
>
> 그러면 여기서 이제 다음과 같은 수식이 만족하는데요.  
> $P_X$는 뭐였었죠? $X$에 대한 PDF function이었습니다.  
> 그것은 뭐 probability distribution,  
> probability density function이었죠.  
>
> 그런데 $Z$에 대한 probability distribution,  
> PDF의 안에다가 $h(X)$를 넣었고  
> 그 뒤에 이제 absolute value라고 되어 있는 $h$의 inverse를 곱했을 때  
> 이게 만족을 한다는 게  
> change of variable이라는 theorem의 핵심 메시지입니다.  
>
> 이게 무슨 말이냐면  
> 우리가 이 $X$와 $Z$는 random variable이잖아요.  
> 그렇죠?  
> 그런데 random variable에 대해서 항상 coupling으로 나오는 게  
> probability density function이거든요.  
>
> 그래서 $X$라는 random variable이 있으면 $P(X)$가 존재했을 거예요.  
> $Z$라는 random variable이 있어도 $P(Z)$라는 게 있겠죠.  
> 여기는 보통 Gaussian distribution을 가정을 해야 될 테고.  
>
> 그랬을 때 이제 okay  
> random variable 입장에서는 저렇게 이제 왔다 갔다 하는 룰은 알겠는데  
> 그럼 PDF 입장에서는  
> one-to-one correspondence가 있어야 되니까  
> $X$라는 PDF에서, $Z$라는 PDF로  
> 어떻게 바뀌어야 되는지에 대한 룰을 묘사를 해야 됩니다.  
>
> 그래서 이 change of variable은 이런 것들을 이제 목적을 두고 하는 거다.  
> 이렇게 생각해 주시면 될 것 같고  
> 이 직관은 좀 잘 이해하셔야 돼요. 어려운 내용은 아니다 보니까.  
>
> 그래서 $P(Z)$에다가  
> 원래 괄호 안에 무언가가 들어가야 되는데, 거기에 $h(Z)$가 대신 들어가 있죠.  
> 그리고 뒤에 또 이상한 게 뭐냐면  
> $h$를 또 미분을 한 다음에 absolute value를 씌우게 돼요.  
>
> 이것도 되게 계산을 하다 보면 자명하게 나오는 term인데  
> 그냥 뭐 이런 게 있다 라고 생각하시면 됩니다.  
>
> 그래서 absolute value를 취한 다음에 곱한 거예요.  
> $P(Z)$는 output이 scalar고  
> $h$는 원래 1D, 그러니까 univariate한 function이기 때문에  
> $h$를 미분하면 우리가 고등학교 때 보아 왔던 미분이고  
> 그 absolute value를 취해도 똑같은 scalar dimension입니다.  
>
> 그래서 여기 수식에 나온 이 모든 애들은  
> 다 scalar dimension으로 표현된 그런 값이라고 생각하시면 될 것 같아요.  
>
> 그래서 change of variable은  
> 이 예제에서 봤듯이, 되게 재밌는, random variable에 따라서,  
> PDF를 바꿔주는 그런 theorem입니다.  
>
> 그리고 중요한  
> 제가 방금 전에 말씀드렸지만 중요한 하나가 있어요.  
> Keyword가 모노톤이라는 게 있거든요.  
>
> 모노톤이라는 게  되게 재밌는 그런 좀 성질인데  
> 칠판 처음 쓰는 것 같은데  
> 설명을 드리기 위해서 한번 써보도록 하겠습니다.  
>
> 모노톤이라는 게  
> 모노톤 increasing하다, 그리고 모노톤 decreasing하다.  
> 이런 두 개의 property로 쪼개서 표현할 수 있는데  
> 뭐 increasing이던 decreasing이던 이런 느낌의 어떤 함수  
> 제가 그릴 함수의 느낌이라고 한다면 다 모노톤이라고 합니다.  
>
> 모노톤이 뭐냐면  
> 한순간도 모노톤 increasing이라고 한다면  
> 각각의 어떤 evaluation 했을 때  
> 이 점들이 무조건 점점점점 커져야 된다는  
> 그런 가정을 생각하시면 됩니다.  
>
> 쉽게 말해서요.  
> 수학적으로 포멀하게 정의할 수 있지만 직관만 전달 드리겠습니다.  
>
> 모든 점들이 조금 조금씩 봤을 때  
> 점점점 커지거나 아니면 같아도 됩니다.  
> 이 높이가.  
>
> 이런 걸 모노톤 increasing하다고 얘기를 하고  
> 거꾸로 모노톤 decreasing하면 완전 여기에 역의 관계예요.  
>
> $x$축에 갈 때 시간에 따라 가지고  
> 이제 $x$축의 어떤 크기에 따라 가지고 점점점 decreasing하는 거.  
>
> 이런 것들을 모노톤 decreasing function이라고 합니다.  
>
> 이 두 개 중에 아무거나 상관이 없습니다.  
> 왜냐면 $F$에다가 inverse $F$를 붙여도 사실 다 똑같은 거기 때문에  
> 그냥 rule이 조금 바뀌는 거였지.  
>
> 그래서 각각의 strict한 ordering이 있다.  
> 그래서 이런 것들을 지켜주는 함수들이 모노톤이겠죠.  
>
> 그러면 예를 들어서  
> $x^2 = y$라는 함수가 있을 때 그게 모노톤일까요?  
>
> 뭐 이런 함수가 있다고 하면  
> 네 모노톤이 아니겠죠.  
>
> 한쪽에서는 decreasing하다 한쪽에서는 올라가다 보니까.  
>
> 그 말은 뭐냐면  
> 지금 change of variable이  
> normalizing flow의 중요한 프로퍼티니까 우리가 사용하는 것일 텐데  
> 그러니까 저는 $x^2$ 형태  
> 그런 걸 이제 convex하다, 뭐 이런 느낌, 이렇게 표현할 수도 있는데  
>
> convex하고 hyperbolic하고 이런 애들은  
> 우리가 뉴럴 네트워크의 $F$라는 이 함수로 사용을 해야 되기 때문에  
> 타겟을 두지 않습니다.  
>
> 그래서 뉴럴 네트워크가 이런 모노톤의 이런 프라퍼티를 잘 따라야겠죠.  
>
> 왜 라고 생각을 하냐면, 쉽습니다.  
>
> 우리는 앞에서  
> $T$라는 어떤 펑션이 있고, $T^{-1}$라는 펑션을 구해야 되는 게  
> normalizing flow의 가장 큰 철학 중의 하나라고 가정을 했는데  
>
> 우리가 수학에서 잘 알려져 있는 바 중에 하나가 뭐냐면  
> 모노톤 펑션은 인버스 펑션을 갖습니다.  
>
> 왜냐하면 one to one correspondence가 있으니까.  
>
> 예를 들어서 얘 같은 경우는 인버스가 없죠.  
> 왜냐하면 하나에 두 개가 맵핑이 될 수 있기 때문에  
> 이건 well defined한 펑션이 아니에요.  
>
> 그런데 이렇게  
> 모노톤 increasing 하거나 decreasing 하다는  
> 모든 것에 one to one correspondence가 다 존재하는 거니까  
>
> 그렇게 우리가 인버스를 시킬 수 있으니까  
> 그런 애들은 인버스 펑션이 존재하고  
> 그래서 normalizing flow에서 사용할 수 있다는 거예요.  
>
> 이런 직관을 좀 이해해 주시면 될 것 같습니다.  
>
> 중간에 example을 한번 보시죠.  
>
> 그래서 $Z$라는 어떤 레이턴트 인포메이션이 존재할 때  
> 여기서 $\mathcal{U}$라고 하는 건 유니폼 디스트리뷰션인 거예요.  
>
> 우리 유니폼 디스트리뷰션 배웠었죠.  
> 0이랑 2 사이에서  
> 우리가 real value를 따지고 있으니까  
> 0이랑 2 사이에서  
> 유니폼하게 균등하게 분포되어 있는 것을 가정했을 때  
>
> 4 곱하기 $Z$ 하면 그냥 0에서 8 사이로 바뀌겠죠.  
> 4를 곱했으니까.  
>
> 그래서 그런 거라고 가정하면  
> $P_X(4)$가 뭐냐.  
>
> 그러니까 우리가 바꾼 pdf에 대해서 4라는 값을 넣으면  
> 어떤 값이 나오냐 하면  
> 간단하게 계산하면 밑에와 같이 표현이 됩니다.  
>
> 이건 되게 쉬운 예제인 것 같고.  
>
> 그래서 밑에 마찬가지로 exponential에 대해서도 똑같이 계산을 할 수 있고요.  
>
> 이거는 좀 직관적으로 이해해 주시면 될 것 같고.  

---

> **수식 도출 과정**
>
> **1단계. 확률은 동일 — 변수만 다르다**
>
> 변수변환의 출발점은 매우 단순하다.
>
> > $X$가 어떤 작은 구간에 있을 확률 = $Z$가 그 구간으로 변환된 구간에 있을 확률
>
> 즉,
>
> $$
> P(X \in [x, x+dx]) = P(Z \in [z, z+dz])
> $$
>
> 여기서  
>
> $$z = h(x)$$
>
> ---
>
> **2단계. 양쪽을 확률밀도로 표현**
>
> 왼쪽(X 쪽):
>
> $$
> P(X \in [x, x+dx]) = p_X(x)\, dx
> $$
>
> 오른쪽(Z 쪽):
>
> $$
> P(Z \in [z, z+dz]) = p_Z(z)\, dz
> $$
>
> 두 확률이 같으므로:
>
> $$
> p_X(x)\, dx = p_Z(z)\, dz
> $$
>
> ---
>
> **3단계. $Z = h(X)$ 를 미분하여 $dz$와 $dx$ 관계 구하기**
>
> $$
> z = h(x)
> $$
>
> 양변 미분하면:
>
> $$
> dz = h'(x)\, dx
> $$
>
> ---
>
> **4단계. $dz = h'(x)\, dx$ 를 확률식에 대입**
>
> 확률식:
>
> $$
> p_X(x)\, dx = p_Z(z)\, dz
> $$
>
> 여기에 $z = h(x)$, $dz = h'(x)\, dx$를 대입하면:
>
> $$
> p_X(x)\, dx = p_Z(h(x))\, h'(x)\, dx
> $$
>
> 양변에서 $dx$ 제거:
>
> $$
> p_X(x) = p_Z(h(x))\, h'(x)
> $$
>
> ---
>
> **5단계. 왜 절댓값이 필요한가?**
>
> 단조 감소일 때:
>
> $$
> h'(x) < 0
> $$
>
> 그러면
>
> $$
> p_Z(h(x))\, h'(x)
> $$
>
> 이 음수가 되어버린다 → 확률밀도는 음수가 될 수 없음.
>
> 또한 변수 변환에서 필요한 것은
>
> > 방향이 아니라 “길이의 변화량(스케일)”
>
> 이며, 이는 항상 양수여야 한다.
>
> 따라서 절댓값을 붙여주면:
>
> $$
> p_X(x) = p_Z(h(x))\, |h'(x)|
> $$

---

## p5. Change of Variables

- $Z$를 $[0,1]^n$ 구간에의 균등(uniform) 확률벡터라고 하자.
- $X = A Z$ 이고, $A$는 역행렬 $W = A^{-1}$을 갖는 정방행렬이라고 하자.  
  이때 $X$는 어떻게 분포하는가?
- 기하학적으로, 행렬 $A$는 단위 하이퍼큐브 $[0,1]^n$을 하나의 parallelotope(평행다포체)로 사상한다.
- 하이퍼큐브와 parallelotope는 정사각형/정육면체와 평행사변형/평행육면체를 고차원으로 일반화한 개념이다.

<img src="/assets/img/lecture/probstat/13/image_3.png" alt="image" width="600px">

그림: 행렬
$$A =
\begin{pmatrix}
a & c \\
b & d
\end{pmatrix}
$$ 는 단위 정사각형을 평행사변형으로 사상한다.

> **강의 내용**  
> 
> 우리가 1-dimensional을 그냥 좀 직관적으로 이해하기 위해서  
> 사용했었던 그런 케이스인 거고.  
> 우리는 사실 n-dimensional로 확장을 해야 됩니다.  
> 이미지던 우리가 다루는 모든 데이터가  
> 굉장히 하이디멘저널한 스페이스에 존재한다고 가정하기 때문에.  
>
> 그래서 이런 걸 한번 생각해보죠.  
> Z라는 게 $[0,1]^n$에 되어 있는 게 되게 이상한 모양으로 보여질 수 있는데.  
> 이것을 유닛 큐브라고 합니다.  
>
> 그래서 $n=1$이면 그냥 이렇게 실선에서 하나의 어떤 한 변을 갖는 거고, $[0,1]$이 되고.  
> $n=2$이면 이렇게 사각형 $[0,1]\times[0,1]$ 이 안을 표현하는 거고요.  
> $n=3$이면 이게 또 z축으로 늘어나서 3차원 큐브로 표현하는 거고.  
>
> 그래서 이런 것도 유닛 큐브라고 하는데  
> 유닛 큐브 안에 유니폼하게 놓여 있는 랜덤 배리어블 Z라는 게 있다고 가정했을 때.  
>
> $X = AZ$ 라고 하고 $A$는 invertible matrix라고 가정해 봅시다.  
> $W$는 $A^{-1}$라고 하고요.  
>
> 그럼 이제 X가 어떻게 distribute되느냐, 이거에 대해서 한번 알아보고 싶은 거예요.  
> 이것도 앞에서 change of variable을 진행하면 되는데  
> 여기서 이 페이지에서 직관을 주려고 하기 위해서 이런 걸 하는 겁니다.  
>
> 그래서 왼쪽 밑 초록색 박스로 되어 있는 게  
> 제가 아까 전에 그렸었던 $n=2$일 때의 유닛 큐브를 가정한 것이고.  
> $A$가 invertible matrix면  
> $AZ$라고 하면 오른쪽과 같이 초록색이었던 영역이 빨간색으로 distortion이 되는 겁니다.  
>
> 그 말은 뭐냐면, 이걸 좀 상상을 하시면 좋을 것 같은데  
> 이게 어디에 쓰이느냐,  
> 이 페이지가 왜 필요하느냐라고 얘기를 해야 할 것 같은데.  
>
> 이 초록색에서 빨간색으로 옮기는 $AZ$라는 것도  
> $f(Z)$라는 function으로 우리가 생각할 수 있는 거거든요.  
> 이것도 function이기 때문에.  
>
> 그리고 초록색에서 빨간색으로 한 번 옮겼어요.  
> 초록색은 우리가 레이턴트 베리어블이라고 상정을 하고 고려했던 거니까.  
>
> 그런데 빨간색에서 또 다른 펑션, 또 다른 매트릭스 $B$라는 걸 생각해서  
> 이 빨간색을 한 번 더 자르지 않고 부드럽게 트랜스포메이션해서  
> 새로운 박스를 만들 수 있겠죠.  
>
> 그리고 그걸 계속 반복하는 거죠.  
> 계속 반복을 하면 우리의 믿음은 뭐냐면,  
> 이렇게 반복을 했을 때 최종적으로 데이터 distribution이랑 같은 모양,  
> 데이터가 표현하는 이 큐브 영역, support 영역에 수렴되어야 될 것이다.  
>
> 라는 게 우리가 change of variable을 통해 normalizing flow에서 하고 싶은 바입니다.  
>
> 상상을 해보시면 될 것 같아요.  
> 여기서는 matrix로 곱해서 비주얼라이제이션을 편하게 했지만  
> 사실 이 $A$가 우리가 뉴럴 네트워크로 표현해야 되는 거잖아요.  
>
> 그럼 이 초록색 박스가 정말 non-linear하게 모양도 복잡하게 표현될 수 있도록 해주겠죠.  
> 뉴럴 네트워크 파라미터가 굉장히 많고 representation이 세다 보니까.  
>
> 그러면 그런 것들을 여러 번 체이닝을 통해서  
> 최종적으로 우리가 원하는 데이터 distribution으로 수렴시키겠다.  
>
> 이게 끝입니다.  
> 이게 normalizing flow를 다 이해하신 거예요.  
> 그걸 이해하면 됩니다.  
> 그 뒤에는 다 테크니컬한 겁니다.  
>
> 조금 더 이 페이지 안에서 수학적인 걸 얘기하면,  
> $A$가 invertible이라고 하면  
> 유닛 스퀘어, 정사각형을 사다리꼴, 평행사변형 이런 걸로  
> 맵핑해주는 것으로 생각할 수 있습니다.  
> (2D에서는.)  
>
> 이런 직관을 가지고 normalizing flow를 이해하면 좀 재밌어요.  
>
> 그러면 첫 번째 질문을 할 게 뭐냐.  
> 그러면 한 번에 맵핑하면 되는 것 아닌가?  
> 왜 normalizing flow에서는 function의 체이닝을 생각해야 하느냐?  
> 이것을 좀 생각해야 될 것 같고.  
>
> 그리고 두 번째는  
> 체이닝을 해야 한다고 했을 때 그럼 얼마나 해야 하느냐?  
> 뉴럴 네트워크의 복잡성에 따라 다르겠죠.  
>
> 그런 것들을 끊임없이 질문할 수 있습니다.  
>
> 일단 첫 번째로,  
> 우리가 normalizing flow에서 왜 뉴럴 네트워크를 체이닝해야 하느냐에 대해 답을 하자면,  
> 어쨌든 invertible 해야 되잖아요.  
>
> $A$는 invertible 하다고 했습니다.  
> 그런데 이 언어를 추상적으로 올려서 $A$라는 것을 뉴럴 네트워크화 시켜야 되는데  
> 이 안에서 우리는 매트릭스라는 것이, invertible 매트릭스가 있다,  
> 이런 것을 공부했고 알고 있기 때문에 아무 문제가 없지만  
>
> 이게 뉴럴 네트워크라고 했을 때  
> invertible 해야 되고  
> invertible 하다는 것을 enforcing시키는 것이 쉽지가 않았기 때문에  
> 모노톤이라는 것을 강조했는데  
>
> 모노톤 function 하나로는  
> 아무리 뉴럴 네트워크 파라미터가 크더라도  
> 뭔가의 모양을 맞추는 것에 잘 수렴시키지 못한다는 것이  
> 이론적으로 나와 있어요.  
>
> 최대치가 있습니다.  
> 그래서 컴퓨터학과 교수님들,  
> 연구자님들이 생각한 거죠.  
>
> 여러분들 알고리즘 중에 divide and conquer 들어보셨나요?  
> 로마군이 옛날에 점령할 때  
> 세력을 나누고 따로따로 점령하는 그런 개념인데  
> divide and conquer라는 개념이 수학에서도 통해요.  
>
> 한 번에 모노톤한 펑션을 모델링하는 게 아니라  
> 그것을 여러 개로 나눈 다음  
> 각각의 모노톤한 펑션을 체이닝해서 표현했을 때 훨씬 더 성능이 좋아진다.  
> 쉽게 말해서 이런 겁니다.  
>
> 그게 이론적으로 다 나와 있습니다.  
> 2015년부터 2018년까지 많이 연구를 했어요.  
>
> 그래서 그런 걸 기반으로  
> 이런 모노톤 펑션, invertible 함수에 대한 체이닝을 통해  
> 데이터를 만들어 나가는 게 우리의 목표다.  
>
> 그리고 그 수식은 여기 나오는 바,  
> $X$라는 distribution이 $Z$라는 distribution과 어떻게 연결되어 있는가,  
> 이것이 여러 개 있는 거죠.  
>
> function composition,  
> 함수 합성을 통해 여러 개가 있는 거죠.  

---

## p6. Change of Variables

- parallelotope의 부피(volume)는 행렬 $A$의 행렬식(determinant)의 절댓값과 동일하다.

  $$
  \det(A)
  =
  \det
  \begin{pmatrix}
  a & c \\
  b & d
  \end{pmatrix}
  =
  ad - bc
  $$

  <img src="/assets/img/lecture/probstat/13/image_4.png" alt="image" width="480px">

- $X = A Z$라고 하자. 여기서 $A$는 정방의 가역 행렬이며, 그 역행렬은 $W = A^{-1}$이다.  
  $X$는 면적이 $|\det(A)|$인 parallelotope 상에서 균일하게 분포한다.  
  따라서 다음을 얻는다:

  $$
  p_X(x) = p_Z(Wx)\,/\,|\det(A)|
  $$

  $$
  = p_Z(Wx)\,|\det(W)|
  $$

- 왜냐하면 $W = A^{-1}$이면  

  $$
  \det(W) = \frac{1}{\det(A)}
  $$

- 1차원 경우의 공식과의 유사성에 주목하라.

> **강의 내용**  
> 
> 그래서 앞에 예제를 보자면.  
> determinent 이런 얘기 있는데 크게 신경은 안 써도 될 것 같고.  
>
> 우리가 X는 AZ.  
> 그러니까 f라는 거를 f(X)는 AX.  
> 이렇게 우리가 만약에 가정한다고 했을 때  
> 이 케이스 같은 경우는 x라는 어떤 랜덤 베리어블에 대한 디스트리뷰션이  
> 이렇게 수식과 같이 표현이 돼가지고  
> 최종적으로 정보들을 바꿀 수 있다.  
> 이렇게 좀 이해를 해주시면 될 것 같습니다.  

---

> **1차원 공식과의 유사성 설명**
>
> 다변량 선형변환에서도 밀도변환 공식의 핵심 구조는  
> **“원래 밀도를 역변환한 지점에서 평가하고, 스케일 변화량의 절댓값으로 나눈다”**는 점에서  
> 1차원 변수변환 공식과 완전히 동일하다.
>
> 1차원에서는  
> $$
> p_X(x) = p_Z(h(x))\,|h'(x)|
> $$
> 로서 스케일 변화량이 도함수 $h'(x)$ 하나였다.
>
> 다변량에서는  
> $$
> p_X(x) = p_Z(Wx)\,|\det(W)|,
> $$
> 여기서 스케일 변화량이 **Jacobian의 절댓값 = $\det(W)$** 로 일반화되었을 뿐이다.
>
> 즉,  
> **1D: 길이(scale) 변화 → $|h'(x)|$**  
> **nD: 부피(volume) 변화 → $|\det(W)|$**
>
> 따라서 다변량 공식은 1차원 변수변환의 자연스러운 확장(extension)이다.

---

## p7. Change of Variables

- $A$를 통한 선형변환의 경우, 부피의 변화는 행렬 $A$의 행렬식(determinant)로 주어진다.

- 비선형 변환 $f(\cdot)$의 경우, 선형화된(linearized) 부피 변화는  
  $f(\cdot)$의 Jacobian의 행렬식(determinant)으로 주어진다.

- **변수변환(일반 경우)**:  
  $f : \mathbb{R}^n \to \mathbb{R}^n$ 이 $X = f(Z)$ 와 $Z = f^{-1}(X)$ 를 만족하도록 가역일 때,  
  $Z$와 $X$의 대응(mapping)은 다음과 같다:

  $$
  p_X(x)
  =
  p_Z\!\bigl(f^{-1}(x)\bigr)
  \left|
    \det\!\left(
      \frac{\partial f^{-1}(x)}{\partial x}
    \right)
  \right|
  $$

- **Note 0**:  
  이는 이전 1차원 경우  
  $p_X(x) = p_Z(h(x))\,|h'(x)|$  
  를 일반화한 것이다.

- **Note 1**:  
  VAE와 달리, $x, z$는 연속적이어야 하며 같은 차원을 가져야 한다.  
  예를 들어, $x \in \mathbb{R}^n$ 이면 $z \in \mathbb{R}^n$ 이다.

- **Note 2**:  
  어떤 가역행렬 $A$에 대해서도  
  $\det(A^{-1}) = \det(A)^{-1}$ 이다.

  따라서,

  $$
  p_X(x)
  =
  p_Z(z)
  \left|
    \det\!\left(
      \frac{\partial f(z)}{\partial z}
    \right)
  \right|^{-1}
  $$

> **강의 내용**  
> 
> 그럼 이제 좀 제대로 된 경우로 볼까요?  
> 그래서 앞에는 A라는 매트릭스를 통해서 리니어 트랜스폼을 했는데  
> 그게 아니라 논리니어 트랜스폼.  
> 뉴럴 네트워크와 비슷한 애들을 생각해 보자는 겁니다.  
>
> 그래서 이제 되게 비슷한 느낌으로  
> change of variable 제너럴 케이스라고 되어 있고.  
> 여기서는 이제 f가.  
> 이게 되게 중요한 부분인데.  
> 얘는 f라는 애는 차원을 바꾸면 안 돼요.  
> 그래서 n차원을 인풋으로 받았으면 n차원을 아웃풋으로 내뱉어야 됩니다.  
>
> 이 f를 이제 우리가 뉴럴 네트워크로 설계할 건데.  
> 뉴럴 네트워크 이제 디자인 specification의 한 팩터가 되겠죠.  
> 우리가 GAN이나 VAE에서는 뭐 그런 경우가 있었나요?  
> 인코더, 디코더.  
> 그러니까 인코더랑 디코더를 다 합쳐놨을 때는 똑같은 디멘젼이 나와야 되지만  
> 뭐 GAN도 그렇고 인코더나 혹은 디코더.  
> 각각의 뉴럴 네트워크 하나씩 떼보면 인풋 디멘션이랑 아웃풋 디멘션의 정보가 다르거든요.  
>
> 그래서 그 말은 뭐냐면 뉴럴 네트워크 스트럭처가 달라져야 된다는 거예요.  
> 여기서부터는.  
>
> 그래서 뒤에 이제 디퓨전 모델 할 때  
> 뭐 잠깐 다룰 예정이지만은 되게 유니버셜하게 많이 쓰는  
> 우리가 이제 그 나노바나나 이런 모델에도 들어가 있는 게 U-Net이라는 스트럭처예요.  
> 뉴럴 네트워크.  
>
> 그래서 그 U-Net의 어떤 배리언트들을 씁니다.  
> 굉장히 U-Net이라는 그 핵심 아이디어를 바꾸지 않은 채로  
> 그 많은 레이어의 컴포넌트들을 다 완전히 하이퍼 파라미터라이징하고  
> 그거를 옵티마이제이션 하면서 만들어 놓은 그 U-Net의 배리언트들을 씁니다.  
>
> 그래서 왜 그런 게 쓰였는지도  
> 이제 뒤에 어떤 그 네트워크 파이프라인을 보면 이해할 수가 있어요.  
>
> 그래서 이런 단순히  
> 이렇게 수학적인 가정으로부터 모델 specification도 달라진다.  
> 그런 개념을 아셔야 될 것 같고  
> U-Net이라는 개념이 있다.  
> 이렇게 이해할 수 있을 것 같습니다.  
>
> 그래서 이제 이거를...  
> 그냥 이렇게 생겼어요.  
> 근데 이게 앞에서 우리가 예제를 보고 와서  
> 이게 대충 어떤 텀들인지 이해가 가서 그런 거지  
> 사실 이게 한 번에 보기에는 쉽지가 않습니다.  
>
> f의 인버스를 미분을 한다.  
> 이게 좀 이해가 안 갈 수도 있어요.  
> 근데 이건 이제 뒤에서 다 묘사를 하겠습니다.  
>
> 그래서 이제 우리가 정보를 변형시키는 룰이 무엇이냐.  
> 그 룰을 점점 더 수학적으로 이론적으로  
> 잘 묘사하는 게 생성 모델의 발전 역사라고 얘기를 드렸는데  
> 이 Change of variables가  
> 그 하나의 어떤 그 GAN이나 VAE에서 진화를 한 것 같아요.  
>
> 이것도 굉장히 Constrained된 폼입니다.  
> Determinant라는 거를 N-Dimensional에서 계산을 해야 되는데  
> 사실 Determinant라는 게 굉장히 비싼 Operator예요.  
> 수학에서.  
>
> 그래서 N이 뭐 1000 Dimensional.  
> 1000 해봤자면 그렇게 크지 않습니다. 이미지로 치면은.  
> 거기서 Determinant Operator를 취해야 되는데 엄청난 에너지가 많이 들고  
> 그리고 이제 그 Inverse function을 구하는 것도 굉장히 에너지가 많이 들지만  
> 그걸 이제 미분을 해야 되잖아요.  
>
> 미분을 했을 때, 이거 이제 미분된 표현.  
> Partial X에 Partial F Inverse로 표현되어 있는.  
> 이게 이제 N 곱하기 N 매트릭스거든요.  
>
> 우리가 데이터가 10만.  
> 데이터의 차원이 10만이라고 가정하면  
> 10만 곱하기 10만이 이 Determinant 안에 들어가는 거고  
> Determinant는 이제 전통적으로 어떤 시뮬레이션 이론 같은 걸 보면  
> N에 3승 정도가 들어가요.  
>
> 그러면 N에 2승을 한 다음에, N에 3승을 해야 되는데  
> 말이 안 되는 거죠.  
>
> 그래서 이거 현실적으로  
> 이 수식을 그대로 쓸 수는 없습니다.  
> 뭔가 엔지니어적인 테크닉이 있어야 돼요.  
>
> 여하튼 근데 이거의 어떤 Motivation이나 철학적 모토는  
> 앞에서 이제 1D Example로 바로 이해하실 수 있을 것 같습니다.  

---

## p8. Example: Two-dimensional Change of Variables

- $Z_1$과 $Z_2$를 결합밀도 $p_{Z_1,Z_2}$를 갖는 연속 확률변수라고 하자.

- $u : \mathbb{R}^2 \to \mathbb{R}^2$ 를 가역변환이라고 하자.  
  두 입력과 두 출력으로 이루어져 있으며, 이를 $u = (u_1, u_2)$로 표기한다.

- $v = (v_1, v_2)$ 를 그 역변환이라고 하자.

- $X_1 = u_1(Z_1, Z_2)$ 이고 $X_2 = u_2(Z_1, Z_2)$ 라고 하자.  
  그러면 $Z_1 = v_1(X_1, X_2)$ 이고 $Z_2 = v_2(X_1, X_2)$ 이다.

  $$p_{X_1,X_2}(x_1,x_2)$$

  $$=
  p_{Z_1,Z_2}\bigl(v_1(x_1,x_2),\, v_2(x_1,x_2)\bigr)
  \left|
  \det
  \begin{pmatrix}
  \dfrac{\partial v_1(x_1,x_2)}{\partial x_1} &
  \dfrac{\partial v_1(x_1,x_2)}{\partial x_2} \\[6pt]
  \dfrac{\partial v_2(x_1,x_2)}{\partial x_1} &
  \dfrac{\partial v_2(x_1,x_2)}{\partial x_2}
  \end{pmatrix}
  \right|
  \quad \text{(inverse)}
  $$

  $$=
  p_{Z_1,Z_2}(z_1,z_2)
  \left|
  \det
  \begin{pmatrix}
  \dfrac{\partial u_1(z_1,z_2)}{\partial z_1} &
  \dfrac{\partial u_1(z_1,z_2)}{\partial z_2} \\[6pt]
  \dfrac{\partial u_2(z_1,z_2)}{\partial z_1} &
  \dfrac{\partial u_2(z_1,z_2)}{\partial z_2}
  \end{pmatrix}
  \right|^{-1}
  \quad \text{(forward)}
  $$

> **강의 내용**  
> 
> 그래서 뭐 이거는 좀 예제가 될 것 같은데요.  
> 2-dimension으로 해서 이제 u라는 어떤 Invertable한 어떤 Transformation.  
> Nonlinear Transformation이 존재했을 때  
> 뭐 이렇게 표현이 된다라고 했는데.  
>
> 제가 아까 앞에서 말씀드렸던 $f^{-1}$를 X에 대해서 미분하는 거.  
> 이게 이제 여기서 표현되어 있죠.  
> 2x2 Matrix로.  
> 그래서 차원이 2니까 2x2 Matrix로 나오는 거예요.  
>
> 그래서 이제 이런 룰을 따르고.  
> 뭐 디테일한 어떤 수학적인 것들은 직접 한번 살펴보셔도 좋을 것 같습니다.  
> straight forward 합니다.  
> 그냥 Notation이 복잡해집니다.  

---

> **inverse의 의미**  
> inverse는 **역변환 Jacobian**을 사용했다는 뜻이다.  
> 즉, $x \mapsto z$ 로 가는 역함수 $v(x)$를 미분하여  
> $ \frac{\partial v(x)}{\partial x} $ 의 Jacobian determinant를 쓰는 방식이다.  
> 이는  
> $$
> p_X(x) = p_Z(v(x))\,\left|\det\!\left(\frac{\partial v(x)}{\partial x}\right)\right|
> $$  
> 형태로 표현된다.
> 
> **forward의 의미**  
> forward는 **정방향 변환 Jacobian**을 사용했다는 뜻이다.  
> 즉, $z \mapsto x$ 로 가는 원래 함수 $u(z)$를 미분하여  
> $ \frac{\partial u(z)}{\partial z} $ 의 Jacobian determinant를 쓰고,  
> 그 역수를 취하는 방식이다.  
> 이는  
> $$
> p_X(x)
> =
> p_Z(z)\,
> \left|
> \det\!\left(\frac{\partial u(z)}{\partial z}\right)
> \right|^{-1}
> $$  
> 형태로 표현된다.
> 
> **두 방식의 관계**  
> 역함수 정리(inverse function theorem)에 의해  
> $$
> \det\!\left(\frac{\partial v(x)}{\partial x}\right)
> =
> \left[\det\!\left(\frac{\partial u(z)}{\partial z}\right)\right]^{-1}
> $$  
> 이므로, inverse 방식과 forward 방식은 완전히 동일한 결과를 준다.

---

## p9. Motivation: Normalizing Flows

- 관측 변수 $X$와 잠재 변수 $Z$ 위의  
  유향(directed) 잠재변수(latent-variable) 모델을 고려하자.

- **normalizing flow 모델**에서,  
  $f_\theta : \mathbb{R}^n \to \mathbb{R}^n$ 로 주어지는 $Z$와 $X$ 사이의 사상(mapping)은  
  결정적(deterministic)이며 가역적(invertible)이다.  
  따라서  
  $$
  X = f_\theta(Z), \quad Z = f_\theta^{-1}(X)
  $$

  <img src="/assets/img/lecture/probstat/13/image_5.png" alt="image" width="200px">

- 변수변환(change of variables)를 사용하면, 주변우도(marginal likelihood) $p(x)$는 다음과 같이 주어진다:

  $$
  p_X(x;\theta)
  =
  p_Z\!\left(f_\theta^{-1}(x)\right)
  \left|
  \det\!\left(
    \frac{\partial f_\theta^{-1}(x)}{\partial x}
  \right)
  \right|
  $$

- 참고: $x, z$는 연속적이어야 하며 동일한 차원을 가져야 한다.

> **강의 내용**  
> 
> 네.  
> 이제 normalizing flow로 정의를 하죠.  
> 그래서 normalizing flow라는 게 뭐냐면  
> Z라는 Latent Variable과 X라는 Data Variable이 존재했을 때  
> 주어진 $f_{\theta}$라는 Neural Network가 존재해서.  
>
> 그리고 그거 Deterministic 해야 됩니다.  
> 뭔가 Random Component가 들어가면 안 돼요.  
> 뒤에서 디퓨젼 모델은 이게 Random이 아니니까 Stochastic하게 되는데  
> Random이 아닌 Deterministic한 Neural Network가 존재해서  
> 같은 차원의 Input을 받고 Output을 내줘야 되고.  
>
> 그리고 Invertable 해야 된다.  
> 혹은 Monotone 해야 된다.  
>
> 그때 $f_{\theta}(Z)$가 X라는 것을 만들어 내고  
> $f^{-1}(X)$가 Z라는 것을 만들어 내게끔  
> 우리가 뭔가 고려하는 모델을 normalizing flow라고 합니다.  
>
> 근데 Flow라는 개념이 사실 이 정의는 정확하지는 않아요.  
> 왜냐하면 Flow라는 개념은 없기 때문에  
> Flow라는 게 뭔가 흐름 이런 건데.  
>
> 우리가 수학에서 vector field 이런 것을 얘기할 때는  
> 이런 게 여러 개가 중첩되어 있는 상태가  
> 이런 체이닝의 여러 중첩 상태의 Flow라고 부르기 때문에  
> 사실 이 자체를 명확하게 이렇게 부르면 안 되지만  
> 일단 우리가 그렇게 얘기를 한번 해볼게요.  
>
> 그리고 앞에서 얘기를 했었던  
> 이 General Case에서의 Change of Variable에서 나오는  
> 이 f들을 똑같이 생겼었는데 f가 그냥 $f_{\theta}$로 변해요.  
>
> 그랬을 때 X에 대한 Distribution을 구했죠.  
> 그럼 이제 여러분들 공부 열심히 하셨으면  
> 여기 그냥 바로 해보고 싶은 게 있는데.  
>
> 양변에 로그 씌우고 싶지 않나요?  
> 저만 그런가요?  
>
> 양변에 로그를 씌워서 이제 계산을 합니다.  
> 왜냐하면 우리가 로그 $P_{X}$ 같은 것을  
> Negative Log-Likelihood 같은 것을 계산할 때에 Evaluation을 해야 되기 때문에.  
>
> 로그를 취하면, 로그 AB는 로그 A 더하기 로그 B가 되잖아요.  
> 그래서 이 항이 이제 Decomposition이 쉽게 되고  
> 다루기 쉬운 형태로 표현이 됩니다.  

---

## p10. Flow of Transformations

**Normalizing:**  
가역 변환을 적용한 후, 변수변환(change of variables)은 정규화된(normalized) 밀도를 제공한다.

**Flow:**  
가역 변환들은 서로 합성될 수 있다.

$$
z_m
=
f_\theta^{m}
\circ \cdots \circ
f_\theta^{1}(z_0)
=
f_\theta^{m}\!\bigl(f_\theta^{m-1}\!(\cdots(f_\theta^{1}(z_0))\bigr)
\;\triangleq\;
f_\theta(z_0)
$$

- $z_0$에 대해 단순한(simple) 분포로 시작한다 (예: 가우시안).
- 최종적으로 $x = z_M$ 을 얻기 위해, $M$개의 가역 변환을 순차적으로 적용한다.
- 변수변환 공식에 의해,

  $$
  p_X(x;\theta)
  =
  p_Z\!\left(f_\theta^{-1}(x)\right)
  \prod_{m=1}^{M}
  \left|
  \det
  \left(
  \frac{\partial (f_\theta^{m})^{-1}(z_m)}
      {\partial z_m}
  \right)
  \right|
  $$

  (참고: 행렬곱의 determinant 는 determinant 들의 곱과 동일하다)

> **강의 내용**  
> 
> 그래서 이 페이지에서 Flow라는 것이 왜 나와야 되는 것이냐를  
> 설명을 하는 겁니다.  
>
> 그래서 $f^{1}$부터 $f^{m}$까지 $m$개의 Neural Network를 준비하고  
> 그 것들은 각각 N-Dimensional의 Vector를 받고 내뱉겠죠.  
>
> $z_{0}$이라는 것이 쉽게 말해서 원래  
> Gaussian Distribution이나 Uniform Distribution에서 Sampling된  
> Random Variables이라고 생각한 다음에  
> 이런 것을 Chaining이라고 합니다.  
>
> 사실 수학에서 Formal한 단어로는 Composition 이렇게 이야기 해야 되는데  
> Learning Theory 하시는 분들이나 ML에서는  
> 이런 식으로 Neural Network나 Function 같은 것들을  
> 여러 개를 중첩시켜 가지고 Transformation 하는 그런 행위를  
> Chaining한다, Chaining되어 있다 이렇게 표현해요.  
>
> 우리가 사슬, Chain할 때 그 Chain.  
>
> 그래서 $f_{\theta}$를 이제 $m$개의 Neural Network의 Composition 상태,  
> Chaining 상태에 통해서 $z_{m}$을 만들어 내는 것이  
> Flow라는 이름이 명명이 된 이유가 될 것이고  
> 그래서 우리가 가정을 했기 때문에  
> 이것을 따라가서 맨 밑에  
> Change of Variable이라는 것을 수식에 집어넣어야겠죠.  
>
> 왜냐하면 위처럼 여러 개로 중첩을 시켰다 보니까  
> 그러면 아까 계산을 하다 보면 밑에처럼 되는데요.  
>
> 그래서 큰 파이라고 표현되어 있는 것은 곱하기를 했다는 거예요.  
>
> 그래서 $m$이 $1$부터 $m$까지  
> Determinant에 대한 Absolute 취한 것이총 $m$개가 있는데  
> 그걸 다 곱한 것이고  
> 그리고 중간에 $z$에 대한 Distribution과 $x$에 대한 Distribution은 그대로 같습니다.  
>
> 그래서 그냥 모양이 되게 이쁘게 변했죠.  
> 이 곱하기밖에 없어야 되니까.  
>
> 그래서 여기서 왜 이런게 되냐.  
> Determinant of Product는 Product of Determinant가 같다.  
> 이것을 좀 이해를 해주셔야 될 것 같아요.  
>
> 그래서 이것이 곱하기가 사실 원래 안에 들어가 있었는데  
> 밖으로 나올 수 있는 이유였었고  
> 중요한 프로퍼티인데 그냥 기억만 해주시면 될 것 같아요.  

---

## p11. Maximum Likelihood Estimation

- 데이터셋 $\mathcal{D}$ 위에서 maximum likelihood를 이용한 학습:

$$
\max_{\theta} \log p_X(\mathcal{D};\theta)
=
\sum_{x \in \mathcal{D}}
\log p_Z\!\left(f_\theta^{-1}(x)\right)
+
\log
\left|
\det\!\left(
\frac{\partial f_\theta^{-1}(x)}{\partial x}
\right)
\right|
$$

- inverse 변환 $x \mapsto z$ 와 변수변환 공식(change of variables)을 통한  
  **정확한 우도 계산(exact likelihood evaluation)**

- forward 변환 $z \mapsto x$ 를 통한 **샘플링(sampling)**

$$
z \sim p_Z(z),
\quad
x = f_\theta(z)
$$

- inverse 변환을 통해 잠재 표현(latent representations) 추론  
  (inference network가 필요 없음!)

$$
z = f_\theta^{-1}(x)
$$

> **강의 내용**  
> 
> 그래서 이 페이지에서는 제가 아까 이렇게 하고 싶다는 것,  
> log를 취하고 싶다는 것을 하는 겁니다.  
>
> 그래서 앞에서 이제 x라는 Random Variable이 있고  
> z라는 Random Variable이 있을 때  
> 양변에 log를 취하게 되어서 이렇게 표현을 한 거고  
> 이것은 지금 체이닝 표시는 안 들어가 있네요.  
>
> 원래 log에다가 이 파이 표시로 되어 있는 것,  
> Product 되어 있는 것이 들어가야 되는데 여기는 안 들어가 있었죠.  
> summation이 되어야 되는데.  
>
> 그냥 표현하기 쉽게끔 그래서 f라는 게 체이닝이 됐다고 가정을 하고  
> f 입장에서 큰 function 입장에서 표현을 한 겁니다.  
>
> 그리고 우리가 이제 x가 여기 summation 밑에  
> x가 D라는 것에 포함되어 있는데  
> D는 이제 training data였죠. 테스트가 아니라.  
>
> 그래서 이게 summation 했다는 것은 이게 뭐죠? 지금?  
> 이게 notation이 좀 달라졌는데, 우리 중간고사 보기 전에 estimator라는 개념을 배웠어요.  
>
> 그래서 n개의 sample에 대해서, iid인 n개의 sample에 대해서 summation을 했을 때  
> true에 대한 estimation.  
> true likelihood에 대한 estimator 이런 얘기를 했었습니다.  
>
> 그래서 이건 estimator입니다.  
>
> 그래서 여기서 이걸로 이제 maximization을 해 줄 수 있는  
> neural network parameter $\theta$를 찾으면  
> 그 앞에서 배웠었던 neural network structure를 통해서  
> gradient descent을 하는 거죠.  
>
> descent가 아니죠.  
> descent 반대를 해야 되는 거죠.  
> 그러니까 gradient에 대해서 크게 만들어야 되는 겁니다.  
> 반대 방향으로.  
>
> 그래서 이제 이 maximum log-likelihood를 이제 훈련을 하면  
> 여기 이제 최종적으로 훈련 끝났을 때는, 우리가 어떻게 되는 거냐.  
>
> 그러면 이 z라는 거를 $f_{\theta}$에다가 input으로 넣으면  
> 그냥 x가 나옵니다.  
> 실제로 이걸 훈련을 마무리하면.  
>
> 그리고 이제 우리가 determinant 구할 때 inverse를 썼었잖아요.  
> 마찬가지로 x라는 데이터를 inverse function에다가 넣게 되면 z가 나옵니다.  
>
> 그래서 이걸 실제로 관찰을 할 수가 있어요.  
> 여러분들이.  
> 이 maximum log-likelihood estimation을 잘 했다고 봐서 한다면.  
>
> 그래서 이렇게 훈련을 하고.  
>
> 그러면 이제 우리가 샘플링 할 때는 어떻게 하냐.  
> 그 GAN 같은 경우도 그렇고 VAE도 그렇고 좀 단점이 있었는데  
> neural network를 두 개를 쓰잖아요. 그렇죠?  
>
> GAN는 discriminator랑 generator를 쓰고  
> VAE는 encoder랑 decoder를 쓰는데  
> 얘는 되게 웃긴 게 뭐냐면  
> 물론 f랑 f-inverse라는 어떤 추상적 객체 입장에서는 두 개인데  
> 사실 inverse를 한다는 건  
> 그냥 같은 neural network에다 뭔가 operation을 취해서 하나를 만든 거잖아요. 그렇죠?  
>
> 그래서 여기에서는 이제 neural network을 하나를 쓴다는 거죠.  
> 그게 이제 차이점이고.  
>
> 그리고 두 번째, 이제 샘플링 혹은 inference. test time.  
> 이때 어떤 데이터를 생성하는 과정을 생각을 해봤을 때  
> 우리가 데이터를 생성할 때 어떻게 했었죠?  
>
> VAE랑 GAN에서.  
> VAE에서는 이제 encoder를 버렸고, 이제 쓸모없다고 안 썼고  
> 그리고 GAN에서는 discriminator를 아예 안 썼잖아요.  
>
> 그런데 얘는 버려지는 게 없습니다.  
>
> 그 말은 뭐냐면  
> 이 variation을 한 formula를 갖고 가고  
> 제가 아까 수업 처음에 얘기했었던 건데  
> 이 variation을 한 formula를 가지고  
> 데이터가 어떻게 변해야 되는지에 대한 수학적 룰을 만들어주면 만들어줄수록  
> neural network를 사용하는 게 brute force manner가 아니라  
> 그게 효율적으로 바뀝니다.  
>
> 그러면 이제, 그냥 단순 산술 계산으로 했을 때  
> encoder랑 decoder가  
> 완전히 identical한 neural network의 스트럭쳐를  
> 갖고 있다고 생각을 해볼게요.  
>
> 제가 그런데 encoder, decoder 실제로 봤을 때 mirrored 이미지였잖아요.  
> 그래서 network parameter가 그렇게 크지가, 다르지가 않습니다.  
>
> 그런데 보기 전 두 배만큼 썼어야 됐는데  
> 여기서는 같은 network 하나로만 써도 됐기 때문에  
> 그럼 parameter가 반이고, 비용이 반이고,  
> 우리가 트레이닝할 때 어떤 인프라가 반이 되는 거고  
> 뭐 시간은 더 걸릴지는 모르겠지만.  
>
> 그래서 이런 추가적인 수학적 추상화가 들어가니까  
> 이 전체적인 이제 비용이 엄청나게 줄어졌습니다.  
>
> 그래서 이것 때문에 GAN을 안 쓰는 거에요.  
> generative AI에서.  
> (디퓨전 모델이 가장 스테이블하고 표현력이 좋지만)  
> 그 뿐만 아니라 인프라가 두 배로 먹는다는 단점이 있습니다.  
>
> 그리고 이거 별거 아닌 얘기일 수도 있는데, 잘 생각을 해보면 그럴까요?  
>
> H100 저는 한 2,000대 하고 훈련을 했거든요.  
> 그게 1년 얼마인데요.  
>
> 이렇게 4,000대로 늘어난다고 가정해봅시다.  
> 그러면 그거는 뭐 감당이 안 되는 거죠.  
>
> 그래서 이렇게 디퓨전 모델의 시대가 와서, 물론 여러 가지로 이제 발전이 있었겠지만,  
> 디퓨전 모델의 시대에 와서  
> 이제야 generative AI의 어떤 상업화를 이룰 수 있는 뭔가가 일어섰다고  
> 저는 평가를 합니다. 개인적으로.  
>
> 생성모델을 오래 공부한 입장에서.  
>
> 제가 처음에, 학부때였으니까, 뭐 한 10년 이상 봤었는데  
> 이제야 조금 뭔가 모델다운 모델들이 나오는 것 같아요.  
>
> 디퓨전 모델도 이제 끝이 아니에요.  
> 훨씬 더 앞으로 발전을 많이 해야 되는데, 실제 지금도 발전을 하고 있고.  
>
> 근데 여하튼 이런 흐름을 조금 이해를 해줘도 좋을 것 같아요.  
>
> 그래서 제가 이 네트워크 파이프라인을 보고  
> 우리가 수학적인 것도 많이 했지만 좀 메시지적으로 봤을 때  
> 생성 모델의 발전 역사가  
> 지금 제가 강의하는 방식이 생성 모델이 발전된 그 시간대로 하고 있거든요.  
>
> 그래서 처음에 원래 VAE라는 게, 이제 통계 같은 데 많이 하다가  
> 그 다음에 GAN이라는 게 딱 나왔어요.  
>
> Ian Goodfellow라는 사람이 엄청난 알고리즘을.  
> 그때 당시에 엄청 파격적인 거라고  
> 막 시니어 교수님들이 엄청 칭찬을 많이 했었어요.  
>
> 내가 본 30년 중에 가장 위대한 알고리즘  
> 이런 거 막 얘기하시는 교수님들이 많이 계셨는데.  
>
> 그렇게 나왔다가, 이제 normalizing flow라는 이런 개념.  
> variation한 걸 갖고 왔고.  
>
> 이것도 이제 모노톤 해야 되는 거고, 뭐 인버스 function 구하고,  
> 이런 것들 때문에 성능이 안 나는데.  
>
> 이게 최종적으로 디퓨전으로 가면서, 이 발전 역사가 있었다.  
>
> 이런 거 그냥, 이거 하면서 좀 얘기를 드리고 싶었어요.  
>
> 그래서 이제 다시 돌아가서, 또 페이지에서 얘기를 해보자면.  
>
> 샘플링 할 때는 어떻게 하느냐.  
> 아까 VAE랑 GAN에 대해서 얘기를 했는데.  
>
> 그럼 이제 그대로 하는 겁니다.  
> 이게 보면 $f_{\theta}$가 체이닝으로 되어 있잖아요.  
>
> 그래서 이거 bold-faced 되어 있는 게 체이닝 됐다는 의미인 것 같은데.  
>
> 사실 이제 이렇게 하는 거죠.  
> z라는 게 $z_{0}$이라고 이렇게 표현이 되는 거고.  
>
> 플로우라는 거에서.  
>
> 그래서 이제 z라는 걸 넣을 때  
> set of neural network에 이제 계속 input output, input output 하면서  
> for 문을 돌리는 거예요.  
>
> 그럼 여기서 이제 심각한 관점이 보여야 됩니다.  
>
> 디퓨전 모델도 그렇고.  
> 항상 뭐 수학적으로 뭐 어쩌고 어쩌고 얘기를 했는데.  
> 뭐 인프라를 반으로 쓰고 어쩌고 어쩌고 했는데.  
>
> 이게 divide and conquer가 문제가 뭐죠?  
>
> 이게 야만족들을 쪼개가지고 점령을 한다는 건데.  
>
> 일단 쪼개면 일이 늘어나는 거 아닌가요?  
>
> 그죠?  
>
> 그럼 이거를 N번 호출을 해야 돼요.  
> neural network를.  
>
> 그러니까 시간 복잡도는 늘어나는 겁니다.  
>
> 총 네트워크, 총 인프라는 줄어도 되는데.  
>
> 그러니까 총 maximum capacity로  
> 우리가 GPU를 얼만큼 넣을 것인가에 대한 얘기를 할 때는  
> 인프라가 전체적으로 줄 수는 있는데.  
>
> 이걸 N번을 호출해야 되잖아요.  
> neural network 똑같은 거를.  
>
> 그럼 시간이 얼마나 많이 걸리겠습니까?  
>
> 한 번 호출하는데도  
> 다 여러분들 API 돈 내잖아요.  
>
> 매니지드 모델 할 때.  
>
> 호출 한 번 할 때 요즘 얼마죠?  
> 잘 몰라서.  
>
> 많이 싸지긴 했는데, 최신 GPT 모델들도 토큰 한 번 뱉는데 어마어마합니다.  
>
> 이걸 N번으로 해야 돼요.  
>
> 전체적인 콤플렉시티가 무조건적으로 줄었다  
> 이렇게 얘기할 수는 없고  
>
> 뭐를 포기하면 뭔가를 얻어간다는  
> 그런 개념으로 봐주셔야 될 것 같아요.  
>
> 디퓨젼 모델도 똑같습니다.  
>
> 디퓨젼 모델도, 이런 variational approach로  
> 생성 모델을 만들었다고 얘기를 했는데.  
>
> 걔도 이렇게 N번 호출해야 되거든요.  
>
> 똑같은 모델에 넣고 빼고 넣고 빼고 하면서.  
>
> 그럼 걔도 시간이 오래 걸리겠죠.  
>
> 그래서 여러분들이  
> 나노바나 같은 거 할 때, 이미지 생성 모델한테 만들어 달라고 할 때  
> 시간이 오래 걸리는 이유가 뭐냐면  
>
> 요거를 막 많으면, 5,000번씩 해야 돼요.  
> 5,000번 neural network 호출하는 거예요.  
>
> GAN은 한 번 하면 되는데.  
>
> 근데 GAN은 대신에 훈련할 때 인프라가 2개를 들고 좀 stable한 러닝이 안 됐었잖아요.  
> 저희 공부를 했었을 때.  
>
> 그렇다고 VAE로 가면 성능이 떨어져요.  
>
> 그래서 그런 trade-off가 다 존재를 한다.  
>
> 마법의 도구들이 아니다.  
>
> 수학적 모델이 어떻게 하느냐에 따라서 이 문제들을 해결했지만  
> 항상 잔존하는 게 있다라는 거를 이면에 메시지를 좀 이해를 하고  
> 있으셔야 될 것 같아요.  
>
> 하여튼 샘플링은 이렇게 체이닝을 통해 하고.  
>
> 그리고 이제 우리가 인코더에 해당하는 부분.  
>
> 그러니까 X라는 이미지를 넣었을 때  
> Z라는 거에 이제 latent representation을 찾는 경우는  
> inverse function을 통해서 mapping을 하는 거다.  
>
> 이것도 마찬가지로, 이제 체이닝을 하겠죠.  
>
> 어쨌든 간에 이 스트럭처를 보면 VAE에서 해방된 거는 명확하게 보입니다.  
>
> 똑같은 얘기지만.  
>
> 하나의 neural network로  
> 이제 인코더, 디코더, 스트럭처 2개를  
> 그냥 하나로 썼다.  
>
> 이게 핵심 메시지라고 생각해 주시면 될 것 같아요.  

---

## p12. Flow of Transformations

- 기본 분포(base distribution): Gaussian

  <img src="/assets/img/lecture/probstat/13/image_6.png" alt="image" width="800px">

- 기본 분포(base distribution): Uniform

  <img src="/assets/img/lecture/probstat/13/image_7.png" alt="image" width="800px">

- 10개의 planar(평면) 변환은 단순한 분포들을 더 복잡한 분포로 변환할 수 있다.

> **강의 내용**  
> 
> 그러면 이제 아까 앞에서 이제 A라는 매트릭스를 말씀드리고  
> 그리고 모양을 순차적으로 변형시키면서  
> 최종적으로 우리가 원하는 모양으로 만드는 게  
> 우리의 지금 공부하는 모델의 어떤 특징이라고 얘기를 드렸었는데  
> 그걸 이제 그림으로 보면서 이해를 한번 해보죠.  
>
> 그래서 이제 Z라는 거.  
> 우리가 latent variable이라고 얘기를 했었던 걸  
> Gaussian으로 만약에 가정을 해봅시다.  
> 그래서 유닛 Gaussian으로. 스탠다드 Gaussian으로.  
>
> Distribution을 고려를 했을 때  
> 이제 위에, 맨 좌측에 있는 이미지부터 시작을 하는 거예요.  
>
> 그래서 우리가 이제 또 하나의 가정은 잘 모르겠지만  
> 일단 모노톤한 neural network가 있다고 가정을 하고  
> 훈련이 끝난다고 가정을 해봅시다.  
>
> 그러면 이 M이라는, 여기서 지금 보이는 M이  
> 이 앞에서 이 체이닝에 대한 횟수거든요.  
>
> 그러면 이 체이닝을 할 때, 중간에 이제 계속 관찰하는 거예요.  
> 이 정보들이 어떻게 바뀌느냐.  
>
> 그래서 처음에는 Gaussian distribution이 되어 있던 게  
> 시간이 지나서 점점 더 복잡한 걸로 쪼개지고  
> 한 번 했을 때는 그냥 가우시안이 이제 눌렸는데  
> 이제 두 번 하니까 그게 쪼개졌습니다.  
>
> 근데 10번을 하다 보니까, 모양들이 굉장히 쪼개지고  
> 복잡한 프로파일도 다 표현할 수 있는 이런 이제 개형을 보여주고  
>
> 그리고 이제 두 번째 로그에서는 이제 유니폼 디스트리뷰션을 가정하는 거예요.  
>
> 유니폼 디스트리뷰션도 마찬가지로  
> 한 번 전파를 시키니까 반으로 쪼개지고, 두 번 할 때 쪼개지고.  
>
> 이건 그냥 예제입니다.  
> 꼭 이렇게 한번 쪼개지고 두번 쪼개지고 이런다는 보장은 없어요.  
> 그거는 뉴럴 네트워크 스트럭처마다 다릅니다.  
>
> 여튼 이제 10번을 propagation 시켰을 때는 되게 복잡한 모양으로 표현할 수 있다는 거죠.  
>
> 그래서 여기에서 이제 가정은, 플레이너 트랜스포메이션이라고 해서  
> $aX + b$ 꼴의 함수를 고려했을 때  
> $aX + b$는 리니어 펑션이기 때문에  
> 이런 건 뉴럴 네트워크로 가정했을 때  
> 이런 모양은 안 나오는데  
>
> 우리가 플레이너 트랜스포메이션  
> 혹은 어파인 트랜스포메이션이라고 했을 때는  
> 우리가 서프트 벡터 머신 공부하셨으면 이해를 하시겠지만  
> 한 번씩 밖에 못 잘라요.  
>
> 우리가 직선을 가하는 거기 때문에  
> 그래서 아핀 트랜스포메이션에서  
> 위에 같은 개형이, 디스트리뷰션으로 변화한다.  
>
> 이해해 주시면 될 것 같습니다.  

---

## p13. Flow of Transformations

- 효율적인 샘플링과 계산 가능한(tractable) 우도 평가(likelihood evaluation)를  
  가능하게 하는 단순한 prior $p_Z(z)$.  
  예: 등방성 가우시안(isotropic Gaussian)

- 계산 가능한 평가를 갖는 가역 변환들:
  - 우도 계산(likelihood evaluation)은  
    $x \mapsto z$ 매핑을 효율적으로 계산할 수 있어야 한다.
  - 샘플링은  
    $z \mapsto x$ 매핑을 효율적으로 계산할 수 있어야 한다.

- 우도를 계산하는 것은 또한  
  $n \times n$ Jacobian 행렬들의 행렬식(determinant) 계산을 필요로 한다.  
  여기서 $n$은 데이터의 차원이다.
  - $n \times n$ 행렬의 행렬식(determinant) 계산은 $O(n^3)$으로,  
    학습 루프 내에서 사용하기에는 지나치게 비싸다.

- **핵심 아이디어(Key idea):**  
  Jacobian 행렬이 특별한 구조를 갖도록 변환을 선택한다.  
  예를 들어, 삼각행렬(triangular matrix)의 행렬식(determinant)은  
  대각 원소들의 곱이며, 이는 $O(n)$ 연산이다.

> **강의 내용**  
> 
> 네, 그래서 여기서는 좀 개요에 대해서 말씀드리는데  
> 아까 말씀드렸었던 것처럼  
> 디터미넌트, n by n 매트릭스의 디터미넌트를 구하려면  
> $n^{3}$이 컴퓨테이션 코스트에 드는 게  
> 이제 정석적, 스탠다드한 우리가 알고 있는 사실이거든요.  
>
> 근데 이건 사실 불가능하다.  
> 디터미넌트를 어떻게 잘 구할 수 있는 걸 찾아야 된다.  
> 이걸 그냥 하면 안 된다 라는 거고  
> 뭐 이런 수학적인 것들은 이제 빼고  
> 뒤에 시간 남으면 또 더 얘기를 드릴게요.  

---

## p14. Triangular Jacobian

$$
\mathbf{x} = (x_1, \ldots, x_n)
= f(\mathbf{z})
= (f_1(\mathbf{z}), \ldots, f_n(\mathbf{z}))
$$

$$
J
=
\frac{\partial f}{\partial \mathbf{z}}
=
\begin{pmatrix}
\frac{\partial f_1}{\partial z_1} & \cdots & \frac{\partial f_1}{\partial z_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial z_1} & \cdots & \frac{\partial f_n}{\partial z_n}
\end{pmatrix}
$$

$x_i = f_i(\mathbf{z})$가 $\mathbf{z}_{\le i}$에만 의존한다고 하자. 그러면

$$
J
=
\frac{\partial f}{\partial \mathbf{z}}
=
\begin{pmatrix}
\frac{\partial f_1}{\partial z_1} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial z_1} & \cdots & \frac{\partial f_n}{\partial z_n}
\end{pmatrix}
$$

는 하삼각(lower triangular) 구조를 갖는다.  
행렬식(determinant)은 **선형 시간(linear time)** 안에 계산될 수 있다.

마찬가지로, $x_i$가 $\mathbf{z}_{\ge i}$에만 의존하는 경우  
Jacobian은 상삼각(upper triangular)이다.

---

## p15. Normalizing Flows: Theoretical Side

**표기(conventions)**

- $\mu$와 $\nu$를 $\mathbb{R}^d$ 위의 확률측도(probability measures)라고 하고,  
  각각 르베그(Lebesgue) 밀도 $p$와 $q$를 갖는다고 하자.

- 우리는 $\mu$와 $p$를 **목표(target) 측도/밀도**라고 부르고,  
  $\nu$와 $q$를 **기준(reference) 측도/밀도**라고 부른다.

<img src="/assets/img/lecture/probstat/13/image_8.png" alt="image" width="480px">

그림: $\mu$는 왼쪽, $\nu$는 오른쪽

---

> **르베그 밀도(Lebesgue density)란?**  
>
> 르베그 밀도는 $\mathbb{R}^d$ 공간에서 정의된 확률측도가  
> **르베그 측도(일반적인 유클리드 공간의 길이·면적·부피 개념)** 에 대해  
> 절대연속일 때 존재하는 확률밀도 함수이다.
>
> 즉, 확률측도 $\mu$가 르베그 측도 $\lambda$에 대해 절대연속이면  
> 라돈-니코딤 정리에 의해 다음을 만족하는 함수 $p(x)$가 존재한다:
>
> $$
> d\mu(x) = p(x)\, d\lambda(x)
> $$
>
> 여기서의 $p(x)$가 바로 **르베그 밀도(Lebesgue density)** 이다.
>
> 직관적으로 말하면,  
> “일반적인 연속 공간에서의 확률밀도 함수(pdf)”가  
> 바로 르베그 밀도라고 이해할 수 있다.

---

## p16. Normalizing Flows: Theoretical Side

**normalizing flow**는  
$p$에서 $q$로 가는 사상(mapping) $T : \mathbb{R}^d \to \mathbb{R}^d$로서,  
다음 조건들을 만족한다:

(i) $T$는 거의 모든 곳에서(a.e.) 미분 가능하며,  
    거의 모든 $z$에 대해 $\det J_T(z) \neq 0$ 이다.

(ii) $$T_{\#}\mu = \nu$$, 혹은 밀도 관점에서 쓰면 다음과 같다:

$$
q(y)
=
\frac{p\!\left(T^{-1}(y)\right)}
     {\left|\det J_T\!\left(T^{-1}(y)\right)\right|}
\qquad \Longleftrightarrow \qquad
p(x)
=
q(T(x)) \,\left|\det J_T(x)\right|
$$

여기서 $J_T$는 $T$의 Jacobian이다.  
이 식을 다음과 같이 쓸 수도 있다:  
$$q = T_{\#}p$$.

식 (1)에 나타난 양의 로그(log)를 자주 고려하게 된다:

$$
\log p(x)
=
\log q \circ T(x)
+
\log\!\left|\det J_T(x)\right|
$$

---

## p17. Normalizing Flows: Theoretical Side

$T_1, T_2, \ldots, T_n$이 normalizing flow라고 하고,  
$T = T_n \circ T_{n-1} \circ \cdots \circ T_1$이 $q$에서 $p$로 가는 normalizing flow라고 하자.  

그러면,

$$
\log p(x)
=
\log(q \circ T(x)) + \log\left|\det J_T(x)\right|
$$

또는,

$$
\log p(x)
=
\log(q \circ T(x))
+
\sum_{j=1}^{n}
\log\left|\det J_{T_j}(z_{j-1})\right|
$$

여기서  
$z_0 = x$,  
$z_1 = T_1(z_0)$,  
$z_2 = T_2(z_1)$,  
$\ldots,$  
$z_{n-1} = T_{n-1}(z_{n-2})$ 이다.

---

## p18. Recap : KL divergence

KL-발산의 성질은 Jensen 부등식으로부터 얻어진다:

$$
\begin{aligned}
- D_{\mathrm{KL}}(\pi_1 \mid \pi_2)
&= - \mathbb{E}_{\pi_1}\!\left( \log \frac{\pi_1}{\pi_2} \right) \\[6pt]
&= \mathbb{E}_{\pi_1}\!\left( \log \frac{\pi_2}{\pi_1} \right) \\[6pt]
&\le
\log\!\left(
        \mathbb{E}_{\pi_1}\!\left( \frac{\pi_2}{\pi_1} \right)
     \right)
\qquad\qquad\qquad\text{since log is concave} \\[6pt]
&=
\log\!\left(
        \int_{\{\pi_1(x) > 0\}}
        \pi_1(x)\, \frac{\pi_2(x)}{\pi_1(x)}\, dx
     \right) \\[6pt]
&\le \log(1)
\qquad\qquad\qquad\quad\quad\quad\quad\quad\text{as log increasing} \\[6pt]
&= 0.
\end{aligned}
$$

---

> **왜 ‘since log is concave’가 등장하는가?**  
> Jensen 부등식에 따르면, 어떤 함수가 **concave(오목)** 하면  
> $$\mathbb{E}[f(X)] \le f(\mathbb{E}[X])$$  
> 이 성립한다.  
> 여기서 로그 함수 $\log(\cdot)$는 concave이므로  
> $$\mathbb{E}_{\pi_1}\!\left( \log \frac{\pi_2}{\pi_1} \right)
> \le
> \log\!\left(
> \mathbb{E}_{\pi_1}\!\left( \frac{\pi_2}{\pi_1} \right)
> \right)$$  
> 이 부등식이 바로 Jensen 부등식의 직접적인 적용이다.
> 
> ---
> 
> **왜 ‘as log increasing’이 등장하는가?**  
> 로그 함수 $\log(\cdot)$는 **증가함수(increasing function)** 이고,  
> $$ \int \pi_1(x)\, \frac{\pi_2(x)}{\pi_1(x)} dx = 1 $$  
> 이므로  
> $$ \log\!\left(\int \cdots \right) \le \log(1) $$  
> 이 성립한다.  
> 즉, 내부 값이 1 이하라면 $\log$를 취해도 그 관계가 유지되며,  
> 결국 최종적으로 $0$ 이하가 되고 KL 발산의 음수 형태가 0 이하임을 보여준다.

---

## p19. Maximum Likelihood Estimation

**최적화 기반 $T$ 학습(Learning $T$ via Optimization)**

$$
\min D_{\mathrm{KL}}(T_{\#}p \mid q)
\quad\text{s.t.}\quad
\det \nabla T > 0 \;\;\text{and}\;\; T \in \mathcal{F}.
$$

$$D_{\mathrm{KL}}(T_{\#}p \mid q)
= D_{\mathrm{KL}}(p \mid T_{\#}^{-1} q)$$ 이므로,  
$\{x_i\}_{i=1}^n$이 $\mu$로부터의 i.i.d. 표본일 때, 표본 평균 근사는 다음과 같다:

$$
\min
\; -\frac{1}{n}
\sum_{i=1}^{n}
\left[
\log\, q \circ T(x_i)
+
\log \left| \det \nabla T(x_i) \right|
\right]
\quad\text{s.t.}\quad
\det \nabla T > 0
\;\;\text{and}\;\;
T \in \mathcal{F}.
$$

> **강의 내용**  
> 
> 네.  
> 그래서 여기까지가 어떤 전체적인 개요고요.  
> 시간이 남으면 이론적인 거 살짝 보겠다고 얘기를 들었는데  
> 조금만 더 얘기하고 마무리 하죠.  
>
> 그래서 우리가 optimization 얘기를 살짝 했었었는데  
> 그럼 optimization 관점에서 이렇게 maximum likelihood와  
> 이 normalizing flow를 엮어가지고 얘기를 한번 해봅시다.  
>
> 제가 말씀드렸었죠.  
> 결국에는 우리가 하는 거는 여기서도 마찬가지로 $KL$ divergence를 줄이는 과정이다.  
> 라는 걸 우리가 이해를 했었었고  
> 
> 여기서 $T_{\sharp}$이라고 하는 거는  
> 그 $p$라는 거가 probability distribution이고  
> $q$라는 것도 probability distribution인데  
> $p$라는 거는 어떤 거냐면 이 Gaussian distribution.  
> 이 Gaussian distribution은 $p$라고 생각하시면 될 것 같아요.  
> (잘못 설명된 부분, q가 Gaussian distribution임)  
>
> 그래서 우리가 $T$가 어떤 transformation이었는데  
> Gaussian distribution을 transform을 해서 나오는 결과가  
> 실제로 데이터 distribution과 $KL$ divergence가 minimization 돼야 된다.  
>
> 근데 여기서 중요한 게 밑에서 가정조건이 들어가죠.  
> $\det \nabla T$가 $0$보다 커야 된다.  
>
> 이게 뭔 뜻이냐면  
> 아까 말했던 convex니, monotony 다 이거 만들고 하는데요.  
> 이게 존재해야만 inverse function이 존재합니다.  
>
> 그래서 되게 중요한 내용인데, 이런 가정조건을 하는데  
> 이게 뭐 이렇게 막 계산하고 어려운 걸  
> 제가 다 말씀드리고 했지만  
> 사실 아까 앞에서 말씀드렸었던 그런 형태로  
> 이 $KL$ divergence 풀게 되면 밑에와 같은 형태가 되고  
> 이 형태는 앞에서 우리가 objective function을 고려하려고 했었잖아요.  
>
> 이렇게 되는 거고  
> 이 $T$를 예를 들어서 우리가 뒤에서 고려했었던 planar flow라든지  
> input convex neural network 같은 것들을 고려하게 되면  
> 이 such that의 이 수학적 constraint를 다 만족하게 된다.  
>
> 그래서 데이터를 잘 생성을 한다.  
> 이렇게 이해해주시면 될 것 같습니다.  

---

> 우리는 변환 $T$ 를 학습하고 싶다.  
> 즉, **복잡한 데이터 분포($x$ 의 분포 = $p$)**를 **단순한 기준 분포($z$ 의 분포 = $q$)**로  
> 최대한 잘 밀어(push) 넣도록 하는 변환 $T$ 를 찾는다는 뜻이다.
> 
>> $$T_{\#}p$$ 는 **푸시포워드(pushforward) 분포**로,  
>> “$x$-공간의 분포 $p$를 변환 $T$에 통과시켜  
>> $z$-공간의 분포로 옮겼을 때 얻어지는 분포”를 의미한다.  
>> 즉,  
>> $x \sim p$ 이면  
>> $$z = T(x)$$ 의 분포가 바로 $$T_{\#}p$$ 이다.  
>> 그리고 $x \mapsto T(x) = z$ 이다.
> 
> ---
> 
> 제한식 $\det \nabla T > 0$ 은  
> $T$ 가 거의 어디서나 가역이며 Jacobian의 행렬식이 0이 되지 않도록 보장한다는 뜻이다.
> 
> ---
> 
> 조건 $T \in \mathcal{F}$ 는  
> $T$ 가 미리 정한 함수 클래스(예: 특정 신경망 구조) 안에 있어야 한다는 뜻이다.
> 
> ---
> 
> 항등식  
> 
> $$
> D_{\mathrm{KL}}(T_{\#}p \mid q)
> =
> D_{\mathrm{KL}}(p \mid T_{\#}^{-1} q)
> $$
> 
> 는 KL 발산이 변수변환 아래에서 형태를 유지한다는 뜻이다.
>
>> KL 발산은 확률밀도에 대한 적분 형태로 정의되는데,  
>> 변수변환(예: $x \mapsto T(x)$ 또는 $z \mapsto T^{-1}(z)$)을 적용해도  
>> Jacobian 항이 나타나면서 양쪽 식이 서로 정확히 보정된다.  
>>
>> 즉,  
>> 분포를 $T$ 로 밀어(push) 옮기느냐,  
>> 혹은 $T^{-1}$ 로 끌어오느냐의 표현만 달라질 뿐  
>> “두 분포 사이의 정보 차이” 자체는 동일하게 유지되기 때문에  
>> 위 항등식이 성립한다는 뜻이다.
> 
> ---
>
> 표본 $\{x_i\}_{i=1}^n$ 을 이용해 기대값을  
> 표본 평균으로 근사할 수 있다는 뜻이다.
> 
> ---
>
> 최종 최적화식  
> 
> $$
> \min
> -\frac{1}{n}
> \sum_{i=1}^{n}
> \left[
> \log(q(T(x_i)))
> +
> \log \left| \det \nabla T(x_i) \right|
> \right]
> $$
> 
> 는 normalizing flow에서 사용되는 **로그-우도의 최대화**와 동등하다는 뜻이다.

---

## p20. Maximum Likelihood Estimation

<img src="/assets/img/lecture/probstat/13/image_9.png" alt="image" width="800px">

> **강의 내용**  
> 
> 그래서 뭐 굉장히 복잡하죠.
> 그리고 이런 거 매일 합니다.
> 논문을 쓰려고.
> 네.
> 그래서 이렇게 막 사라지는 항들이 있고  
> KL divergence가 어떻게 유도가 되느냐 그런 것들을 계산을 직접 할 수 있고 그렇습니다.

---

> **(1) 첫 번째 줄**
> 
> $$
> D_{\mathrm{KL}}(T_{\#}p \,\|\, q)
> =
> \int_{\mathbb{R}^d} (T_{\#}p)(y)\,\log\frac{(T_{\#}p)(y)}{q(y)}\,dy
> $$
> 
> KL 발산의 정의를 그대로 쓴 것이다.
>
>
> **(2) 두 번째 줄 — pushforward 밀도 $$(T_{\#}p)(y)$$ 를 전개한 단계**
> 
> $$
> \int_{\mathbb{R}^d}
> p(T^{-1}(y))\,\det\nabla T^{-1}(y)\,
> \log
> \frac{
> p(T^{-1}(y))\,\det\nabla T^{-1}(y)
> }{
> q(y)
> }
> dy
> $$
> 
> >
> > 이유:
> > pushforward 밀도는 정의상
> > 
> > $$
> > (T_{\#}p)(y)
> > =
> > p(T^{-1}(y))\,
> > \left|\det\nabla T^{-1}(y)\right|
> > $$
> > 
> > KL 식 안의 모든 $$(T_{\#}p)(y)$$ 를 이 정의로 치환했기 때문에 이렇게 바뀐다.
>
>
> **(3) 세 번째 줄**
> 
> 아래 줄은 앞 단계의 적분을  
> **변수변환 $y = T(x)$** 로 바꾸고 난 뒤의 결과이다:
>
> $$
> \int_{\mathbb{R}^d}
> p(x)\,
> \log
> \frac{
> p(x)
> }{
> q(T(x))\,\det\nabla T(x)
> }
> dx
> $$
>
> ---
>
> > **(a) 먼저, 치환을 적용한다:  $y = T(x)$**
> >
> > 이제 적분 변수는 $y$ 대신 $x$ 가 된다.  
> > 이때 **항등적으로**  
> > $$
> > T^{-1}(y) = x
> > $$
> > 이므로,
> > 기존 적분에 나타났던  
> > $p(T^{-1}(y))$ 는 단순히  
> > $$
> > p(x)
> > $$  
> > 로 바뀐다.
>
> ---
>
> > **(b) 미분 형식 변화:  $dy = \mid \det\nabla T(x) \mid \,dx$**
> >
> > 적분에서 변수 치환을 하면  
> > $dy$ 를 $dx$ 로 바꿔야 하는데,  
> > 이때 “부피 변화율”이 Jacobian determinant 이다.
> >
> > 즉,
> > $$
> > dy = \left|\det\nabla T(x)\right|\,dx.
> > $$
> >
> > 따라서 이전 적분에 있던 $dy$ 자리는  
> > $$
> > \left|\det\nabla T(x)\right|\,dx
> > $$  
> > 로 바뀐다.
>
> ---
>
> > **(c) 역함수 Jacobian 관계를 적용한다**
> >
> > inverse function theorem 에 의해,
> >
> > $$
> > \det\nabla T^{-1}(y)
> > =
> > \frac{1}{\det\nabla T(x)}.
> > $$
> >
> > 이것이 아주 중요하다.  
> > 왜냐하면 이전 단계의 integrand 에는 $p(T^{-1}(y))\det\nabla T^{-1}(y)$  
> > 그리고 measure $dy$ 안에는 $|\det\nabla T(x)|$  
> > 라는 두 개의 Jacobian 이 **곱 형태**로 등장하기 때문이다.
>
> ---
>
> > **(d) Jacobian 항들이 정확히 상쇄되는 과정**
> >
> > 치환 전 integrand 의 Jacobian 파트:
> > 
> > $$
> > \det\nabla T^{-1}(y)
> > $$
> >
> > 치환으로 생긴 measure 변화:
> > $$
> > dy = (\det\nabla T(x))\,dx
> > $$
> >
> > 둘을 곱하면:
> > 
> > $$
> > \det\nabla T^{-1}(y)\; \det\nabla T(x)
> > =
> > \frac{1}{\det\nabla T(x)}\; \det\nabla T(x)
> > =
> > 1.
> > $$
> >
> > 즉, **완전히 사라진다.**
>
> ---
>
> > **(e) log 안에서 일어나는 변화**
> >
> > 기존 식에서는  
> > 
> > $$
> > \log\frac{
> > p(T^{-1}(y))\,\det\nabla T^{-1}(y)
> > }{
> > q(y)
> > }
> > $$
> >
> > 이제 각각을 $x$ 기준으로 바꾸면:
> >
> > - $p(T^{-1}(y)) \rightarrow p(x)$  
> > - $q(y) \rightarrow q(T(x))$  
> > - $\det\nabla T^{-1}(y) \rightarrow 1/\det\nabla T(x)$
> >
> > 따라서 log 내부는:
> > 
> > $$
> > \log\frac{
> > p(x)
> > }{
> > q(T(x))\,\det\nabla T(x)
> > }.
> > $$
>
> ---
>
> > **(f) 최종 정리**
> >
> > 적분 변수는 전부 $x$ 기준,
> > Jacobian 은 log 안과 measure 에서 정확히 정리되고,
> > 전체 결과는 다음과 같이 깔끔해진다:
> >
> > $$
> > \int_{\mathbb{R}^d}
> > p(x)\,
> > \log
> > \frac{
> > p(x)
> > }{
> > q(T(x))\,\det\nabla T(x)
> > }
> > dx
> > $$
>
> **(4) 네 번째 줄 — 로그 분리 및 적분 분리**
> >
> 로그 성질 적용:
> 
> $$
> \log \frac{p(x)}{q(T(x))\,\det\nabla T(x)}
> =
> \log p(x)
> -
> \log q(T(x))
> -
> \log \det\nabla T(x)
> $$
> 
> 적분의 선형성 때문에 세 항을 분리 가능:
>  
> $$=
> \int_{\mathbb{R}^d} p(x)\log p(x)\,dx
> -
> \int_{\mathbb{R}^d} p(x)\log q(T(x))\,dx
> -
> \int_{\mathbb{R}^d} p(x)\log\det\nabla T(x)\,dx
> $$

---

## p21. Maximum Likelihood Estimation

<img src="/assets/img/lecture/probstat/13/image_10.png" alt="image" width="800px">

---

## p22. Normalizing Flows의 예시

- 비선형성 함수 $h : \mathbb{R} \to \mathbb{R}$

- Jacobian 계산에는 “행렬 행렬식 보조정리(matrix determinant lemma)”가 필요하다:

  $$
  \det(A + u v^{T})
  =
  \left( 1 + v^{T} A^{-1} u \right)\det(A)
  $$

- 그림: $h$의 예시들

<img src="/assets/img/lecture/probstat/13/image_11.png" alt="image" width="600px">

---

> **행렬 행렬식 보조정리(matrix determinant lemma)**란?
>
> 이 보조정리는  
> 행렬 $A$ 에 **랭크 1(rank-1) 행렬** $u v^{T}$ 를 더한 행렬  
> $A + u v^{T}$ 의 행렬식을  
>
> $$
> \det(A + u v^{T})
> =
> \left( 1 + v^{T} A^{-1} u \right)\det(A)
> $$
>
> 형태로 매우 간단하게 계산할 수 있게 해주는 정리이다.
>
> 즉,  
> **큰 행렬의 행렬식을 직접 계산하지 않고**,  
> **$A^{-1}$과 $u, v$의 내적만으로**  
> 새로운 행렬의 행렬식을 얻을 수 있게 해주는 도구이다.
>
> 이 성질은 normalizing flow에서 Jacobian 행렬이  
> “기본 행렬 + rank-1 보정” 형태로 나올 때  
> 연산을 크게 줄여주는 핵심 트릭이다.
>
> ---
>
> **그림의 의미 (Examples of $h$)**  
>
> 그림은 normalizing flow에서 사용되는  
> **비선형성 함수 $h$** 의 여러 예시를 보여준다.
>
> - 파란색: 완만한 S-curve 형태의 비선형 함수  
> - 빨간색: 단순 선형 함수  
> - 초록색: sigmoid-like 형태의 비선형 함수  
>
> 이러한 $h$들은 flow 모델에서  
> **복잡한 분포를 만들기 위한 비선형 변형의 기본 요소**로 사용된다.
>
> 즉,  
> *간단한 분포(z)를 복잡한 분포(x)로 만들기 위해  
> 어떤 모양의 비선형 함수를 사용할 수 있는지 시각적으로 보여주는 그림*이다.

---

## p23. Planar Flows (Single Layer SVM)

- $u, w \in \mathbb{R}^d$, $b \in \mathbb{R}$, 그리고 비선형성 $h : \mathbb{R} \to \mathbb{R}$가 주어졌을 때,  
  planar flow $T_{pf} : \mathbb{R}^d \to \mathbb{R}^d$ 를 다음과 같이 정의한다:

  $$
  T_{pf}(z)
  =
  z + u\, h(w^{T} z + b).
  $$

- 우리는 다음을 계산한다:

  $$
  J_{T_{pf}}(z)
  =
  I_d
  +
  (u w^{T})\, h'(w^{T} z + b)
  \;\;\Longrightarrow\;\;
  \det J_{T_{pf}}(z)
  =
  1 + w^{T} u\, h'(w^{T} z + b).
  $$

- 그림: Planar flow에서의 변형 예시

<img src="/assets/img/lecture/probstat/13/image_12.png" alt="image" width="250px">

> **강의 내용**  
> 
> 그게 아니라 그럼 우리가 어떻게 해야 되느냐.  
> 디터미넌트, $n^{3}$ 해서 그거 뉴머리컬한 그런 알고리즘을 통해서 구하게 되면  
> 굉장히 에너지가 많이 들고 오래 걸리니까  
> 어떻게 하냐면 함수를 정해놓습니다.  
>
> 어떤 함수를 정해놓느냐.  
> 첫 번째, 디터미넌트가 잘 알려져 있는 함수, 펑션 패밀리.  
> 두 번째는 아까 말씀드렸던 것처럼, 모노톤한, 인버스 펑션이 존재하는  
> 그런 어떤 폼을 찾아요.  
>
> 그래서 이 PPT 같은 경우는 플레이너 플로우라고 해서  
> $z$라는 게 인풋으로 들어왔을 때  
> $w$가 파라미터고 $b$가 파라미터고 그리고 $u$도 파라미터죠.  
> $z$는 인풋이고 $h$는 여기서 논리니어리티인데  
> 보통 어떤 걸 고려하냐면 탄젠트 하이퍼볼릭, 렐루 이런 겁니다.  
>
> 그럼 이 플레이너 플로우가 뭐죠?  
> 이 모양을 보면 이게 정확하게 풀리 리니어, 풀리 커넥티드 레이어거든요.  
> 그죠?  
> 그래서 그거를 이렇게 표현한 겁니다.  
>
> 그래서 풀리 커넥티드 레이어의 모양으로, 어떤 하나의 플로우를 가정하고  
> 그렇게 됐을 때 자코비안도 구할 수 있고  
> 자코비안에 대한 디터미넌트도 구할 수 있기 때문에  
> 우리가 앞에서 change of variable을 하기 위해서 필요한 모든 것들이  
> 이미 다 세팅이 돼 있는 형태입니다.  
>
> 여기서 뉴럴 네트워크로 가정을 하면  
> nn.Parameter, 파이토치로 예제를 들자면,  
> nn.Parameter를 통해서 파라미터라이징 하는 거는  
> $w, u, b$ 이런 애들이 있는 거죠.  
> 그래서 걔네들이 학습되는 거예요.  
>
> 근데 상식적으로 딱 봤을 때  
> 얘가 굉장히 복잡해 보이지 않지 않습니까?  
> 굉장히 간단해 보이지 않습니까?  
> 그러면 플로우를 여러 번 하면은 그래서 의미가 있는 거예요.  
>
> 플로우를, 그러니까 제가 말씀드렸던 mathematical constraint 때문에  
> 어쩔 수 없이 이 싱글 플로우 하나의 펑션 자체는 너무나도 간단하게 만들 수 밖에 없는데  
> 이거를 여러 번 중첩시키면 파라미터의 어떤 개수가 계속 늘어나겠죠.  
>
> 늘어나고 그 각각의 이제 펑션들 자체가  
> 파라미터의 어떤 그 뭐랄까요, representation power를 다 갖고 있기 때문에  
> 만약에 이런 게 한 10만 개, 100만 개 있다고 한번 해봅시다.  
> 그럼 뭔가 잘 할 수 있을 것 같다는 희망이 생기지 않을까요?  
>
> 그래서 그런 관점으로  
> 이런 것들을 체이닝을 한다고 생각하시면 될 것 같아요.  

---

> (1) 변환 정의식  
> 
>   $$T_{pf}(z) = z + u\,h(w^{T}z + b)$$  
> 
> - 기본 입력 $z$ 에 작은 변화량을 더해 주는 구조이다.  
> - 이 변화량은 **스칼라 값** $h(w^{T}z + b)$ 를 계산한 뒤,  
>   이를 **벡터** $u$ 방향으로 늘려주어 적용하는 형태이다.  
> - 즉, “$z$ 를 $u$ 방향으로 밀어주는(push) 비선형 변형”이라고 볼 수 있다.  
> - $w^{T}z + b$ 는 하이퍼플레인(평면)을 정의하며, $h(\cdot)$ 는 그 평면을 기준으로  
>   어느 정도 변형을 줄지 결정한다.
>
> ---
>
> (2) Jacobian 계산식  
> 
>   $$
>   J_{T_{pf}}(z)
>   =
>   I_d + (u w^{T})\, h'(w^{T}z + b)
>   $$
> 
> - $T_{pf}$ 는 항등변환(identity) $I_d$ 에  
>   랭크-1(rank-1) 행렬 $u w^{T}$ 를 더한 구조이다.  
> - $h'(w^{T}z + b)$ 는 스칼라이므로, 전체로는 “랭크-1 업데이트(Jacobian)”가 된다.  
> - 이 구조 덕분에 행렬식(det)을 매우 효율적으로 계산할 수 있다.
>
> ---
>
> (3) 행렬식(det) 계산식  
> 
>   $$
>   \det J_{T_{pf}}(z)
>   =
>   1 + w^{T}u\, h'(w^{T} z + b)
>   $$
> 
> - 이는 **matrix determinant lemma**  
>   $\det(A + uv^{T}) = (1 + v^{T} A^{-1} u)\det(A)$  
>   를 그대로 적용한 결과이다.  
> - 여기서는 $A = I_d$ 이므로 $\det(I_d)=1$, $A^{-1} = I_d$ 가 되어  
>   계산이 매우 단순해진다.  
> - 따라서 planar flow는 “가벼운 비용으로” Jacobian determinant 를  
>   계산할 수 있게 설계된 구조이다.
>
> ---
>
> (4) 그림의 의미  
>
> - 그림의 파란 직선은 벡터 $w$ 가 정의하는 **하이퍼플레인(직선)** 을 나타낸다.  
> - 이 직선에 수직 방향이 $w$ 의 방향이며,  
>   planar flow는 이 평면 주변에서만 변형이 세게 일어난다.  
> - 점선으로 표시된 두 개의 선은 $w^{T} z + b = \text{constant}$ 형태의 레벨셋(level set)이며,  
>   flow가 해당 영역 근처에서 공간을 **뒤틀고(push)** 있다는 것을 보여준다.  
> - 화살표들은 실제로 입력 공간이 planar flow에 의해  
>   “어떤 방향으로 변형되는지” 시각적으로 표현한 것이다.  
> - 요약하면,  
>   **planar flow는 어떤 하나의 평면에 기반하여 데이터를 u 방향으로 비선형적으로 밀어서  
>   분포를 더 복잡하게 만드는 변환**이다.

---

## p24. Radial Flows (Physically-informed NN)

- $$a \in \mathbb{R}_{>0}$$, $$b \in \mathbb{R}$$, 그리고 $$z_0 \in \mathbb{R}^d$$ 가 주어졌을 때,  
  우리는 radial flow $$T_{rf} : \mathbb{R}^d \rightarrow \mathbb{R}^d$$를 다음과 같이 정의한다:

  $$
  T_{rf}(z)
  =
  z
  +
  \frac{b}{\,a + \|z - z_0\|_2\,}
  (z - z_0).
  $$

<img src="/assets/img/lecture/probstat/13/image_13.png" alt="image" width="250px">

> **강의 내용**  
> 
> 그리고 뭐 이거는 뭐 physical informed neural net이라고 해서  
> 요즘 뭐 physical AI라는 이런 뭐 peer도 많이 있는데  
> 과학자들이 이런 형태의 플로우 모델들을 가지고  
> 뭐 유체를 해석한다든지, 뭐 단백질 뭐 동력학라든지  
> 이런 것들을 많이 모델링할 때  
> 이런 radial flow 같은 걸 많이 했었습니다.  
>
> 수학적인 이유도 있어요. 뭐 직관적인 이유는 아니고.  
>
> 그래서 이런 형식의 그게 재밌게 생긴 애들도  
> flow로 표현할 수 있고  

---

## p25. Sylvester Flows (Single Layer MLP)

- 양의 정수 $m < d$,  
  $A \in \mathbb{R}^{d \times m}$,  
  $B \in \mathbb{R}^{d \times m}$,  
  $b \in \mathbb{R}^m$,  
  $h : \mathbb{R} \rightarrow \mathbb{R}$ 가 주어졌을 때,  
  Sylvester flow $T_{\text{syl}} : \mathbb{R}^d \rightarrow \mathbb{R}^d$ 를 다음과 같이 정의한다:

  $$
  T_{\text{syl}}(z)
  =
  z + A\,h(B^T z + b).
  $$

- 계산하면,

  $$
  J_{T_{\text{syl}}}(z)
  =
  I_d
  +
  A\,\mathrm{diag}\!\big(h'(B^T z + b)\big)\,B^T.
  $$

- Sylvester의 determinant 항등식(Kobyzev et al. (2020))에 의해, 다음을 얻는다:

  $$
  \det J_{T_{\text{syl}}}(z)
  =
  \det\!\left(
      I_d
      +
      A\,\mathrm{diag}\!\big(h'(B^T z + b)\big)\,B^T
  \right)
  $$
  
  $$=
  \det\!\left(
      I_m
      +
      \mathrm{diag}\!\big(h'(B^T z + b)\big)\,B^T A
  \right).
  $$

- 참고 문헌:  
  Kobyzev, I., Prince, S. J., and Brubaker, M. A. (2020).  
  *Normalizing flows: An introduction and review of current methods.*  
  IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):3964–3979.

> **강의 내용**  
> 
> 네 그래서 이제 앞에서 이 planar flow 같은 경우는  
> 이 벡터들이 이 파라미터가 고려가 되는데  
> 그게 아니라 그 벡터로 하면 벡터는 그냥 파라미터가 이래서  
> n dimensional짜리 벡터는 파라미터가 n인데  
> 이 모든 structure를 다 그대로 유지하면서  
> 이 벡터를 matrix화를 시킬 수 있는 거 아닌가  
> 라고 해서 나온 게 이제 실베스터 플로우라는 개념입니다.  
>
> 여기서 나머지는 다 똑같다고 생각해주시면 될 것 같고  
> 이 벡터가 matrix화가 된 거예요.  
>
> 근데 벡터가 n dimensional이고  
> n dimensional짜리를 이제 matrix화 시키면 n 곱하기 n이 되잖아요.  
> 그럼 n에서 n의 제곱이 되니까 파라미터가 엄청나게 많이 늘어나겠죠.  
>
> 엄청나게 많이 늘어났으니까  
> flow에서 함수들의 개수를 좀 줄일 수도 있을 것 같고  
> 그래서 실제로 아까 앞에서 말씀드린 건 예제를 위해서 말씀드린 거고  
> 우리가 affine transform의 성질을 갖고 있는 function을 고려한다고 가정을 하면  
> 보통 이런 느낌,  
> 실베스터 flow single layer MLP의 형태를 갖고 있는  
> 이런 function을 고려합니다.  
>
> 그럼 이게 항상 그래요.  
> flow 모델을 공부하다 보면  
> 그리고 제가 이제 시퀀셜 모델들을 많이 했는데  
> 이게 여러분도 막 mamba나 이런 것들 들어보셨나요?  
>
> state space 모델도 보면  
> 그 안에서도 이렇게 관점이 좀 달라요.  
> 그 mamba나 어떤 그런 state space 모델이나  
> 뭔가 시간에 따라 정보를 흘리게 한다는 개념의  
> 어떤 철학을 갖고 있는 것을 통해서, 타임 시리즈 모델링하는 모델들이 있거든요.  
>
> 근데 얘도 사실 그런 관점인 겁니다.  
> 이게 사실 MLP인데  
> 이거를 그냥 계속 concatenation 시키면 그게 neural network 아닌가요?  
>
> 그래서 그렇게 neural network가 되는데  
> MLP가 뭐 이게 8개가 있으면, 8층 짜리 MLP neural network가 되는 거죠.  
>
> mamba도 마찬가지.  
> mamba도 이런 되게 간단하게 생긴  
> 이런 정보를 흐르게 하는 어떤 function을 가정하고  
> 걔를 이제 concatenation 시켜 가지고  
> 타임 시리즈 자료를 하겠다고 해서  
> 그 결과적으로 우리가 neural network를 만드는 데 있어서  
> information flow를 시키는 어떤 그런 세부 component들을 쌓아서  
> 뭔가를 하겠다는 관점이 공유가 되는 지점들이 되게 많습니다.  
>
> 그래서 그런 것들도 제가 뒤에 또 시간이 나면  
> 이런 모델들이 이제 mamba나 이런 것들에 어떻게 영향을 끼쳤고  
> 이런 것들도 설명을 드릴 텐데  
> 뭐 말을 어렵게 했지만  
> 사실 요거가 사실 neural network를 여러 개 합쳐놓은 거나 마찬가지다.  
>
> 그렇게 생각하시면 될 것 같아요.  
> 우리가 여러 개의 function을 통해서 체이닝 한다고 가정하면.  
>
> 그래서 여기는 어떤 그 특정한 수학적인  
> 그런 것들이 이미 만족되어 있는 어떤 함수 구조를 가지고 뭔가 만드는 거고  

---

> Sylvester flow는 다층 퍼셉트론(MLP)의 단일 레이어 형태를 이용해  
> 입력 벡터 $z \in \mathbb{R}^d$ 를 변환하는 normalizing flow의 한 종류이다.
>
> ---
>
> 먼저, $m < d$ 인 양의 정수와  
> 행렬 $A \in \mathbb{R}^{d \times m}$, $B \in \mathbb{R}^{d \times m}$,  
> 그리고 편향 $b \in \mathbb{R}^m$,  
> 비선형 함수 $h : \mathbb{R} \to \mathbb{R}$ 를 정의한다.
>
> ---
>
> 그런 다음 변환
>
> $$
> T_{\text{syl}}(z) = z + A\,h(B^T z + b)
> $$
>
> 를 사용해 입력 $z$ 를 변형한다.  
> 이는 선형 변환 $B^T z + b$에 비선형 $h$를 취하고,  
> 이를 다시 행렬 $A$ 를 통해 원래 공간 $\mathbb{R}^d$로 올려서  
> 잔차 형태(residual form)로 더해주는 구조이다.
>
> ---
>
> Jacobian은
>
> $$
> J_{T_{\text{syl}}}(z)
> =
> I_d + A\,\mathrm{diag}(h'(B^T z + b))\,B^T
> $$
>
> 와 같이 계산된다.  
> 여기에서 $\mathrm{diag}(h'(\cdot))$ 는 비선형 함수의 도함수를  
> 각 성분에 대해 대각행렬로 만든 것이다.
>
> ---
>
> 그러나 일반적으로 $d \times d$ Jacobian의 행렬식을 직접 계산하는 것은 어렵다.  
> 따라서 **Sylvester의 행렬식 항등식**을 이용해 계산을 크게 단순화한다.
>
> ---
>
> Sylvester의 항등식에 따르면
>
> $$
> \det(I_d + A D B^T)
> =
> \det(I_m + D B^T A)
> $$
>
> 로 바꿀 수 있다. (여기에서 $D = \mathrm{diag}(h'(B^T z + b))$)
>
> 즉, $d \times d$ 행렬식을  
> **더 작은 크기인 $m \times m$ 행렬식**으로 변환할 수 있다.  
> ($m < d$ 이므로 계산량이 매우 감소한다.)
>
> ---
>
> 따라서 최종적으로
>
> $$
> \det J_{T_{\text{syl}}}(z)
> =
> \det\!\big(
> I_m
> +
> \mathrm{diag}(h'(B^T z + b))\,B^T A
> \big)
> $$
>
> 와 같이 계산된다.
>
> ---
>
> 이 구조 덕분에 Sylvester flow는  
> **표현력은 충분히 유지하면서도 Jacobian 행렬식 계산을 효율적으로 수행**할 수 있어  
> normalizing flow 모델에서 매우 유용하게 쓰인다.

---

## p26. Generic Neural Networks

- 정규화 흐름(normalizing flows)의 결합 레이어(coupling layers)는  
  신경망(neural networks)을 사용하여 모델링될 수 있다.

- 일반적으로, 신경망은 가역(invertible)이 아니다.  
  그러나, 가역성은 종종 네트워크가 전단사(bijective)임을 보임으로써 보장된다.

**Lemma (보조정리)**

만약 $NN(\cdot) : \mathbb{R} \rightarrow \mathbb{R}$이,  
다층 퍼셉트론(multilayer perceptron)이고,  
모든 가중치가 양수이며,  
모든 활성화 함수가 엄격히 단조(strictly monotone)라면,  
$NN(\cdot)$ 은 엄격히 단조 함수이다.

**가역성을 강제로 만족시키는 다른 방법들:**

- $NN(\cdot) : \mathbb{R} \rightarrow \mathbb{R}_{>0}$이 되도록 강제하고  
  그것을 적분(integrate)한다.

- $NN(\cdot) : \mathbb{R} \rightarrow \mathbb{R}$이 볼록(convex)이 되도록 강제하고  
  그것을 미분(differentiate)한다  
  (입력에 대해 볼록한 신경망 - input convex neural networks).

> **강의 내용**  
> 
> 그래서 generic한 neural network을 생각을 해보자는 거죠.  
> 그래서 이건 약간 이론적인 부분인데  
> 만약에 우리가 general한 neural network을 통해 가지고  
> 체이닝을 하고 싶다고 한다면 밑에 두 가지를 만족을 해야 됩니다.  
>
> 첫 번째 어떤 force.  
> 그러니까 이것도 되게 전문 용어인데 이제는 좀 간략하게 말씀드리자면  
> convex하다는 개념으로, 이제 좀 하나를 더 mathematical constraint를 더 봅니다.  
>
> 그래서 이게 우리가 일반적인,  
> 그러니까 앞에서는 monotone하기만 하면 inverse function이 고려가 되는데  
> 여기서 이제 convex하다는 거를 다시 가는 거예요.  
>
> 그래서 우리가 최종적으로 목표로 하는  
> 어떤 가장 큰 개념의 이제 상위 개념이 이게 이제 invertible하다라는 건데  
> 이거를 하기 위해서 이제 우리가 어떻게 했었죠?  
> monotone한 거를 찾았어요.  
>
> 근데 monotone하다는 거에 조금 더 상위 개념,  
> 조금 더 상위 개념이고 만족시키기 어려운 게 convex라는 개념입니다.  
>
> 그래서 모든 convex function이 invertible하냐 하면 invertible하게 만들 수 있고  
> 모든 monotone function이 invertible하냐 하면 그렇게 할 수 있습니다.  
>
> 그래서 이런 어떤 그런 집합 관계에 있는 수학적인 구조체를 찾아서  
> 굉장히 일반적인 neural network가  
> 앞에서와 같이 정해져 있는 form이 아니라  
> 일반적인 neural network를 가지고  
> 이제 flow model을 만드는 그런 시도도 많이 합니다.  
>
> 앞에서 봤던 이 x제곱, 이 convex랑 조금 다른데  
> 그냥 이건 더 디테일하게 다루면 어려우니까  
> 그냥 그렇게 이해를 해주시면 될 것 같아요.  
>
> 키워드만 가져가시면 될 것 같습니다.  
>
> 그래서 그런 것들이  
> input convex neural network라고 합니다.  
> 이런 convex하다는 것들이 만족되는 neural network항.  
>
> 그래서 invertible한 것들.  

---

> 첫 번째 방법:  
> $NN(\cdot) : \mathbb{R} \rightarrow \mathbb{R}_{>0}$ 이 되도록 강제하고, 그것을 적분한다.  
>  
> ---
>
> > 핵심 아이디어:  
> > NN의 출력이 항상 양수라는 것은 NN이 “입력이 증가하면 기울기가 절대로 음수가 되지 않는”  
> > 형태의 함수를 만들 수 있다는 뜻이다.  
> >
> > NN(x) > 0 이므로, 다음과 같은 함수를 정의할 수 있다:  
> >  
> > $$
> > F(x) = \int NN(t)\, dt
> > $$
> >  
> > 이때 F(x)는 입력이 증가할수록 항상 증가하는 **단조 증가(strictly monotone)** 함수가 된다.  
> >  
> > 단조 증가 함수는 서로 다른 입력이 같은 출력을 만들 수 없으므로 가역적이다.  
> >
> > 즉,  
> > **양수 출력 NN → 적분 → 단조 증가 함수 → 가역성 확보**.  
>
> ---
>
> 두 번째 방법:  
> $NN(\cdot) : \mathbb{R} \rightarrow \mathbb{R}$ 이 볼록(convex)이 되도록 강제하고, 그것을 미분한다.  
>
> ---
>
> > 핵심 아이디어:  
> > 볼록 함수 $\phi(x)$ 는 미분했을 때 $\phi'(x)$ 가 항상 **단조 증가**한다.  
> > 즉, 입력 x가 증가하면 기울기 또한 절대로 감소하지 않는다.  
> >
> > 그래서 NN을 “입력에 대해 볼록한 신경망(input convex neural network)”으로 만들었을 때,  
> > NN을 $\phi(x)$ 라고 보고,  
> > 그 도함수  
> >  
> > $$
> > \phi'(x)
> > $$
> >  
> > 를 사용하면 자동으로 단조 증가하는 함수가 된다.
> >
> > 단조 증가 함수는 가역성을 만족한다.
> >
> > 즉,
> > **볼록 NN → 미분 → 단조 증가 함수 → 가역성 확보**.

---

## p27. Input Convex Neural Network

- PICNN $$f(x, y; \theta)$$는 두 개의 은닉층 집합 $$\{u_i\}_{i=1}^k$$와 $$\{z_i\}_{i=1}^k$$에 의해  
  기술되며, 각각 $x$-경로(x-path)와 $y$-경로(y-path)에 대응한다.  

- $K$-층 PICNN의 구조는 다음과 같은 반복되는 은닉 유닛들에 의해 주어진다:  

  $$
  \begin{aligned}
  u_{i+1}
  &=
  \tilde{g}_i\!\left(\tilde{W}_i u_i + \tilde{b}_i\right)
  \\[10pt]
  z_{i+1}
  &=
  g_i\Big(
        W_i^{(z)}
        \big(
            z_i \circ [ W_i^{(zu)} u_i + b_i^{(zu)} ]
        \big)
        +
        W_i^{(y)}
        \big(
            y \circ [ W_i^{(yu)} u_i + b_i^{(yu)} ]
        \big)
        +
        W_i^{(u)} u_i
        +
        b_i
    \Big)
  \end{aligned}
  $$

  여기서 $\tilde{g}_i$ 와 $g_i$ 는 비선형 활성화 함수들이다.  
  PICNN의 최종 스칼라 값 출력은  

  $$
  f(x, y; \theta) = z_K
  $$

  이다.  

> **강의 내용**  
> 
> 그래서 여러 가지로 고려를 할 수 있는데 예를 들어서 이런 표현상 있을 수 있다.
> 디테일은 생략하도록 하겠습니다.  

---

> PICNN이 $x$에 대해 볼록(convex)이 되는 핵심 이유는  
> 네트워크 구조 전체가 “볼록성을 보존하는 연산들”로만 구성되기 때문이다.  
>   
> 첫 번째로, $u$-경로는  
> $$u_{i+1} = \tilde{g}_i(\tilde{W}_i u_i + \tilde{b}_i)$$  
> 의 형태인데, 여기서 $$\tilde{W}_i$$ 는 비음수(nonnegative) 행렬로 제한된다.  
> 비음수 행렬과 단조 증가 활성화 함수 $$\tilde{g}_i$$ 는  
> 볼록성을 깨지 않는 연산이다.  
>   
> 또한 매우 중요한 점은, 비록 위 식이 $u_i$만을 포함하는 것처럼 보이지만  
> 실제로는 $$u_0 = x$$ 에서 시작해  
> $$u_1 = \tilde{g}_0(\tilde{W}_0 x + \tilde{b}_0), \quad u_2 = \tilde{g}_1(\tilde{W}_1 u_1 + \tilde{b}_1)$$  
> 와 같이 모든 $u_i$ 가 본질적으로 $x$의 함수라는 것이다.  
> 즉, $u_i = u_i(x)$ 이므로 $u$-경로는 $x$와 직접적 관계가 있으며,  
> 그 업데이트 규칙이 볼록성을 보존하는 구조이기 때문에  
> $u_{i+1}$ 역시 $x$에 대해 볼록이 된다.  
> 이는  
> “볼록 함수에 비음수 조합 + 단조 활성화 = 여전히 볼록”  
> 이라는 볼록성 보존 법칙에서 비롯된다.  
>   
> 두 번째로, $z$-경로는  
> $$
> z_{i+1}
> =
> g_i\!\Big(
>       W_i^{(z)}(z_i \circ [ W_i^{(zu)}u_i + b_i^{(zu)} ])
>       +
>       W_i^{(y)}(y \circ [ W_i^{(yu)}u_i + b_i^{(yu)} ])
>       +
>       W_i^{(u)}u_i
>       +
>       b_i
> \Big)
> $$  
> 와 같은 구조를 갖는다.  
>   
> 여기서 중요한 점은 $W_i^{(u)}$, $W_i^{(zu)}$, $W_i^{(yu)}$ 등이  
> 모두 **비음수 행렬로 제한**된다는 사실이다.  
> 비음수 조합은 볼록성을 보존하며,  
> $u_i$ 자체가 이미 $x$에 대해 볼록이므로  
> $u_i$ 를 포함하는 모든 항들이 여전히 $x$에 대해 볼록이다.  
>   
> 또한 Hadamard 곱(원소별 곱) $a \circ b$ 는  
> 두 항 중 하나가 비음수일 경우 볼록성을 보존하는 연산이며,  
> 이 조건 또한 네트워크 설계에서 보장된다.  
> 따라서  
> $z_i \circ [ W_i^{(zu)} u_i + b_i^{(zu)} ]$  
> 역시 볼록성을 유지한다.  
>   
> 세 번째로, 마지막에는 단조 증가 활성화 함수 $g_i$ 를 적용한다.  
> 단조 증가 함수는 볼록함수의 볼록성을 절대 깨지 않는다.  
> 즉, $g_i(\cdot)$ 의 출력도 볼록이다.  
>   
> 이런 식으로 각 계층에서  
> “볼록성 보존 → 볼록성 보존 → 볼록성 보존”  
> 구조가 반복되므로,  
> $z_K$ 는 $x$에 대해 확실하게 볼록(convex)하게 된다.  
>   
> PICNN이 본질적으로 convexity를 강제할 수 있는 이유는  
> **모든 경로가 볼록성을 파괴할 수 있는 연산(음수 가중치, 비단조 활성화 등)을 제거하고,  
> 볼록성을 보존하는 연산만으로 전체 네트워크를 구성했기 때문**이다.  
>   
> 요약하면:  
>   
> - 비음수 가중치 → 볼록 조합  
> - 단조 증가 활성화 → 볼록성 유지  
> - Hadamard 곱의 구조적 제약 → 볼록성 유지  
> - $u$ 경로가 실제로는 $x$로부터 시작하는 연쇄 구조 → $x$에 대한 볼록성 유지  
> - $z$ 경로 역시 볼록성을 보존하는 연산으로 구성  
>   
> 결과적으로  
> $$f(x, y; \theta) = z_K$$  
> 는 $x$에 대해 볼록함수(convex function)가 된다.  
 
---

## p28. Input Convex Neural Network

- **명제(Proposition) 1, Amos et al. (2017)**    
  모든 $\{ W_i^{(z)} \}_{i=1}^{k-1}$ 이 비음수(non-negative)이고,  
  모든 함수 $\tilde{g}_i$ 가 볼록(convex)이며  
  단조 증가(non-decreasing)일 때,  
  함수 $f$ 는 $y$ 에 대해 볼록(convex)이다.

- 증명은  
  볼록 함수들의 **볼록 조합(convex combinations)** 과  
  볼록 함수들의 **합성(compositions)** 역시 볼록이라는  
  단순한 아이디어를 따른다.

- $f^j$ 가 $x_j$ 에 대한 볼록 함수가 되도록 보장하려면,  
  $W^{(z)}$ 의 모든 항목이 비음수(non-negative)가 되도록 제약해야 한다.

- 학습 과정은 (3)번 식의 $KL(\theta)$가 최소화되도록  
  파라미터 $\theta$ 를 최적화한다.

- 정규화 항(regularization term)은  
  모든 층(layer)의 모든 $W^{(z)}$ 가중치 행렬 항목이  
  양수(positive)가 되도록 보장하기 위해 추가되며,  
  다음과 같은 식을 갖는다:

  $$
  -\frac{1}{n}
  \sum_{i=1}^{n}
  \Bigg(
      \log g \circ T(\theta)(x_i)
      +
      \sum_{j=1}^{d}
      \log(\nabla T(\theta))_j(x_i)
  \Bigg)
  +
  \lambda
  \sum_{j=1}^{d}
  \sum_{k=1}^{K}
  \left\|
      \max\!\big(-(W_j^{(z)})_k,\, 0\big)
  \right\|_{F}^{2}.
  $$

> **강의 내용**  
> 
> 마찬가지로 디테일은 생략하고, 되게 복잡해집니다.  
>
> 예를 들어서 $w_i$가 이제 neural network parameter인데  
> 이게 non-negative 해야 되고, 그러면 convex하고 non-declining 해줘가지고  
> monotonity를 만족하고, 그것까지 invertible하고  
> 되게 복잡해져요.  
>
> 하여튼 점점점 이제 우리가 function 패밀리를 크게 잡으면 잡을수록  
> 수학적 가정 조건을 더 정밀하게 되는 최신 연구들이 있다.  
> 그렇게 이해해주시면 될 것 같아요.  
>
> 그걸 통해 가지고  
> 이제 log likelihood를 maximize하는  
> loss function이 밑에와 같이 표현이 됩니다.  
>
> 이거는 뭐 아실... 외우시고 뭐 이해하실 필요는 없는데  
> 되게 복잡해진다.  
> 이 개념만 이해하시면 될 것 같아요.  

---

> 위 식은 두 부분으로 구성되어 있다.  
>   
> 첫 번째 부분  
> $$-\frac{1}{n}\sum_{i=1}^{n}\Big(\log g\!\circ\!T(\theta)(x_i)+\sum_{j=1}^{d}\log(\nabla T(\theta))_j(x_i)\Big)$$  
> 는 normalizing flow 학습에서 사용하는 **기본 로그-우도(log-likelihood) 최대화 항**이다.  
> 이는 변환 $T(\theta)$ 를 적용한 뒤 얻어지는 분포가  
> 데이터에 잘 맞도록 만드는 역할을 한다.  
>   
> 두 번째 부분  
> $$\lambda\sum_{j=1}^{d}\sum_{k=1}^{K}\left\|\max(-(W_j^{(z)})_k,0)\right\|_F^2$$  
> 는 **정규화 항(regularization term)** 으로,  
> $W^{(z)}$ 의 항목이 **음수가 되지 않도록 강제**하는 역할을 한다.  
>   
> 여기서 $\max(-(W_j^{(z)})_k, 0)$ 는  
> $W^{(z)}$ 의 요소가 음수일 경우 그 절댓값을 반환하고,  
> 양수일 경우 0을 반환한다.  
> 따라서 이 항을 제곱해 패널티로 더하면  
> **음수 가중치가 등장할 때에만 비용이 증가**하게 된다.  
>   
> 즉, 이 항은  
> “$W^{(z)}$ 의 모든 요소가 비음수(non-negative)가 되도록 만드는 페널티”  
> 로 작동하며, 이것이 PICNN의 **볼록성(convexity) 보장을 위한 핵심 제약**이다.  
>   
> 최종적으로 이 정규화 항을 포함시키면  
> 학습 과정 동안 $W^{(z)}$ 의 값이 음수가 되지 않도록 유지되어  
> 네트워크의 볼록성과 가역성(invertibility) 조건을 지속적으로 만족하게 된다.  

---

> **강의 마무리**  
> 
> 여기까지가 오늘 어떤 전체적인 내용이라고 봐주시면 될 것 같고요.  
> 그 직관은 전달됐다라고 생각을 합니다.  
> 디퓨젼 모델이 사실 더 어렵거든요. 이것보다.  
> 더 어렵고 지금 이 normalizing flow에서의 직관을 많이 전달드리려고 하고  
> 메시지를 많이 전달드리려고 했는데  
> 그런 관점에서 좀 이해를 잘 하셔야 될 것 같아요.  
> 그래야 다음 이제 마지막 저희 그 컨텐츠인 디퓨젼 모델 DDPM에 대해서  
> 더 잘 이해를 하실 수 있을 것 같습니다.  
> 거기서는 수학적인 얘기도 많이 나오거든요.  
> 그때는 이런 직관을 다 갖고 있다고 가정을 하고  
> 수학적인 것도 다 다루겠습니다.  
> 디퓨젼 모델이 제일 중요한 것이기 때문에.  
>
> 이 앞에서 모델들은  
> 제가 말씀드렸던 것처럼 flow based model 같은 경우는  
> 어떤 특별한 purpose들이 있는데, 쓰는 경우들.  
> 뭐 optimal test, AI process 이런 것들에서 잠깐 쓰는데  
> GAN이랑 VAE는 머신러닝에서 없다고 생각하시면 됩니다. 안 씁니다.  
> 그럼에도 우리가 역사를 알아야 할 때에도 그런 걸 하는 건데  
> 디퓨젼 모델이 완전 뉴노멀이고  
> 완전한 이제 이것도 이제 그룹 간의 헤게모니 싸움에서 또 졌어요. 학문에서도.  
> 그래서 완전히 이제 이쪽으로 다 치우쳐져 있기 때문에  
> 코드 저희가 이제 공부를 하잖아요.  
>
> 다음 주에 GAN이랑 이제 normalizing flow 코드 실습을 할 텐데  
>   
> 그다음에 이제 디퓨젼 모델은 여러분들이 완벽하게 이해를 하신다고 가정을 하고  
> 에너지를 많이 써서 공부를 하신다고 봐주시면 될 것 같아요.  
>
> 네.  
> 오늘은 좀 이 정도까지가 메인 콘텐츠고 질문 있으시면.  
>
> (질문)  
> 15페이지에서 보면 mu하고 p가 왼쪽이라고 되어 있거든요.  
> 그런데 아까 p가 가우시안이라는 설명하셨는데  
> 그 p하고 여기 p하고 서로 다른 p일까요?  
>
> (답변)  
> 제가 거꾸로 말씀드린 것 같아요.  
> mu랑 p가 데이터.  
> nu랑 q가 레퍼런스니까.  
>
> (질문)  
> 그러니까 t샵은 그 데이터 샘플 스페이스에서  
> 가우시안으로 옮기는 거죠.  
>
> (답변)  
> 네.  
> 맞습니다.  
> 제가 거꾸로 말했습니다.  
> 질문 감사합니다.  
>
> 또 질문 사항 있으실까요?  
> 네.  
> 감사합니다.  
> 1분만 더 있다가 강의 마무리하도록 하겠습니다.  
>
> (질문)  
> 아까 잘 못 들었는데 23페이지에서 ... (멀어서 잘 안 들림)  
>
> (답변)  
> 여기 어떤 뜻이냐면 정확하게는  
> 이제 이거에 이제 벡터로 표현이 되어있을 때  
> 이거에 이제 매트릭스화로 되어 있는 게 실베스터 플로우고  
> 얘가 이제 싱글레이어 mlp라고 되어 있는데  
> 여기 mlp 구조랑 똑같이 생긴 거예요. 실제로.  
>
> b 곱하기 z 플러스 b 되어 있는 게  
> 우리가 뉴럴 네트워크 파이토치 같은 코드를 열어보다 보면  
> 이렇게 생겼습니다. 실제로.  
> 그리고 h는 우리가 앞에서 CNN 공부할 때도  
> 레이어를 이렇게 그 파라미터 레이어로,  
> 파라미터가 들어있는 레이어에서 컨볼루션 레이어 같은 것들을  
> 통과한 다음에 우리가 뭘 했죠?  
> relu나 이런 걸 통해서 이제 한번 바꿔주잖아요.  
> 그 relu 같은 역할을 h라고 보시면 될 것 같아요.  
>
> 네.  
> 그다음에 이게 왜 레이어가 하나 더 있는 거냐라고 하면은  
> 그 아웃풋에다가 곱하기 a를 한 다음에 또 z를 더했습니다.  
> 그러니까 또 한번 뭔가 매니플레이션을 한 거거든요.  
> 그래서 이거를 이제 싱글레이어 mlp라고 이제 표현을 한 거죠.  
>
> (질문)  
> 지금 이게 T랑 T의 역함수를 찾는 거라고 저는 이해를 했는데.  
> 어떻게 보면은 인코더가 T고 디코더가 T의 역함수  
> 인코더의 역함수라고 생각을 할 수 있을 것 같은데.  
> 인코더와 디코더를 넣는 거는 성립하지 않나요?  
>
> (답변)  
> 자, 인코더, 디코더는 아주 좋은 말씀이신데요.  
> 어떤 추상적 역할 입장에서는 맞는데  
> 이 mathematical constraint가 만족을 하느냐.  
> 그러니까 결국 우리가 change of variable를 해야 되거든요.  
> 그 change of variable를 하기 위해서의 관점에서  
> 실제로 계산을 해보면 아무것도 성립을 안 합니다.  
>
> 결과적으로 이제 여기 이 그림으로 보면(p7),  
> 이 f라는 게 이제 디코더가 되는 거고  
> f 인버스가 인코더가 되잖아요.  
> 그냥 우리가 인코더 디코더에서 스트럭처 VAE를 학습을 하고  
> 그 VAE를 인코더 디코더를 f랑 f 인버스에 딱 넣어가지고  
> 이 change of variable을 하잖아요.  
> 그럼 아무것도 만족을 안 해요.  
>
> 그러니까 수학적 constraint가 더 세다는 거죠.  
> 그거는 이제 우리가 VAE를 통해서  
> VAE의 어떤 오브젝티브 펑션을 통해서 학습했었던 방법이  
> 이 change of variable과 호환되는 형태는 아니었기 때문에 그런 겁니다.  
> 목표는 같았지만, 역할은 같지만  
> 추가적인 수학적 구조가 들어갔기 때문에  
> 바로 치환시킬 수는 없었다.  
> 근데 역할은 같다.  
>
> (질문)  
> 인코더랑 디코더가 모노톤하지 않아서 그렇다는 말씀이실까요?  
>
> (답변)  
> 네, 그것뿐만 아니라요.  
> 그거는 어떻게 보면 뭐라고 얘기할까  
> 인버스 펑션의 개념은 아닙니다.  
> 인코더랑 디코더는.  
> 그냥 뉴럴 네트워크 구조가 미러드 이미지라는 거지  
> 수학적으로 봤을 때 인코더랑 디코더 했을 때  
> 그러니까 같아져야 되는 요건 아닙니다.  
>
> (질문)  
> 그럼 어떻게 보면 VAE는  
> 인코더 디코더가 별도의 학습되는 별도의 파라미터고  
> normalizing flow는 한 개의 파라미터가  
> T 역할도 하고 T 인버스 역할도 한다.  
> 이렇게 보면 될까요?  
>
> (답변)  
> 네, 그렇게 생각하시면 될 것 같아요.  
> 이게 되게 오묘합니다.  
> 그래서 상상하기가 힘들 수도 있는데  
> 여하튼 제가 단호하게 말씀드릴 수 있는 거는  
> 바로 넣는다고 되는 건 아니에요.  
> 네.  
> 조금 차이는 있어요.  