---
layout: post
title: "SVD and Low-Rank Matrix Approximations"
date: 2025-09-30 11:00:00 +0900
categories: ["수학"]
tags: []
---

> 출처: Tim Roughgarden & Gregory Valiant, *CS168: The Modern Algorithmic Toolbox*, “Lecture #9: The Singular Value Decomposition (SVD) and Low-Rank Matrix Approximations,” Stanford, April 30, 2024.  
> 원문: https://web.stanford.edu/class/cs168/l/l9.pdf

CS168: 현대 알고리즘 툴 박스  
강의 #9: 특이값 분해 (SVD, The Singular Value Decomposition)  
그리고 저순위 행렬 근사 (Low-Rank Matrix Approximations)  

Tim Roughgarden & Gregory Valiant∗  
2024년 4월 30일

## 1. 누락된 항목들은 무엇인가?  

여기 퀴즈가 있다: 다음과 같은 5×3 행렬을 생각해보라. 7개의 항목은 보이고, 8개의 항목은 누락되어 있다:  

$$
\begin{bmatrix}
7 & ? & ? \\
? & 8 & ? \\
? & 12 & 6 \\
? & ? & 2 \\
21 & 6 & ? \\
\end{bmatrix}
$$

누락된 항목들은 무엇일까?  

이 행렬 완성 문제는 좀 불공평해 보이지 않는가?  
결국, 각 미지수 항목은 무엇이든 될 수 있고, 그것들이 무엇인지 알 방법이 없다.  

하지만 만약 내가 “완전한 행렬이 멋진 구조(nice structure)를 가진다”라는 추가 힌트를 준다면 어떨까?  

이것은 여러 가지 의미일 수 있지만, 예시에서는 극단적인 가정을 사용해보자:  
**모든 행이 서로의 배수(multiples of each other)** 라는 것이다.

이제 모든 누락된 항목들을 복원할 수 있다!  

예를 들어, 세 번째 행이 두 번째 행의 배수라면, 후자의 각 항목은 전자의 해당 항목의 $\tfrac{3}{2}$ 배가 되어야 한다  
(중간 열의 “12”와 “8” 때문).  

따라서 두 번째 행의 세 번째 항목은 “4”여야 한다는 결론을 내릴 수 있다.  
비슷하게, 네 번째 행의 가운데 항목은 “4,” 마지막 행의 마지막 항목은 “3”이 된다.  
그리고 계속해서 계산하면 다음과 같은 완전한 행렬을 얻을 수 있다:  

$$
\begin{bmatrix}
7 & 2 & 1 \\
28 & 8 & 4 \\
42 & 12 & 6 \\
14 & 4 & 2 \\
21 & 6 & 3 \\
\end{bmatrix}
\tag{1}
$$

이 예시의 요점은, 어떤 부분적으로만 알려진 행렬의 **“구조(structure)”** 에 대해 무언가 알고 있다면,  
때때로 모든 “잃어버린(lost)” 정보를 복원할 수 있다는 것이다.  

물론, 모든 행이 서로의 배수라는 가정은 꽤 극단적이다.  
그렇다면 일반적으로 “행렬의 구조(matrix structure)”란 무엇을 의미할까?  

다음 절에서 설명할 자연스럽고 유용한 정의는 **저순위(low rank)** 를 가지는 것이다.

## 2. 행렬의 랭크(Matrix Rank)  

아마 이전 과정에서 행렬의 **랭크(rank)** 개념을 본 적이 있을 것이다.  
여기서 관련 개념들을 다시 한 번 되짚어 보자.  

### Rank-0 행렬  
주어진 크기에서 **랭크가 0인 행렬**은 단 하나뿐이며, 바로 **모든 원소가 0인 행렬(all-zero matrix)** 이다.  

### Rank-1 행렬  
**랭크가 1인 행렬**은 위의 예시에서 가정한 것처럼, 모든 행이 서로의 (정수일 필요는 없는) 배수인 **0이 아닌 행렬**이다.  
예시 (1)에서 모든 열 역시 서로의 배수인데, 이는 우연이 아니다.  

랭크-1의 $m \times n$ 행렬은 다음과 같이 정의할 수도 있다:  
$m$차원 벡터 $u$와 $n$차원 벡터 $v$의 **외적(outer product)** 으로 표현된다.  

<img src="/assets/img/math/svd/image_1.png" alt="image" width="500px">

여기서, 각 **행(row)** 은 $v^{\top}$ 의 배수이고, 각 **열(column)** 은 $u$의 배수이다.  

---

> **참고:** 이는 **내적(inner product, dot product)** 과 대비된다.  
> 내적은 다음과 같이 정의된다:  
>
>$$
>u^{\top} v = \sum_{i=1}^n u_i v_i,
>$$  
>
>이는 두 벡터에 대해서만 정의된다.

---

### Rank-2 행렬  
**랭크가 2인 행렬**은 단순히 두 개의 랭크-1 행렬의 **중첩(superposition, 합)** 이다.  

<img src="/assets/img/math/svd/image_2.png" alt="image" width="720px">

---

<img src="/assets/img/math/svd/image_3.png" alt="image" width="600px">  

그림 1: 랭크가 $k$인 임의의 행렬 **A**는 하나는 길고 가느다란 행렬, 다른 하나는 짧고 긴 행렬의 곱으로 분해될 수 있다.  

---

식 (2)의 등식을 점검하고 내재화하는 데 시간을 들일 가치가 있다.  

단, 조금 더 정확히 말하면: 랭크-2 행렬은 두 개의 랭크-1 행렬의 합으로 쓸 수 있는 행렬이며,  
자체적으로는 랭크-0이나 랭크-1 행렬이 아니다.

### Rank-k 행렬  

이제 일반적인 **행렬의 랭크(rank)** 정의가 명확해졌을 것이다.  
행렬 **A**가 랭크 $k$라는 것은, **A**가 $k$개의 랭크-1 행렬의 합으로 표현될 수 있으며,  
$k-1$개 이하의 랭크-1 행렬의 합으로는 표현될 수 없다는 것을 의미한다.  

행렬 곱셈 관점에서 다시 표현하면,  
**A**는 **길고 가느다란 $m \times k$ 행렬 Y**와 **짧고 긴 $k \times n$ 행렬 $Z^{\top}$**의 곱으로  
분해될 수 있다는 것이다 (그림 1).  
(그리고 **A**는 $m \times (k-1)$ 행렬과 $(k-1) \times n$ 행렬의 곱으로는 분해될 수 없다.)  

행렬 **A**의 랭크에는 여러 가지 동치 정의가 있다.  
다음의 두 조건은 서로 동치이며, 또한 위 정의와도 동치이다  
(즉, 셋 중 하나가 성립하면 나머지 두 조건도 성립한다):  

1. **A의 열(column)들 중에서 가장 큰 선형독립(linearly independent) 부분집합의 크기가 $k$이다.**  
   즉, **A**의 모든 $n$개의 열은 그 중 $k$개의 열의 선형결합으로 표현될 수 있다.  

2. **A의 행(row)들 중에서 가장 큰 선형독립 부분집합의 크기가 $k$이다.**  
   즉, **A**의 모든 $m$개의 행은 그 중 $k$개의 행의 선형결합으로 표현될 수 있다.  

첫 번째 조건이 두 번째와 세 번째 조건을 함의한다는 것은 명백하다.  
만약 $A = YZ^{\top}$라면,  
- **A**의 모든 열은 $Y$의 $k$개의 열의 선형결합으로 표현될 수 있고,  
- **A**의 모든 행은 $Z^{\top}$의 $k$개의 행의 선형결합으로 표현될 수 있다.  

다른 함의들에 대한 증명은 선형대수학 과정에서 다루었을 것이다  
(그리 어렵지 않다).

## 3. 저순위 행렬 근사(Low-Rank Matrix Approximations): 동기  

이 강의의 주요 목표는, 주어진 행렬 **A**를 목표 랭크 $k$에 맞는 **랭크-$k$ 행렬**로  
가장 “좋은(best)” 방식으로 근사하는 방법을 찾는 것이다.  

이러한 행렬을 **저순위 근사(low-rank approximation)** 라고 부른다.  

그렇다면, 왜 이런 일을 하고 싶을까?

### 1. **압축(Compression).**  

저순위 근사는 행렬의 **(손실 있는) 압축된 버전**을 제공한다.  

원래 행렬 **A**는 $mn$개의 수로 표현되지만,  
**Y**와 $Z^{\top}$를 기술하는 데에는 오직 $k(m+n)$개의 수만 필요하다.  
$k$가 $m$과 $n$에 비해 작을 때, 곱 $m \times n$을 합 $m+n$으로 대체하는 것은 큰 이득이다.  

예를 들어, **A**가 이미지를 나타낸다고 하자 (원소 = 픽셀 강도).  
이 경우 $m$과 $n$은 보통 수백 단위이다.  
다른 응용에서는 $m$과 $n$이 수만 개 이상이 될 수도 있다.  

이미지의 경우, $k=100$ 또는 $150$ 정도의 비교적 작은 값으로도  
원본 이미지와 매우 흡사하게 보이는 근사를 얻을 수 있다.  

따라서 저순위 근사는 **벡터에 대한 차원 축소(dimensionality reduction)** 개념의  
행렬 버전에 해당한다 (강의 #4와 #7 참고).

### 2. **거대한 ML 모델 업데이트(Updating Huge ML Models).**  

저순위 행렬 근사의 매우 현대적인 응용 중 하나는 거대한 모델의 **미세 조정(fine-tuning)** 에 있다.  

대규모 언어 모델(LLMs)의 경우, 보통 수십억(또는 그 이상)의 파라미터를 가진  
사전 학습된(off-the-shelf) 거대한 모델이 존재한다.  
이 모델은 방대한 양의 일반적인 말뭉치(웹 텍스트 등)로 학습되어 있다.  

이후 보통 **미세 조정(fine-tuning)** 과정을 수행한다.  
이 단계는 특정 도메인 데이터셋(예: 고객 서비스 대화, 교육 포럼 질의응답, 의학 보고서 등)으로  
두 번째 라운드의 학습을 진행하는 것이다.  
이 데이터셋은 일반적으로 크기가 상당히 작다.  

그러나 미세 조정의 어려움은, 이렇게 거대한 모델을 업데이트하는 데  
계산 비용이 엄청나게 많이 든다는 점이다.  

2021년 논문 **LoRA: Low-Rank Adaptation of Large Language Models [1]** 은  
다음과 같은 주장을 한다:  

1. 미세 조정 과정에서의 업데이트는 일반적으로 저순위에 가깝다.  
2. 따라서 이러한 업데이트를 원래 모델에 대해 **분해된 형태(factorized form)** 로 명시적으로 학습할 수 있다.  

즉, **파라미터 수를 1000배~10,000배 줄인 모델을 학습**하는 것과 거의 같다는 의미다!!!  

관심이 있다면, 원 논문이나 이를 다룬 수많은 블로그 글들을 참고해 보라.

### 3. **노이즈 제거(De-noising).**  

만약 **A**가 어떤 “실제 신호(ground truth)”의 잡음(noise)이 섞인 버전이고,  
그 실제 신호가 대략적으로 저순위라면,  

원시 데이터 **A**를 저순위 근사로 변환하는 과정에서  
많은 **노이즈는 제거**되고, **신호는 거의 손실되지 않게** 된다.  

그 결과, 원래 데이터보다 오히려 더 **정보량이 풍부한 행렬**을 얻을 수 있다.

### 4. **행렬 완성(Matrix Completion).**  

저순위 근사는 1절에서 소개한 **행렬 완성(matrix completion)** 문제에 대한  
초기 접근법(first-cut approach)을 제공한다.  
(9주차에서는 더 발전된 기법들을 살펴볼 것이다.)  

누락된 항목이 있는 행렬 **A**가 주어졌을 때,  
첫 번째 단계는 누락된 항목들을 “기본(default)” 값으로 채워 넣어  
완전한 행렬 $\hat{A}$를 얻는 것이다.  

이 기본값이 무엇이어야 하는지는 시행착오가 필요하며,  
방법의 성공 여부는 종종 이 선택에 크게 의존한다.  

기본값으로 자연스럽게 시도해볼 수 있는 것들은 다음과 같다:  
- 0  
- 같은 열(column)에 있는 알려진 항목들의 평균  
- 같은 행(row)에 있는 알려진 항목들의 평균  
- 행렬 전체에서 알려진 항목들의 평균  

두 번째 단계는 $\hat{A}$에 대해 **최적의 랭크-$k$ 근사**를 계산하는 것이다.  

이 접근법은 **미지의 행렬이 랭크-$k$ 행렬에 가깝고**,  
**누락된 항목의 수가 많지 않을 때** 비교적 잘 작동한다.

---

행렬 **A**의 랭크-$k$ 근사를 계산하기 위한 고수준 계획은 다음과 같다:  

1. **A**를 그 “구성 요소(ingredients)”들의 목록으로 표현하되,  
   이를 “중요도(importance)” 순으로 정렬한다.  
2. 그중 가장 중요한 $k$개의 구성 요소만 유지한다.  

비자명한 단계인 (1)은, 다음 절에서 다룰 일반적인 행렬 연산인  
**특이값 분해(Singular Value Decomposition, SVD)** 를 통해 쉽게 해결된다.

## 4. 특이값 분해(The Singular Value Decomposition, SVD)  

### 4.1 정의(Definitions)  

우선 공식적인 정의부터 시작한 후, 해석, 응용, 그리고 이전 강의의 개념들과의 연결을 논의할 것이다.  

$m \times n$ 행렬 **A**의 **특이값 분해(SVD)** 는 다음과 같이 세 개의 “단순한” 행렬의 곱으로 표현된다:  

$$
A = USV^{\top}, \tag{3}
$$

여기서:  

1. **U**는 $m \times m$ 직교 행렬이다.  
2. **V**는 $n \times n$ 직교 행렬이다.  
3. **S**는 $m \times n$ 대각 행렬(diagonal matrix)이며, 대각 성분들은 음이 아닌 값이고,  
   “좌상단에서 우하단 방향(북서쪽에서 남동쪽)”으로 갈수록 큰 값에서 작은 값 순으로 정렬되어 있다.  

---

강의 #8에서 다룬 분해($A = QDQ^{\top}$, 단 $A = X^{\top}X$ 형태일 때)와 달리,  
여기서는 **직교 행렬 U와 V가 동일하지 않다.**  
$A$가 정방 행렬일 필요가 없으므로, $U$와 $V$는 차원조차 같을 필요가 없다.  

- **U의 열(column)** 은 **A의 왼쪽 특이벡터(left singular vectors)** 라고 하며, $m$차원 벡터이다.  
- **V의 열(column)** (즉, $V^{\top}$의 행(row))은 **A의 오른쪽 특이벡터(right singular vectors)** 라고 하며, $n$차원 벡터이다.  
- **S의 성분(entries)** 은 **A의 특이값(singular values)** 이다.  

따라서 각 특이벡터(왼쪽이든 오른쪽이든)에는 하나의 특이값이 대응된다.  
“첫 번째(first)” 또는 “최상위(top)” 특이벡터란, 가장 큰 특이값에 대응하는 벡터를 의미한다.  

(그림 2 참고)  

---

> **주석**  
> 1. 지난 강의에서 배운 것처럼, 어떤 행렬이 **직교 행렬(orthogonal matrix)** 이라는 것은  
>    그 행렬의 열(또는 동등하게, 행)들이 모두 **직교정규(orthonormal)** 벡터임을 의미한다.  
>    즉, 각 벡터의 노름은 1이고, 서로 다른 벡터 쌍의 내적은 0이다.  
>   
> 2. (꼭 정방행렬일 필요는 없지만) “대각 행렬(diagonal matrix)”이라고 할 때의 의미는  
>    일반적으로 생각하는 그대로, $(i, i)$ 형태의 원소만 0이 아닐 수 있다는 것이다.  
>   
> 3. 작은 수치 예제조차도 직접 계산하기에는 매우 번거롭다.  
>    이는 특이벡터에 대한 직교성 제약 때문에 수들이 복잡해지기 때문이다.  
>    SVD가 실제로 어떤 모습인지 감을 잡는 가장 쉬운 방법은  
>    작은 행렬들을 **SVD를 지원하는 환경(Matlab, Python의 NumPy 라이브러리 등)** 에 입력해 보는 것이다.

<img src="/assets/img/math/svd/image_4.png" alt="image" width="600px">  

그림 2: **특이값 분해(SVD).**  
$S$의 각 특이값은 $U$의 왼쪽 특이벡터(left singular vector), 그리고 $V$의 오른쪽 특이벡터(right singular vector)와 짝을 이룬다.  

---

SVD가 행렬 **A**를 “구성 요소들의 목록(list of its ingredients)”으로 표현하는 방식을 더 잘 이해하기 위해,  
분해식 $A = USV^{\top}$ 가 다음과 동치임을 확인하라:  

$$
A = \sum_{i=1}^{\min\{m, n\}} s_i \cdot u_i v_i^{\top}, \tag{4}
$$

여기서 $s_i$는 $i$번째 특이값이고, $u_i$, $v_i$는 각각 대응되는 왼쪽과 오른쪽 특이벡터이다.  

즉, **SVD는 A를 $\min\lbrace m, n\rbrace$개의 랭크-1 행렬들의 비음수 선형 결합**으로 표현한다.  
이때 특이값들은 결합의 계수 역할을 하고,  
왼쪽·오른쪽 특이벡터의 외적(outer product)이 랭크-1 행렬을 제공한다.

모든 행렬 **A**는 SVD를 가진다.  
그 증명은 깊지 않지만, 여기보다는 선형대수학 강의에서 다루는 것이 더 적절하다.  

기하학적으로 생각하면, $m \times n$ 행렬을 $\mathbb{R}^n$에서 $\mathbb{R}^m$으로의 사상(mapping)으로 보는 경우,  
이 사실은 꽤 놀랍다:  
아무리 복잡한 행렬 **A**라도 결국 다음 과정을 수행하는 것일 뿐이다.  

1. 정의역에서의 회전 (즉, $V^{\top}$와의 곱),  
2. 필요한 만큼의 축척(scaling)과 차원 추가/삭제 (즉, $S$와의 곱),  
3. 공역에서의 회전 (즉, $U$와의 곱).  

지난 강의에서의 논의와 비슷하게, SVD는 “대체로 유일하다(more or less unique).”  
- 행렬의 특이값들은 유일하다.  
- 어떤 특이값이 여러 번 나타나는 경우,  
  해당 왼쪽·오른쪽 특이벡터가 생성하는 부분공간(subspace)은 유일하게 정의되지만,  
  각 부분공간의 **직교정규 기저(orthonormal basis)** 는 임의로 선택할 수 있다.  

> **추가 사항:**  
> 각 $i$번째 왼쪽·오른쪽 특이벡터에 -1을 곱해도 또 다른 SVD가 된다.

---

행렬의 SVD를 계산하는 꽤 좋은 알고리즘들이 있으며,  
세부 사항은 수치해석 강의에서 다룬다.  
직접 구현할 필요는 거의 없다.  

예를 들어 Matlab에서는 단순히 다음 한 줄로 SVD를 계산할 수 있다:  

```
[U, S, V] = svd(A)
```


알고리즘의 실행 시간은 $\mathcal{O}(m^2 n)$과 $\mathcal{O}(n^2 m)$ 중 더 작은 쪽이다.  
표준 구현들은 매우 최적화되어 있다.  

일반적인 노트북이라면 $5000 \times 5000$ 크기의 밀집 행렬(dense matrix)의 SVD는 문제없이 계산할 수 있지만,  
$10000 \times 10000$ 크기의 경우는 다소 시간이 걸릴 수 있다.  

또한, 만약 가장 큰 $k$개의 특이값과 그에 대응하는 특이벡터만 필요하다면,  
이를 훨씬 빠르게 계산할 수 있으며, 대략 $\mathcal{O}(kmn)$ 시간에 가능하다.  
(이 마지막 코멘트는 이번 주 미니 프로젝트와 밀접한 관련이 있다.)  

---

## 5. SVD로부터의 저순위 근사(Low-Rank Approximations from the SVD)  

행렬 **A**를 랭크-$k$ 행렬로 가장 잘 근사하려면 어떻게 해야 할까?  

만약 데이터 행렬 **A**를 여러 “구성 요소(ingredients)”들의 합으로 표현할 수 있고,  
이 구성 요소들이 “중요도” 순으로 정렬되어 있다면,  
우리는 가장 중요한 $k$개만 유지하면 될 것이다.  

그런데 바로 SVD가 이런 표현을 제공한다!  

SVD가 행렬 **A**를 (각각의 특이값으로 가중된) 랭크-1 행렬들의 합으로 표현한다는 사실을 상기하자.  
따라서 자연스러운 아이디어는 식 (4)의 우변에서 처음 $k$개의 항만 유지하는 것이다.  

즉, 식 (4)의 **A**와 목표 랭크 $k$에 대해 제안되는 랭크-$k$ 근사는 다음과 같다:  

$$
\hat{A} = \sum_{i=1}^{k} s_i \cdot u_i v_i^{\top}, \tag{5}
$$

여기서 특이값들은 정렬되어 있다고 가정한다  
($s_1 \geq s_2 \geq \cdots \geq s_{\min\lbrace m,n\rbrace} \geq 0$).  
또한 $u_i$와 $v_i$는 각각 $i$번째 왼쪽·오른쪽 특이벡터이다.  

$\hat{A}$는 $k$개의 랭크-1 행렬들의 합이므로,  
명백히 랭크가 (최대) $k$이다.  

---

### 제안된 랭크-$k$ 근사를 이해하는 또 다른 방법 (그림 3 참고)  

1. SVD $A = USV^{\top}$를 계산한다.  
   - $U$: $m \times m$ 직교 행렬  
   - $S$: 음이 아닌 대각 원소를 가지며 대각 성분이 큰 값에서 작은 값 순으로 정렬된 $m \times n$ 대각 행렬  
   - $V^{\top}$: $n \times n$ 직교 행렬  

2. 상위 $k$개의 오른쪽 특이벡터만 유지한다.  
   - $V^{\top}_k$를 $V^{\top}$의 처음 $k$개의 행으로 설정한다. ($k \times n$ 행렬)  

3. 상위 $k$개의 왼쪽 특이벡터만 유지한다.  
   - $U_k$를 $U$의 처음 $k$개의 열로 설정한다. ($m \times k$ 행렬)  

4. 상위 $k$개의 특이값만 유지한다.  
   - $S_k$를 $S$의 처음 $k$개의 행과 열로 설정한다. ($k \times k$ 행렬)  
   - 이는 $A$의 가장 큰 $k$개의 특이값에 대응한다.  

5. 최종적으로 랭크-$k$ 근사는 다음과 같이 주어진다:  

$$
A_k = U_k S_k V^{\top}_k. \tag{6}
$$

<img src="/assets/img/math/svd/image_5.png" alt="image" width="600px">  

그림 3: **SVD를 통한 저순위 근사(Low-rank approximation via SVD).**  $S$는 대각 성분에서만 0이 아니며,  그 대각 성분들은 큰 값에서 작은 값 순으로 정렬되어 있음을 상기하라.  우리의 저순위 근사는 다음과 같다:  $$A_k = U_k S_k V^{\top}_k$$

식 (6)의 우변에 있는 행렬들을 저장하는 데에는 $\mathcal{O}(k(m+n))$ 공간이 필요하다.  
이는 원래 행렬 **A**를 저장하는 데 필요한 $\mathcal{O}(mn)$ 공간과 대조된다.  
따라서 $k$가 상대적으로 작고, $m$과 $n$이 상대적으로 큰 경우(많은 응용에서 그러하다),  
이는 큰 이득이다.  

식 (6)을 해석하는 자연스러운 방법은 원시 데이터 **A**를 $k$개의 “개념(concepts)”으로 근사하는 것이다.  
예: “수학(math),” “음악(music),” “스포츠(sports)”  

- $S_k$의 특이값들은 이러한 개념들의 **신호 강도(signal strength)** 를 나타낸다.  
- $V^{\top}$의 행들과 $U$의 열들은 각각 각 개념과 연관된 **표준 행/열(canonical row/column)** 을 나타낸다.  
  - 예: 음악 제품만 좋아하는 고객, 또는 음악 고객만이 좋아하는 제품.  
- $U$의 행(각각)과 $V^{\top}$의 열(각각)은 행렬 **A**의 각 행(또는 열)을  
  이 “표준 행/열”들의 선형결합으로 (그리고 $S_k$로 스케일링하여) 근사적으로 표현한다.

개념적으로, 저순위 근사를 생성하는 이 방법은 상상할 수 있을 만큼 깔끔하다.  
우리는 **SVD**를 사용하여 **A**를 다시 표현하고,  
이 과정에서 **A**의 “구성 요소들(ingredients)”을 “중요도” 순으로 나열한 뒤,  
가장 중요한 $k$개만 유지한다.  

그러나 이렇게 우아한 계산의 결과는 실제로 유용할까?  

다음 사실이 이 접근 방식을 정당화한다:  
이 저순위 근사는 자연스러운 의미에서 **최적(optimal)** 이다.  

보장은 행렬 **M**의 **프로베니우스 노름(Frobenius norm)** 에 관한 것이다.  
이는 행렬을 벡터라고 생각했을 때의 $\ell_2$ 노름을 의미한다:  

$$
\lVert M \rVert_F = \sqrt{\sum_{i,j} m_{ij}^2}.
$$

---

**사실 5.1 (Fact 5.1)**  
모든 $m \times n$ 행렬 **A**, 랭크 목표 $k \geq 1$, 그리고 임의의 랭크-$k$ $m \times n$ 행렬 **B**에 대해:  

$$
\lVert A - A_k \rVert_F \leq \lVert A - B \rVert_F,
$$

여기서 $A_k$는 **A의 SVD**로부터 도출된 랭크-$k$ 근사(식 (6))이다.  

---

우리는 사실 5.1을 형식적으로 증명하지는 않을 것이다.  
하지만 8장에서, 우리가 이미 확립한 **PCA 방법**의 성질에 기초한 개연성 있는 설명을 볼 수 있다.

**비고 5.2 (k를 선택하는 방법).**  
저순위 행렬 근사를 생성할 때, 우리는 목표 랭크 $k$를 매개변수로 두었다.  
그렇다면 $k$는 어떻게 선택해야 할까?  

이상적인 경우라면, 행렬 **A**의 특이값들이 강력한 지침을 제공한다:  
상위 몇 개의 특이값들이 크고 나머지가 작다면,  
자연스러운 해답은 $k$를 큰 값들의 개수로 두는 것이다.  

덜 이상적인 경우에는, **유용한 근사**를 얻을 수 있는 한에서  
$k$를 가능한 작게 잡는다.  
물론 여기서 “유용한”의 의미는 응용에 따라 달라진다.  

경험적인 규칙(rule of thumb)은 종종 다음과 같은 형태를 가진다:  
상위 $k$개의 특이값의 합이, 나머지 특이값들의 합보다  
적어도 $c$배 이상이 되도록 $k$를 선택한다.  
여기서 $c$는 도메인에 따라 달라지는 상수(예: 10)이다.  

---

**비고 5.3 (Lossy Compression via Truncated Decompositions).**  
SVD를 사용해 저순위 행렬 근사를 생성하는 것은,  
**손실 압축(lossy compression)** 을 위한 유용한 패러다임의 또 다른 예시이다.  

이 패러다임의 첫 단계는 원시 데이터를 여러 항(term)으로 정확히 분해하는 것이다 (식 (3)처럼).  
두 번째 단계는 “가장 중요한” 항들을 제외한 나머지를 버려,  
원래 데이터의 근사를 얻는 것이다.  

이 패러다임은 데이터의 대부분의 흥미로운 정보가  
분해의 소수의 성분들에 집중되어 있는 표현을 찾을 수 있을 때 잘 작동한다.  

적절한 표현은 데이터셋에 따라 달라진다 —  
몇 가지 경험적인 규칙들은 존재하지만,  
데이터셋이 충분히 복잡하다면 적절한 표현이 전혀 존재하지 않을 수도 있다.

## 6. PCA는 SVD로 환원된다 (PCA Reduces to SVD)  

SVD와 지난주에 다룬 분해들 사이에는 흥미로운 관계가 있다.  

지난 강의에서, 대칭 $n \times n$ 행렬 $X^{\top}X$가  

$$
X^{\top}X = QDQ^{\top}
$$  

로 쓸 수 있음을 이용했다.  
여기서 $Q$는 $n \times n$ 직교 행렬이고, $D$는 $n \times n$ 대각 행렬이다.  
($X$는 데이터 행렬이며, 각 $m$개의 행은 $\mathbb{R}^n$에서의 데이터 포인트를 나타낸다.)  

---

SVD $X = USV^{\top}$를 고려하고, 이것이 $X^{\top}X$에 대해 의미하는 바를 살펴보자:  

$$
X^{\top}X = (USV^{\top})^{\top}(USV^{\top})  
= VS^{\top} \underbrace{U^{\top}U}_{= I} SV^{\top}  
= VDV^{\top}, \tag{7}
$$  

여기서 $D$는 대각 행렬이며, 대각 원소들은 $S$의 대각 원소들의 제곱과 같다.  
($m < n$인 경우, 나머지 $n-m$개의 대각 원소는 0이다.)  

---

지난 강의에서, $X^{\top}X = QDQ^{\top}$로 분해하면,  
$Q^{\top}$의 행들이 $X^{\top}X$의 고유벡터(eigenvectors)임을 보았다.  

따라서 식 (7)의 계산은 $V^{\top}$의 행들이 $X^{\top}X$의 고유벡터임을 보여준다.  
즉, **$X$의 오른쪽 특이벡터(right singular vectors)는 $X^{\top}X$의 고유벡터와 동일하다.**  

또한, $X^{\top}X$의 고유값들은 $X$의 특이값들의 제곱이다.  

---

따라서 **주성분분석(PCA)** 은 $X^{\top}X$를 실제로 만들 필요 없이,  
단순히 $X$의 **SVD**를 계산하는 문제로 환원된다.  

목표 $k$가 주어졌을 때, PCA의 출력은 단순히  
공분산 행렬 $X^{\top}X$의 상위 $k$개의 고유벡터이다.  

$X = USV^{\top}$의 SVD는 이 고유벡터들을 바로 제공한다 —  
즉, 그것들은 단순히 $V^{\top}$의 처음 $k$개의 행이다.  

이는 지난 강의에서 다룬 **거듭제곱 반복(Power Iteration)** 방법의 대안이다.  
그렇다면 어떤 방법이 더 나을까? 명확한 답은 없다.  

- 많은 경우 둘 다 잘 동작하며,  
- 성능이 중요하다면 두 방법 모두를 실험해 보는 것이 바람직하다.  

특히, $X^{\top}X$의 상위 몇 개 고유벡터만 원한다면  
(PCA의 데이터 시각화 응용처럼),  
하나씩 고유벡터를 찾아가는 Power Iteration이 좋아 보인다.  

반대로, 많은 고유벡터나 전체를 원한다면,  
**모두를 한꺼번에 제공하는 SVD**가 아마 가장 먼저 시도해볼 방법일 것이다.

이제 SVD와 PCA 방법 사이의 밀접한 관계를 이해했으므로,  
다시 **사실 5.1**로 돌아가 보자.  
사실 5.1은 **SVD 기반 랭크-$k$ 근사가 프로베니우스 노름(Frobenius norm)에 대해 최적(optimal)** 임을 말한다.  

직관적으로, 이 사실이 성립하는 이유는 다음과 같다:  

1. 프로베니우스 노름 $\lVert A - B \rVert_F$을 최소화하는 것은,  
   $A$와 $B$의 $i$번째 행들 사이의 **제곱 유클리드 거리(squared Euclidean distance)** 의 평균을 최소화하는 것과 동치이다.  

2. SVD는 $A$의 행들을 근사하기 위해 **PCA와 동일한 벡터들**을 사용한다.  
   (즉, $A^{\top}A$의 상위 고유벡터들 = $A$의 오른쪽 특이벡터들)  

3. PCA는 정의상, $A$의 행들과 이 벡터들의 선형결합으로 이루어진  
   $k$차원 부분공간 사이의 평균 제곱 유클리드 거리를 최소화하는  
   $k$개의 벡터를 선택한다.  

---

따라서 $A - A_k$의 한 행이 프로베니우스 노름에 기여하는 양은,  
바로 이들 **제곱 유클리드 거리들 중 하나**에 정확히 대응한다.

## 7. PCA와 SVD에 대한 추가 논의 (More on PCA vs. SVD)  

PCA와 SVD는 밀접하게 관련되어 있으며, 데이터 분석 분야에서는  
두 용어가 거의 **서로 바꿔 사용(interchangeably)** 되는 경우가 많다.  

그러나 차이는 존재한다.  
- **PCA**는 데이터 분석 접근법을 의미한다 —  
  소수의 벡터들의 선형결합으로 여러 데이터 포인트를 근사하는  
  “최적(best)” 방법을 정의하는 선택이다.  

- 반면, **SVD**는 모든 행렬에 대해 정의되는 일반적인 연산이다.  
  예를 들어, 행렬 $A$에 “PCA를 적용한다”고 말하는 것은 그다지 적절하지 않다.  
  PCA의 입력은 $\mathbb{R}^n$에서의 벡터 집합 $x_1, \dots, x_m$이지, 행렬 자체가 아니기 때문이다.  

대조적으로, SVD (식 (3))는 모든 행렬 $A$에 대해 잘 정의된다.  
특히 $A$가 행이 데이터 포인트를 나타내는 행렬인 경우,  
SVD는 PCA에서 요구되는 계산을 수행한다고 해석할 수 있다.  
(물론, SVD는 다른 많은 계산 작업에도 유용하다.)  

---

### PCA 연산과 SVD 연산의 비교  

더 “동일한 기준(apples vs. apples)”으로 비교하면 다음과 같다.  

- **PCA 연산**: $m \times n$ 데이터 행렬 $X$를 입력받고,  
  (필요하다면 파라미터 $k$도 함께)  
  공분산 행렬 $X^{\top}X$의 모든 고유벡터(또는 상위 $k$개)를 출력한다.  

- **SVD 연산**: $m \times n$ 행렬 $X$를 입력받아 $U, S, V^{\top}$를 출력한다.  
  여기서 $V^{\top}$의 행들은 $X^{\top}X$의 고유벡터이다.  

따라서 SVD는 PCA가 요구하는 것보다 더 많은 정보를 제공한다 —  
즉, 행렬 **U**도 함께 준다.  

---

### SVD가 제공하는 추가 정보 U는 유용한가?  

응용에서, 데이터 행렬 $X$의 **행 구조(row structure)** 뿐만 아니라  
**열 구조(column structure)** 를 이해하고자 한다면, 답은 **“그렇다.”**  

이를 보기 위해, SVD (식 (3))에 대한 해석을 다시 살펴보자.  

- 한편으로, 이 분해는 $X$의 각 행을 $V^{\top}$의 행들의 선형결합으로 표현한다.  
  이때 $US$의 행들이 그 선형결합의 계수 역할을 한다.  
  즉, $X$의 행들을 $V^{\top}$의 행들로 해석할 수 있으며,  
  $V^{\top}$의 행들이 흥미로운 의미를 가진다면 이는 매우 유용하다.  

- 마찬가지로, 식 (3)의 분해는 $X$의 각 열을 $U$의 열들의 선형결합으로 표현한다.  
  이때 계수는 $SV^{\top}$의 열들로 주어진다.  
  따라서 $U$의 열들이 해석 가능하다면,  
  이 분해는 $X$의 열들을 이해하는 방법을 제공한다.

일부 응용에서는, 우리는 단지 **X의 행(row)** 을 이해하는 데만 관심이 있으며,  
SVD가 PCA에 비해 추가로 제공하는 정보 $U$는 중요하지 않다.  

그러나 다른 응용에서는, **X의 행과 열(column)** 이 각각 독립적으로 흥미롭다.  
예를 들어:  

1. **행 = 고객, 열 = 제품**  
   - 행렬의 항목은 “누가 무엇을 좋아하는지”를 나타낸다고 하자.  
   - 우리는 행(고객)을 이해하는 데 관심이 있다.  
   - 최선의 경우, 오른쪽 특이벡터($V^{\top}$의 행들)은  
     “고객 유형(customer types)” 또는 “표준 고객(canonical customers)”으로 해석 가능하다.  
   - 그러면 SVD는 각 고객을 여러 고객 유형의 혼합으로 표현한다.  
     예: 각 학생의 구매 이력이 단순히  
     “CS 고객,” “음악 고객,” “의류 고객”의 혼합으로 이해될 수 있다.  

   - 이상적인 경우, 왼쪽 특이벡터($U$의 열들)은 “제품 유형(product types)”으로 해석 가능하며,  
     이때 “유형(types)”은 고객의 유형과 동일하다.  
   - 그러면 SVD는 각 제품을 여러 제품 유형의 혼합으로 표현한다.  
     (즉, 한 제품이 “CS 고객,” “음악 고객” 등에게 얼마나 매력적인지)  

2. **행 = 단백질/경로, 열 = 화학물질/약물**  
   - 행렬이 약물 상호작용 데이터를 나타낸다고 하자.  
   - 우리는 단백질과 약물을 각각 독립적으로,  
     소수의 “기본 유형(basic types)”들의 혼합으로 이해하고 싶다.  

---

위 두 가지 예시에서, 우리가 정말로 관심 있는 것은  
**두 집단 간의 관계** — 고객과 제품, 또는 단백질과 약물이다.  
한 집단을 행으로, 다른 집단을 열로 배치하는 것은 임의적이다.  

이러한 경우, 데이터를 더 잘 이해하기 위한 잠재적 도구로  
곧바로 **SVD**를 떠올려야 한다.  

반대로, **X의 열 자체가 흥미롭지 않다면**,  
이미 PCA만으로도 충분한 정보를 제공한다.

## 8. PCA 기반 저순위 근사 (PCA-Based Low-Rank Approximations, Optional)  

PCA를 위해 개발된 기법들은 다음과 같이 저순위 행렬 근사를 생성하는 데에도 사용할 수 있다.  

1. 주어진 행렬 **A**를 전처리한다.  
   - 모든 행의 합이 영벡터가 되도록 만들고,  
   - 필요하다면 각 열을 정규화한다 (지난주와 같이).  

2. 공분산 행렬 $A^{\top}A$를 만든다.  

3. 그림 1의 표기법에서, $Z^{\top}$의 $k$개 행을 **A의 상위 $k$개의 주성분(principal components)** 으로 둔다.  
   - 즉, $A^{\top}A$의 가장 큰 고유값을 가지는 $k$개의 고유벡터 $v_1, v_2, \dots, v_k$이다.  

4. $i = 1, 2, \dots, m$에 대해, 행렬 $Y$의 $i$번째 행은 다음과 같이 정의된다:  
   $x_i$를 $v_1, \dots, v_k$에 사영(projection)한 값, 즉  
   $$(\langle x_i, v_1 \rangle, \dots, \langle x_i, v_k \rangle)$$  
   이다.  
   이는 $x_i$를 $v_1, \dots, v_k$의 선형결합으로 근사했을 때,  
   $x_i$와의 유클리드 거리(Euclidean distance)가 최소가 되는 최적의 근사이다.  

---

위 네 단계는 확실히 다음 행렬을 만든다:  

$$
Y \cdot Z^{\top} \tag{8}
$$  

이 행렬은 랭크가 최대 $k$이다.  
그렇다면 이것은 SVD 기반 저순위 근사와 어떻게 비교될까?  

사실, 두 결과는 **완전히 동일하다!**  

---

**사실 8.1 (Fact 8.1)**  
식 (8)에서 정의된 행렬 $A_k$와, 식 (6)에서 정의된 행렬 $A_k$는 동일하다.  

우리는 사실 8.1을 증명하지는 않지만, 그 개연성을 설명할 수 있다.  

- $Z^{\top}$는 **A의 상위 $k$개의 주성분** — 즉, 공분산 행렬 $A^{\top}A$의 처음 $k$개의 고유벡터로 정의했다.  
- 6장에서 보았듯이, $A$의 오른쪽 특이벡터들($V^{\top}$의 행들)은 $A^{\top}A$의 고유벡터이기도 하다.  
- 따라서 $Z^{\top}$와 $V^{\top}_k$는 동일하다.  
  (둘 다 $A^{\top}A$의 상위 $k$개 고유벡터, 즉 $A$의 상위 $k$개 오른쪽 특이벡터와 같다.)  

이 점을 고려하면, 두 정의의 $A_k$가 동일한 것은 놀라운 일이 아니다.  

- 식 (8)의 $Y$와 식 (6)의 $U_k S_k$는 모두,  
  $Z^{\top}$와 $V^{\top}_k$의 행들의 선형결합을 정의하여  
  $A$를 가장 잘 근사하는 방식이다.  

- PCA 기반 해법에서는, $Y$가 바로 이렇게 정의된다.  
- SVD는 동일한 선형결합을 $U_k S_k$의 형태로 인코딩한다.  

---

### 참고문헌 (References)  
[1] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,  
and Weizhu Chen. **LoRA: Low-rank adaptation of large language models.**  
CoRR, abs/2106.09685, 2021.