---
layout: post
title: "[논문] RAPTOR: RECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL"
date: 2025-12-17 08:00:00 +0900
categories:
  - "논문"
tags: []
---
> 논문 출처  
> Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning.  
> RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval.  
> Stanford University.  
> psarthi@cs.stanford.edu
> <a href="https://arxiv.org/abs/2401.18059" target="_blank">🔗 원문 링크 (arXiv: 2401.18059)</a>

저자  
- Parth Sarthi  
- Salman Abdullah  
- Aditi Tuli  
- Shubh Khanna  
- Anna Goldie  
- Christopher D. Manning  

(Stanford University)

---

## 초록 (Abstract)

검색 증강 언어 모델(retrieval-augmented language models)은  
세계 상태(world state)의 변화에 더 잘 적응하고  
롱테일 지식(long-tail knowledge)을 통합할 수 있다.  

그러나 기존의 대부분의 방법들은  
검색 코퍼스(retrieval corpus)로부터  
짧고 연속적인 청크(short contiguous chunks)만을 검색하며,  
이로 인해 전체 문서 컨텍스트(overall document context)에 대한  
총체적 이해(holistic understanding)가 제한된다.  

우리는 텍스트 청크들을  
재귀적으로 임베딩하고, 클러스터링하며, 요약하는  
새로운 접근법을 소개하며,  
상향식(bottom up)으로 서로 다른 요약 수준을 갖는  
트리를 구성한다.  

추론 시점(inference time)에서  
우리의 RAPTOR 모은 이 트리로부터 정보를 검색하며,  
서로 다른 추상화 수준(levels of abstraction)에서  
긴 문서들(lengthy documents)에 걸친 정보를 통합한다.  

통제된 실험(controlled experiments)은  
재귀 요약(recursive summaries)을 활용한 검색이  
기존의 검색 증강 언어 모델들에 비해  
여러 과제들에서 유의미한 성능 향상을 제공함을 보여준다.  

복잡하고 다단계 추론(complex, multi-step reasoning)을 포함하는  
질의응답 과제(question-answering tasks)에서,  
우리는 최신 최고 성능(state-of-the-art) 결과를 보인다;  

예를 들어 RAPTOR 검색을 GPT-4의 사용과 결합함으로써,  
QuALITY 벤치마크에서  
절대 정확도 기준으로 20%의 성능 향상을  
달성할 수 있음을 보인다.  

---

## 1 서론 (Introduction)

대규모 언어 모델(Large Language Models, LLMs)은  
다양한 과제에서 인상적인 성능을 보이는  
변혁적인(transformative) 도구로 등장하였다.  

LLM의 규모가 커짐에 따라,  
이들은 파라미터 내부에 사실(facts)을 인코딩함으로써  
매우 효과적인 지식 저장소로서  
독립적으로(standalone) 동작할 수 있으며  
(Petroni et al., 2019; Jiang et al., 2020;  
Talmor et al., 2020; Rae et al., 2021;  
Hoffmann et al., 2022; Chowdhery et al., 2022;  
Bubeck et al., 2023; Kandpal et al., 2023),  

또한 모델은 다운스트림 과제에 대한 파인튜닝을 통해  
추가적으로 성능을 향상시킬 수 있다  
(Roberts et al., 2020).  

그럼에도 불구하고,  
아무리 큰 모델이라 하더라도  
특정 과제에 필요한 충분한 도메인 특화 지식을 포함하지는 못하며,  
세계는 계속 변화하여 LLM 내부의 사실들을 무효화한다.  

추가적인 파인튜닝이나 편집을 통해  
이러한 모델들의 지식을 업데이트하는 것은 어렵고,  
특히 방대한 텍스트 코퍼스를 다룰 때 더욱 그러하다  
(Lewis et al., 2020; Mitchell et al., 2022).  

대안적인 접근법으로,  
오픈 도메인 질의응답 시스템에서  
처음 제안된 방식(Chen et al., 2017; Yu et al., 2018)은,  

텍스트를 청크(문단) 단위로 분할한 후  
별도의 정보 검색 시스템에서 대량의 텍스트를 인덱싱하는 것이다.  

이후 검색된 정보는 질문과 함께 컨텍스트로서  
LLM에 제공되며 (“검색 증강”, retrieval augmentation;  
Lewis et al., 2020; Izacard et al., 2022;  
Min et al., 2023; Ram et al., 2023),  

이를 통해  
특정 도메인에 대한 최신 지식을 시스템에 제공하는 것이 쉬워지고,  
해석 가능성 및 출처 추적을 용이하게 할 수 있다.  

반면, LLM의 파라메트릭 지식은 불투명하며  
그 출처를 추적하기가 어렵다 (Akyurek et al., 2022).

---

그럼에도 불구하고, 기존의 검색 증강 접근법들 역시 결함을 가지고 있다.  

우리가 다루는 문제는, 대부분의 기존 방법들이  
짧고, 연속적인(contiguous) 텍스트 청크 몇 개만을 검색한다는 점이며,  
이는 대규모 담화(discourse) 구조를 표현하고 활용하는 능력을 제한한다는 것이다.  

이러한 한계는 특히  
텍스트의 여러 부분으로부터의 지식을 통합해야 하는  
주제적 질문(thematic questions)에 대해 중요하게 작용한다.  

예를 들어, NarrativeQA 데이터셋(Kočiský et al., 2018)에서처럼  
하나의 책 전체를 이해해야 하는 경우가 그렇다.  

신데렐라 동화를 생각해 보고,  
“신데렐라는 어떻게 행복한 결말에 도달했는가?”라는 질문을 고려해 보자.  

상위 $k$개로 검색된 짧은 연속 텍스트들만으로는  
이 질문에 답하기에 충분한 맥락을 포함하지 못할 것이다.

---

이를 해결하기 위해, 우리는  
텍스트에 대한 고수준(high-level) 정보와  
저수준(low-level) 세부 정보를 모두 포착하기 위해  
트리 구조를 사용하는 인덱싱 및 검색 시스템을 설계한다.  

그림 1에 나타난 것처럼,  
우리의 시스템인 RAPTOR는 텍스트 청크들을 클러스터링하고,  
해당 클러스터들의 텍스트 요약을 생성한 뒤,  
이를 반복함으로써 하위에서 상위로(bottom up) 트리를 생성한다.  

이 구조는 RAPTOR가  
서로 다른 수준에서 텍스트를 대표하는 청크들을  
LLM의 컨텍스트에 로드할 수 있도록 하며,  
이를 통해 서로 다른 수준의 질문들에 대해  
효과적이고 효율적으로 답변할 수 있게 한다.

---

**그림 1: 트리 구성 과정:**  
RAPTOR는 벡터 임베딩에 기반하여  
텍스트 청크들을 재귀적으로 클러스터링하고,  
해당 클러스터들의 텍스트 요약을 생성함으로써,  
하위에서 상위로(bottom up) 트리를 구성한다.  

함께 클러스터링된 노드들은 형제(siblings)이며,  
부모 노드는 해당 클러스터의 텍스트 요약을 포함한다.

<img src="/assets/img/paper/raptor/image_1.png" alt="image" width="800px">  

---

우리의 주요 기여는  
서로 다른 스케일에서의 컨텍스트에 대한 검색 증강을 가능하게 하기 위해  
텍스트 요약을 사용하는 아이디어를 제안하고,  
긴 문서들의 컬렉션에 대한 실험을 통해  
그 효과성을 보여주는 것이다.  

UnifiedQA(Khashabi et al., 2020),  
GPT-3(Brown et al., 2020), 그리고 GPT-4(OpenAI, 2023)라는  
세 가지 언어 모델을 사용한 통제된 실험은  
RAPTOR가 기존의 검색 증강 기법들을 능가함을 보여준다.  

더 나아가, GPT-4와 결합된 RAPTOR는,  
때로는 UnifiedQA와 결합된 경우에도,  
세 가지 OA 작업에서 새로운 최신(state-of-the-art) 성과를 달성한다.  

이들 작업은,  
책과 영화에 대한 자유 텍스트 응답 질문  
(NarrativeQA, Kočiský et al. 2018),  
전체 텍스트 NLP 논문  
(QASPER, Dasigi et al. 2021),  
그리고 중간 길이의 지문에 기반한 객관식 질문  
(QuALITY, Pang et al. 2022)이다. <sup>1</sup>

> <sup>1</sup> 우리는 RAPTOR의 코드를 공식적으로 <a href="https://github.com/parthsarthi03/raptor" target="_blank">여기</a>에서 배포한다.

---

## 2 관련 연구 (RELATED WORK)

**왜 검색(Retrieval)인가?**  

하드웨어와 알고리즘에 있어서의 최근의 발전들은  
모델이 처리할 수 있는 컨텍스트 길이를 실제로 확장시켰으며,  
그로 인해 검색 시스템의 필요성에 대한 질문이 제기되고 있다  
(Dai et al., 2019; Dao et al., 2022; Liu et al., 2023).  

그러나 Liu et al. (2023)과 Sun et al. (2021)이 지적했듯이,  
모델들은 범위가 긴(long-range) 컨텍스트를 충분히 활용하지 못하는 경향이 있으며,  
컨텍스트 길이가 증가함에 따라 성능이 점차 저하되는 모습을 보인다.  

이는 특히 관련 정보가 매우 긴 컨텍스트 안에 포함되어 있을 때 더욱 두드러진다.  

더 나아가, 실제적으로 긴 컨텍스트를 사용하는 것은  
비용이 많이 들고 속도가 느리다.  

이는 지식 집약적인 작업을 위해  
가장 관련성 높은 정보를 선택하는 것이 여전히 중요함을 시사한다.

---

**검색 방법(Retrieval Methods)**  

검색 증강 언어 모델(RALMs)은 여러 구성 요소에서의 개선을 보여 왔다:  
검색기(retriever), 판독기(reader), 그리고 종단 간(end-to-end) 시스템 학습  

검색 방법들은  
TF-IDF(Spärck Jones, 1972)와 BM25(Robertson et al., 1995; Roberts et al., 2020)와 같은  
전통적인 용어 기반(term-based) 기법들로부터  
딥러닝 기반 전략들(Karpukhin et al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023)로  
전환되어 왔다.  

일부 최근 연구들은  
방대한 지식을 기억할 수 있는 능력 때문에  
대규모 언어 모델을 검색기(retriever)로 사용하는 것을 제안한다  
(Yu et al., 2022; Sun et al., 2022).  

판독기(reader) 구성 요소에 대한 연구로는,  

검색을 위해 DPR과 BM25를 모두 사용하고,  
인코더에서 지문들을 독립적으로 처리하는  
Fusion-in-Decoder(FiD)(Izacard & Grave, 2022)와,  

교차 청크 어텐션(cross-chunked attention)과,  
청크 단위 검색(chunkwise retrieval)을 활용하여  
검색된 컨텍스트에 기반한 텍스트를 생성하는  
RETRO(Borgeaud et al., 2022; Wang et al., 2023)가 포함된다.

> DPR(Dense Passage Retrieval)는  
> 질문과 문서를 각각 밀집 벡터(dense vector)로 임베딩한 뒤,  
> 벡터 유사도를 기반으로 관련 문서를 검색하는  
> 신경망 기반 검색 기법이다.  
>  
> 주로 bi-encoder 구조를 사용하여  
> 질문 인코더와 문서 인코더를 독립적으로 학습한다.

---

종단 간(end-to-end) 시스템 학습에 관한 연구에는  
검색기와 함께 인코더-디코더 모델을 파인튜닝하는  
Atlas(Izacard et al., 2022)가 포함된다.  

또한, 개방 도메인(open-domain) 질문 응답을 위해 파인튜닝된  
양방향(bidirectional) 마스크드 언어 모델인  
REALM(Guu et al., 2020)과,  

사전 학습된 시퀀스-투-시퀀스 모델을 신경망 검색기와 통합하는  
RAG(Retrieval-Augmented Generation)(Lewis et al., 2020)도 포함된다.  

Min et al. (2021)은  
다중 정답 검색에서 지문의 다양성과 관련성을 처리하기 위해  
트리 디코딩 알고리즘을 사용하는  
Joint Passage Retrieval(JPR) 모델을 제안하였다.  

Dense Hierarchical Retrieval(DHR)과  
Hybrid Hierarchical Retrieval(HHR)은  
각각 문서(document) 수준과 지문(passage) 수준 검색을 결합하고,  
희소(sparse) 검색과 밀집(dense) 검색 방법을 통합함으로써  
검색 정확도의 향상을 보여준다  
(Liu et al., 2021; Arivazhagan et al., 2023).

---

방법들의 다양성에도 불구하고,  
모델들의 검색 구성 요소는 주로 표준적인 접근법들에 의존한다.  

즉, 코퍼스를 청크화하고 BERT 기반 검색기를 사용하여 인코딩하는 방식이다.  

이러한 접근법은 널리 채택되고 있지만,  
Nair et al. (2023)은 잠재적인 한계를 지적한다.  

연속적인 분할(contiguous segmentation)은  
텍스트의 전체적인 의미적 깊이를 포착하지 못할 수 있다는 것이다.  

기술 문서나 과학 문서로부터 추출된 스니펫(snippet)들을 읽는 것은  
중요한 컨텍스트가 결여되어 있을 수 있으며,  
이로 인해 읽기 어렵거나 심지어 오해를 불러일으킬 수도 있다  
(Cohan & Goharian, 2017; Newman et al., 2023; Zhang et al., 2023).

---

**컨텍스트로서의 재귀적 요약(Recursive summarization as Context)**  

요약 기법들은 문서에 대한 압축된 관점을 제공함으로써,  
콘텐츠에 대해 보다 집중적인 상호작용을 가능하게 한다  
(Angelidis & Lapata, 2018).  

Gao et al. (2023)의  
요약/스니펫 모델(summarization/snippet model)은  
지문들의 요약과 스니펫을 사용하며,  
이는 대부분의 데이터셋에서 정답 정확도를 향상시키지만  
때로는 손실이 있는 압축 수단이 될 수 있다.  

Wu et al. (2021)의  
재귀적-추상적 요약 모델(recursive-abstractive summarization model)은  
작업 분해(task decomposition)를 활용하여  
더 작은 텍스트 청크들을 요약하고,  
이후 이를 통합하여 더 큰 섹션들에 대한 요약을 형성한다.  

이 방법은 더 넓은 주제들을 포착하는 데에는 효과적이지만,  
세부적인(granular) 정보들을 놓칠 수 있다.  

LlamaIndex(Liu, 2022)는  
인접한 텍스트 청크들을 유사하게 요약하는 동시에  
중간 노드들을 유지함으로써 다양한 수준의 세부 정보를 저장하고,  
세밀한 정보들을 보존함으로써 이 문제를 완화한다.

그러나 두 방법 모두  
인접한 노드들을 그룹화하거나 요약하기 위해  
인접성(adjacency)에 의존하기 때문에,  
텍스트 내에 존재하는  
멀리 떨어진 상호의존성(distant interdependencies)을  
여전히 간과할 수 있으며,  
이는 우리가 RAPTOR를 통해 찾아내고 그룹화할 수 있는 부분이다.

---

## 3 방법(Methods)

**RAPTOR 개요**  

긴 텍스트는 보통 하위 주제들과 계층적 구조를 제시한다는 아이디어에 기반하여  
(Cao & Wang, 2022; Dong et al., 2023b),  
  
RAPTOR는  
세밀한 정보들에 대한 더 넓은 주제적 이해(thematic comprehension)를 제공하고,  
노드들이 텍스트 내의 순서뿐만 아니라  
의미적 유사성에 기반하여 그룹화될 수 있도록 하는  
재귀적 트리 구조를 구축함으로써,  
읽기 과정에서의 의미적(semantic) 깊이와 연결성의 문제를 다룬다.  

---

RAPTOR 트리의 구성(construction)은  
전통적인 검색 증강 기법들과 유사하게  
검색 코퍼스(retrieval corpus)를  
길이 100의 짧고 연속적인 텍스트들(short, contiguous texts)로  
분할하는 것에서 시작된다.  

한 문장에 의해 100 토큰 제한을 초과하는 경우,  
문장 중간에서 잘라내는 대신 문장 전체를 다음 청크로 이동시킨다.  

이는 각 청크 내에서 텍스트의 맥락적 및 의미적 일관성  
(contextual and semantic coherence)을 보존한다.  

이러한 텍스트들은 이후 BERT 기반 인코더인 SBERT  
(multi-qa-mpnet-base-cos-v1)를 사용하여  
임베딩된다 (Reimers & Gurevych, 2019).  

이러한 청크들과 이에 대응하는 SBERT 임베딩들은  
우리의 트리 구조에서 리프 노드(leaf nodes)를 형성한다.

---

유사한 텍스트 청크들을 묶기 위해,  
우리는 클러스터링 알고리즘을 사용한다.  

클러스터링이 완료되면, 그룹화된 텍스트들을 요약하기 위해  
언어 모델(Language Model)을 사용한다.  

이렇게 요약된 텍스트들은 이후 다시 임베딩되며,  
임베딩, 클러스터링, 요약의 순환 과정은  
추가적인 클러스터링이 불가능해질 때까지 계속된다.  

그 결과, 원본 문서들에 대한  
구조화된 다층(multi-layered) 트리 표현이 형성된다.  

RAPTOR의 중요한 특징 중 하나는  
계산 효율성(computational efficiency)이다.  

이 시스템은  
구성 시간(build time)과 토큰 소모(token expenditure)  
양쪽 모두에서 선형적으로 확장되며,  
이로 인해 크고 복잡한 코퍼스(corpora)를 처리하는 데 적합하다.  

RAPTOR의 확장성(scalability)에 대한 포괄적인 논의는  
부록 Appendix A를 참조하라.

---

이 트리 내에서의 질의를 위해,  
우리는 두 가지 서로 다른 전략을 도입한다:  
트리 순회(tree traversal)와 축약된 트리(collapsed tree)이다.  

트리 순회 방법은  
트리를 층별(layer-by-layer)로 순회하며,  
각 수준에서 가장 관련성 높은 노드들을  
가지치기(pruning)하고 선택한다.  

축약된 트리 방법은  
모든 계층에 걸쳐 노드들을 집합적으로(collectively) 평가하여,  
가장 관련성 높은 노드들을 찾는다.

---

**클러스터링 알고리즘(Clustering Algorithm)**  

클러스터링은 RAPTOR 트리를 구축하는 데에서 핵심적인 역할을 수행하며,  
텍스트 조각들을 응집력 있는 그룹들로 조직화한다.  

이 단계는 관련된 콘텐츠를 함께 묶어 주며, 이는 이후의 검색 과정을 도와준다.

---

우리의 클러스터링 접근법의 고유한 특징 중 하나는  
고정된 클러스터 수를 요구하지 않으면서  
노드들이 여러 클러스터에 속할 수 있도록 하는  
소프트 클러스터링(soft clustering)의 사용이다.  

이러한 유연성은 개별 텍스트 조각들이 종종  
다양한 주제들과 관련된 정보를 포함하고 있으며,  
그 결과 이들이 여러 요약들에 포함되는 것이 타당하기 때문에 필수적이다.

---

우리의 클러스터링 알고리즘은  
유연성과 확률적 프레임워크를 모두 제공하는 접근법인  
가우시안 혼합 모델(Gaussian Mixture Models, GMMs)에 기반한다.  

GMM은 데이터 포인트들이  
여러 개의 가우시안 분포들의 혼합으로부터 생성된다고 가정한다.

---

$N$개의 텍스트 조각들로 이루어진 집합이 주어졌다고 할 때,  

각각은 $d$차원의 밀집 벡터 임베딩으로 표현되며,  
$k$번째 가우시안 분포에 속한다는 조건 하에서,  

텍스트 벡터 $\mathbf{x}$의 가능도(likelihood)는  
다음과 같이 표기된다.  

$$
P(\mathbf{x}\mid k) = \mathcal{N}(\mathbf{x}; \mu_k, \Sigma_k)
$$

전체 확률 분포는  
가중 결합(weighted combination)으로 주어지며,  
다음과 같이 표현된다.  

$$
P(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}; \mu_k, \Sigma_k)
$$

여기서 $\pi_k$는  
$k$번째 가우시안 분포에 대한  
혼합 가중치(mixture weight)를 의미한다.

---

벡터 임베딩의 높은 차원은 고차원 공간에서 유사도를 측정할 때  
거리 척도(distance metrics)가 잘 동작하지 않을 수 있기 때문에,  
전통적인 가우시안 혼합 모델(GMMs)에 문제가 된다  
(Aggarwal et al., 2001).  

이를 완화하기 위해,  
우리는 차원 축소를 위한 매니폴드 학습 기법인  
Uniform Manifold Approximation and Projection(UMAP)을 사용한다  
(McInnes et al., 2018).  

UMAP에서 최근접 이웃의 개수를 나타내는 *n_neighbors* 파라미터는  
로컬 구조(local structures)와  
전역 구조(global structures)를  
보존하는 것 사이의 균형을 결정한다.  

우리의 알고리즘은 계층적 클러스터링 구조를 생성하기 위해  
*n_neighbors*를 변화시키며,  
먼저 전역 클러스터들을 식별한 다음,  
이러한 전역 클러스터들 내부에서 로컬 클러스터링을 수행한다.  

이러한 두 단계의 클러스터링 과정은  
광범위한 주제들로부터 구체적인 세부 사항들에 이르기까지,  
텍스트 데이터들 사이의 폭넓은 관계 스펙트럼을 포착한다.

---

로컬 클러스터의 결합된 컨텍스트가  
요약 모델의 토큰 임계값을 초과하게 되는 경우,  

우리의 알고리즘은  
해당 클러스터 내부에서 재귀적으로 클러스터링을 적용하여,  
컨텍스트가 토큰 임계값 내에 유지되도록 보장한다.

---

최적의 클러스터 수를 결정하기 위해,  
우리는 모델 선택을 위해  
베이지안 정보 기준(Bayesian Information Criterion, BIC)을 사용한다.  

BIC는 모델의 복잡도를 벌점화(penalize)할 뿐만 아니라,  
적합도의 우수성(goodness of fit) 또한 보상한다  
(Schwarz, 1978).  

주어진 가우시안 혼합 모델(GMM)에 대한  
BIC는 다음과 같이 정의된다.  

$$
\mathrm{BIC} = \ln(N)k - 2\ln(\hat{L})
$$

여기서 $N$은 텍스트 조각들(또는 데이터 포인트들)의 개수이며,  
$k$는 모델 파라미터의 개수이고,  
$\hat{L}$은 모델의 가능도 함수의 최대화된 값이다.  

GMM의 컨텍스트에서,  
모델 파라미터의 개수를 나타내는 값 $k$는  
입력 벡터들의 차원과 클러스터의 개수에 의해 결정된다.

---

BIC에 의해 결정된 최적의 클러스터 수를 바탕으로,  
이후 기대-최대화(Expectation-Maximization) 알고리즘을 사용하여  
GMM의 파라미터들, 즉 평균(means), 공분산(covariances),  
그리고 혼합 가중치(mixture weights)를 추정한다.  

---

GMM에서의 가우시안 가정은,  
종종 희소하고 치우친 분포(sparse and skewed distribution)를 보이는  
텍스트 데이터의 특성과 완벽하게 일치하지 않을 수 있지만,  
우리의 경험적 관찰은  
이 모델이 우리의 목적에 대해 효과적인 모델을 제공함을 시사한다.  

우리는 GMM 클러스터링을, 연속적인 청크들을 요약하는 방법과 비교하는  
소거 실험(ablation)을 수행하였으며,  
그에 대한 자세한 내용은 부록 Appendix B에 제시한다.

---

**모델 기반 요약(Model-Based Summarization)**

가우시안 혼합 모델을 사용하여 노드들을 클러스터링한 이후,  
각 클러스터에 속한 노드들은 요약을 위해 언어 모델로 전달된다.  

이 단계는 모델이 큰 텍스트 청크들을  
선택된 노드들에 대한 간결하고 일관된 요약으로  
변환할 수 있도록 한다.  

우리의 실험에서는, 요약을 생성하기 위해  
gpt-3.5-turbo를 사용한다.  

요약 단계는 잠재적으로 매우 클 수 있는 검색된 정보의 양을  
관리 가능한 크기로 압축한다.  

우리는 요약으로 인해 발생하는 압축에 대한 통계를  
부록 Appendix C에 제공하며,  
요약에 사용된 프롬프트는  
부록 Appendix D에 제시한다.  

---

요약 모델은 일반적으로 신뢰할 수 있는 요약을 생성하지만,  
집중된 주석(annotation) 연구를 통해  
요약의 약 4%가 경미한 환각(hallucinations)을  
포함하고 있음이 밝혀졌다.  

이러한 환각들은 상위 노드(parent nodes)로 전파되지 않았으며,  
질의응답 과제(question-answering tasks)에  
식별 가능한 영향도 미치지 않았다.  

환각에 대한 심층 분석은 부록 Appendix E를 참조하라.

---

**질의(Querying)**

이 절에서는 RAPTOR에서 사용되는 두 가지 질의 메커니즘,  
즉 트리 순회(tree traversal)와  
축약된 트리(collapsed tree)에 대해 자세히 설명한다.  

이러한 방법들은 다층 구조의 RAPTOR 트리를 순회하여  
관련 정보를 검색하는 서로 다른 방식을 제공하며,  
각각 고유한 장점과 트레이드오프를 가진다.  

우리는 두 방법 모두에 대한 의사코드(pseudocode)를  
부록 Appendix F에 제공한다.  

모든 노드들은 SBERT를 사용하여 임베딩된다는 점에 유의하라.  

---

트리 순회(tree traversal) 방법은 먼저,  
질의 임베딩에 대한 코사인 유사도를 기준으로  
가장 관련성이 높은 상위 $k$개의 루트 노드들을 선택한다.  

이렇게 선택된 노드들의 자식 노드들은 다음 계층에서 고려되며,  
이 풀(pool)로부터 다시  
질의 벡터에 대한 코사인 유사도를 기준으로  
상위 $k$개의 노드들이 선택된다.  

이 과정은 리프(leaf) 노드에 도달할 때까지 반복된다.  

마지막으로, 선택된 모든 노드들로부터의 텍스트가 연결(concatenated)되어  
검색된 컨텍스트(retrieved context)을 형성한다.  

알고리즘의 단계들은 아래에 개요로 제시되어 있다.

1. RAPTOR 트리의 루트 계층에서 시작한다.  
   질의 임베딩과 이 초기 계층에 존재하는 모든 노드들의 임베딩들 사이의  
   코사인 유사도를 계산한다.  

2. 가장 높은 코사인 유사도 점수에 기반하여,  
   상위 $k$개의 노드들을 선택하여,  
   집합 $S_1$을 형성한다.  

3. 집합 $S_1$에 속한 요소들의 자식 노드들로 이동한다.  
   질의 벡터와 이들 자식 노드들의 벡터 임베딩들 사이의  
   코사인 유사도를 계산한다.  

4. 질의에 대해 가장 높은 코사인 유사도 점수를 갖는  
   상위 $k$개의 자식 노드들을 선택하여,  
   집합 $S_2$를 형성한다.  

5. 이 과정을 $d$개의 계층에 대해 재귀적으로 계속 수행하여,  
   집합 $S_1, S_2, \ldots, S_d$를 생성한다.  

6. 집합 $S_1$부터 $S_d$까지를 연결(concatenate)하여,  
   질의에 대한 관련 컨텍스트(relevant context)을 구성한다.  

---

각 계층에서 선택되는 노드의 수 $k$와 깊이 $d$를 조정함으로써,  
트리 순회 방법은 검색되는 정보의 구체성(specificity)과  
범위(breadth)에 대한 제어를 제공한다.  

이 알고리즘은 트리의 상위 계층들을 고려함으로써  
넓은 관점(broad outlook)에서 시작하며,  
하위 계층들을 따라 내려가면서  
점진적으로 더 세밀한 세부 사항들에 집중한다.

---

**그림 2: 트리 순회와 축약된 트리 검색 메커니즘의 도식적 설명**  

트리 순회는 트리의 루트 레벨에서 시작하여,  
질의 벡터에 대한 코사인 유사도에 기반해  
상위 $k$개(여기서는 top-1)의 노드(들)를 검색한다.  

각 수준에서, 이전 계층의 상위 $k$개 노드들의  
자식 노드들로부터 상위 $k$개 노드(들)를 검색한다.  

축약된 트리는 트리를 단일 계층으로 축약하고,  
질의 벡터에 대한 코사인 유사도에 기반하여  
토큰의 개수가 임계값에 도달할 때까지 노드들을 검색한다.  

코사인 유사도 검색이 수행되는 노드들은  
두 도식 모두에서 강조 표시되어 있다.

<img src="/assets/img/paper/raptor/image_2.png" alt="image" width="800px">  

---

축약된 트리(collapsed tree) 접근법은 그림 2에 나타난 바와 같이,  
트리 내의 모든 노드들을 동시에 고려함으로써  
관련 정보를 검색하는 더 단순한 방식을 제공한다.  

계층별(layer-by-layer)로 이동하는 대신,  
이 방법은 다층 구조의 트리를 단일 계층으로 평탄화하여,  
본질적으로 모든 노드들을 비교를 위해 동일한 수준으로 가져온다.  

이 방법의 단계들은 아래에 개요로 제시되어 있다.

1. 먼저, 전체 RAPTOR 트리를 단일 계층으로 축약한다.  
   $C$로 표기되는 이 새로운 노드 집합은  
   원래 트리의 모든 계층으로부터의 노드들을 포함한다.  

2. 다음으로, 질의 임베딩과 축약된 집합 $C$에 존재하는  
   모든 노드들의 임베딩들 사이의 코사인 유사도를 계산한다.  

3. 마지막으로, 질의에 대해 가장 높은 코사인 유사도 점수를 갖는  
   상위 $k$개의 노드들을 선택한다.  
   사전에 정의된 최대 토큰 수에 도달할 때까지  
   결과 집합에 노드들을 계속 추가하되,  
   모델의 입력 제한을 초과하지 않도록 보장한다.

---

**그림 3: 질의 방법들의 비교**  

서로 다른 top-$k$ 값들을 사용한 트리 순회와,  
서로 다른 컨텍스트 길이를 사용한 축약된 트리를 이용하여,  
QASPER 데이터셋의 20개 스토리에 대한 결과를 제시한다.  

2000 토큰을 사용하는 축약된 트리는 가장 좋은 결과를 산출하므로,  
우리는 주요 결과(main results)를 위해 이 질의 전략을 사용한다.

<img src="/assets/img/paper/raptor/image_3.png" alt="image" width="600px">  

---

우리는 QASPER 데이터셋의 20개 스토리에 대해  
두 가지 접근법 모두를 테스트하였다.  

그림 3은 서로 다른 top-$k$ 값들을 사용한 트리 순회와,  
서로 다른 최대 토큰 수를 사용한 축약된 트리의 성능을 보여준다.  

축약된 트리 접근법은 일관되게 더 나은 성능을 보인다.  

우리는 축약된 트리 검색이 트리 순회보다  
더 큰 유연성을 제공하기 때문에 더 우수하다고 믿는다;  

즉, 모든 노드들을 동시에 검색함으로써 주어진 질문에 대해  
올바른 세분성 수준(correct level of granularity)에 있는  
정보를 검색해낸다.  

이에 비해, 동일한 $d$와 $k$ 값을 사용하여 트리 순회를 수행할 경우,  
트리의 각 계층으로부터 선택되는 노드들의 비율은 일정하게 유지된다.  

따라서,  
고차 주제 정보(higher-order thematic information)와  
세밀한 세부 정보(granular details) 사이의 비율은  
질문과 무관하게 동일하게 유지된다.

---

그러나 축약된 트리 접근법의 한 가지 단점은  
트리 내의 모든 노드들에 대해  
코사인 유사도 검색을 수행해야 한다는 점이다.  

하지만 이는 FAISS와 같은  
빠른 $k$-최근접 이웃(k-nearest neighbor) 라이브러리를 사용함으로써  
더 효율적으로 만들 수 있다 (Johnson et al., 2019).

---

전반적으로, 축약된 트리 접근법이 지닌 더 큰 유연성과  
QASPER 데이터셋의 부분집합에서 보인 우수한 성능을 고려할 때,  
우리는 이 질의 접근법을 사용하여 이후의 절차를 진행한다.  

구체적으로, 우리는 최대 2000 토큰을 사용하는  
축약된 트리를 사용하며, 이는 대략적으로  
상위 20개의 노드들을 검색하는 것에 해당한다.  

토큰 기반 접근법을 사용하는 것은,  
노드들마다 토큰 수가 달라질 수 있기 때문에,  
컨텍스트가 모델의 컨텍스트 제약을 초과하지 않도록 보장한다.  

UnifiedQA 모델을 사용하는 실험의 경우,  
UnifiedQA의 최대 컨텍스트 길이가 512 토큰이므로,  
우리는 400 토큰의 컨텍스트를 제공한다.  

우리는 RAPTOR와 베이스라인들 모두에 대해  
동일한 양의 컨텍스트 토큰을 제공한다.

---

**정성적 연구(Qualitative Study)**

우리는 Dense Passage Retrieval(DPR) 방법들과 비교하여  
RAPTOR의 검색 과정이 제공하는 이점을 이해하기 위해  
정성적 분석을 수행한다.  

우리의 연구는 1500단어로 이루어진 신데렐라 동화를 사용한  
주제적(thematic)이며 다중 홉(multi-hop) 질문들에  
초점을 맞춘다.  

그림 4에 나타난 바와 같이, RAPTOR의 트리 기반 검색은  
질문의 세부 수준(detail level)에 맞추어  
서로 다른 트리 계층들로부터 노드들을 선택할 수 있도록 한다.  

이러한 접근법은 DPR에 비해 다운스트림 과제들에 대해  
더 관련성이 높고 더 포괄적인 정보를 제공하는 경우가 많다.  

RAPTOR와 DPR이 특정 질문들에 대해 검색한 텍스트를 포함한  
자세한 논의와 예시를 위해서는, 부록 Appendix G를 참조하라.

---

**그림 4: 질의 과정(Querying Process):**  
신데렐라(Cinderella) 이야기와 관련된 두 가지 질문,  
즉 “이 이야기의 중심 주제는 무엇인가?”와  
“신데렐라는 어떻게 행복한 결말을 맞이했는가?”에 대해  
RAPTOR가 정보를 검색하는 방식을 도식적으로 보여준다.  

강조 표시된 노드들은 RAPTOR의 선택을 나타내며,  
화살표는 DPR의 리프 노드들을 가리킨다.  

주목할 만하게도, RAPTOR의 맥락은  
직접적으로 또는 상위 계층 요약들 내에서,  
DPR에 의해 검색된 정보를 종종 포괄한다.

<img src="/assets/img/paper/raptor/image_4.png" alt="image" width="800px">  

---

## 4 실험 (Experiments)

**데이터셋(Datasets)**

우리는 세 가지 질의응답 데이터셋,  
즉 NarrativeQA, QASPER, 그리고 QuALITY에 걸쳐  
RAPTOR의 성능을 측정한다.

---

NarrativeQA는 책과 영화 대본의 전체 텍스트를 기반으로 한  
질문–답변 쌍들로 구성된 데이터셋으로, 총 1,572개의 문서로 이루어져 있다  
(Kociský et al., 2018; Wu et al., 2021).  

NarrativeQA-Story 과제는 질문에 정확하게 답변하기 위해  
전체 내러티브에 대한 포괄적인 이해를 요구하며,  
이를 통해 문학 도메인에서 더 긴 텍스트를 이해하는 모델의 능력을 평가한다.  

우리는 이 데이터셋에서의 성능을  
표준 BLEU (B-1, B-4), ROUGE (R-L),  
그리고 METEOR (M) 지표를 사용하여 측정한다.  

우리의 실험에서 사용된 NarrativeQA 평가 스크립트에 대한  
추가적인 세부 사항은 부록 Appendix H를 참조하라.

---

QASPER 데이터셋은 1,585편의 NLP 논문에 걸쳐 5,049개의 질문을 포함하며,  
각 질문은 전체 텍스트 내에 내재된 정보를 탐색하도록 구성되어 있다  
(Dasigi et al., 2021).  

QASPER에서의 답변 유형은  
답변 가능/답변 불가능, 예/아니오, 추상적, 그리고 추출형으로 분류된다.  

정확도는 표준 F1을 사용하여 측정된다.

---

마지막으로, QuALITY 데이터셋은 객관식 질문들로 구성되어 있으며,  
각 질문에는 평균적으로 약 5,000 토큰 길이의 컨텍스트 문단들이 함께 제공된다  
(Pang et al., 2022).  

이 데이터셋은 질의응답 과제를 위해 전체 문서에 걸친 추론을 요구하며,  
이를 통해 중간 길이 문서들에서의 우리 검색 시스템의 성능을 측정할 수 있도록 한다.  

이 데이터셋에는 QuALITY-HARD라는 도전적인 부분집합이 포함되어 있는데,  
이는 속도 제한이 있는 설정에서 대다수의 인간 주석자들이 잘못 답변한 질문들을 포함한다.  

우리는 전체 테스트 세트와 HARD 부분집합 모두에 대해 정확도를 보고한다.

---

**통제된 베이스라인 비교(Controlled Baseline Comparisons)**  

우리는 먼저 UnifiedQA 3B를 리더(reader)로 사용하고,  
SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009),  
그리고 DPR (Karpukhin et al., 2020)을 임베딩 모델로 사용하여,  
RAPTOR 트리 구조를 사용한 경우와 사용하지 않은 경우에 대해  
세 개의 데이터셋, 즉 QASPER, NarrativeQA, 그리고 QuALITY에서  
통제된 비교를 제시한다.  

표 1과 표 2에서 보이듯이,  
우리의 결과는 RAPTOR가 어떤 검색기(retriever)와 결합되더라도  
모든 데이터셋 전반에 걸쳐 해당 검색기 자체보다  
일관되게 더 우수한 성능을 보인다는 것을 보여준다. <sup>2</sup>  

> <sup>2</sup> 표 1과 표 2의 DPR 실험을 위해,  
> 우리는 앞선 실험들의 나머지 부분에서 사용되었던 `dpr-single-nq-base`와는 달리  
> `dpr-multiset-base` 모델을 사용하였다.  
> 
> 이 결정은 Karpukhin et al. (2020)에서 관찰된 성능에 근거한 것으로,  
> 해당 연구에서 `dpr-multiset-base`가 더 우수한 결과를 보였기 때문이다.

---

**표 1: RAPTOR 적용 및 미적용 시 NarrativeQA 성능:**  
UnifiedQA-3B를 언어 모델로 사용하여, NarrativeQA 데이터셋에서  
RAPTOR 적용 여부에 따른 다양한 검색 방법(SBERT, BM25, DPR)의 성능을 비교한다.  
RAPTOR는 각 검색 방법에 대한 베이스라인을 상회하는 성능을 보인다.

<img src="/assets/img/paper/raptor/image_5.png" alt="image" width="640px">  

---

**표 2: RAPTOR 적용 및 미적용 시 QuALITY 및 QASPER 성능:**  
UnifiedQA-3B를 언어 모델로 사용하여,  
QuALITY 및 QASPER 데이터셋에서 RAPTOR 적용 여부에 따른  
다양한 검색 방법(SBERT, BM25, DPR)의 성능을 비교한다.  
RAPTOR는 두 데이터셋 모두에서 각 검색 방법의 베이스라인을 상회하는 성능을 보인다.

<img src="/assets/img/paper/raptor/image_6.png" alt="image" width="640px">  

---

SBERT와 결합된 RAPTOR가 가장 좋은 성능을 보이므로,  
우리는 이후의 모든 실험에서 이를 사용한다.  

이제 우리는 세 가지 서로 다른 LLM, 즉 GPT-3, GPT-4, 그리고 UnifiedQA를 사용하여  
RAPTOR를 BM25 및 DPR과 비교한다.  

표 3에 나타난 바와 같이, RAPTOR는 QASPER 데이터셋에서  
세 가지 언어 모델 모두에 걸쳐 BM25와 DPR을 일관되게 상회하는 성능을 보인다.  

RAPTOR의 F1 매치 점수는 각각 GPT-3, GPT-4, UnifiedQA를 사용할 때  
53.1%, 55.7%, 36.6%이다.  

이 점수들은 DPR을 각각 1.8, 2.7, 4.5 포인트 차이로 상회하며,  
각각의 LLM에 대해 BM25를 6.5, 5.5, 10.2 포인트 차이로 능가한다.  

QASPER는 NLP 논문 내의 정보를 종합하는 것을 요구하므로,  
RAPTOR의 상위 수준 요약 노드들이  
단지 가장 유사한 상위 $k$개의 원시(raw) 텍스트 청크만을 추출할 수 있는 방법들보다  
더 우수한 성능을 보이는 것은 놀라운 일이 아니다.  

이러한 원시 텍스트 청크들은 개별적으로는  
올바른 응답을 포함하지 않을 수도 있다.

---

**표 3: QASPER 데이터셋에서의 F-1 점수에 대한 통제된 비교,**  
세 가지 서로 다른 언어 모델(GPT-3, GPT-4, UnifiedQA 3B)과  
다양한 검색 방법을 사용한다.  

“Title + Abstract” 열은  
논문의 제목과 초록만을 컨텍스트로 사용할 때의 성능을 반영한다.  

RAPTOR는 테스트된 모든 언어 모델에 걸쳐  
기존 베이스라인인 BM25와 DPR을 상회하는 성능을 보인다.  

구체적으로, RAPTOR의 F-1 점수는  
DPR보다 최소 1.8%포인트 높고,  
BM25보다 최소 5.3%포인트 더 높다.

<img src="/assets/img/paper/raptor/image_7.png" alt="image" width="700px">  

---

