---
layout: post
title: "[ë…¼ë¬¸] Anomaly Transformer"
date: 2025-10-03 05:00:00 +0900
categories:
  - "ë…¼ë¬¸"
tags: []
---

> **ë…¼ë¬¸ ì¶œì²˜**  
> Xu, J., Wu, H., Wang, J., & Long, M.  
> *Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy*.  
> International Conference on Learning Representations (ICLR 2022).  
> <a href="https://arxiv.org/abs/2110.02642" target="_blank">ğŸ”— ì›ë¬¸ ë§í¬ (arXiv:2110.02642)</a>

# ANOMALY TRANSFORMER: TIME SERIES ANOMALY DETECTION WITH ASSOCIATION DISCREPANCY  

**ì €ì**  
- Jiehui Xu (Tsinghua University, BNRist, School of Software) - xjh20@mails.tsinghua.edu.cn  
- Haixu Wu (Tsinghua University, BNRist, School of Software) - whx20@mails.tsinghua.edu.cn  
- Jianmin Wang (Tsinghua University, BNRist, School of Software) - jimwang@tsinghua.edu.cn  
- Mingsheng Long (Tsinghua University, BNRist, School of Software) - mingsheng@tsinghua.edu.cn  

---

**ì£¼ì„**  
  
âˆ— ê³µë™ ê¸°ì—¬(Equal contribution).

---

## ì´ˆë¡ (Abstract)  

ì‹œê³„ì—´(time series)ì—ì„œì˜ ì´ìƒ ì§€ì (anomaly points)ì˜ ë¹„ì§€ë„ íƒì§€ëŠ”  
ë„ì „ì ì¸ ë¬¸ì œì´ë‹¤.  

ì´ëŠ” ëª¨ë¸ì´ **êµ¬ë³„ ê°€ëŠ¥í•œ ê¸°ì¤€(distinguishable criterion)** ì„  
ë„ì¶œí•´ë‚´ëŠ” ê²ƒì„ ìš”êµ¬í•œë‹¤.  

ì´ì „ ë°©ë²•ë“¤ì€ ì£¼ë¡œ **í¬ì¸íŠ¸ ë‹¨ìœ„ í‘œí˜„(pointwise representation)**  
ë˜ëŠ” **ìŒë³„ ì—°ê´€ì„±(pairwise association)** ì„ í•™ìŠµí•¨ìœ¼ë¡œì¨  
ì´ ë¬¸ì œë¥¼ ë‹¤ë£¨ì–´ ì™”ë‹¤.  

ê·¸ëŸ¬ë‚˜ ì–´ëŠ ìª½ë„ ë³µì¡í•œ ë™ì—­í•™(intricate dynamics)ì„  
ì¶”ë¡ í•˜ê¸°ì—ëŠ” ì¶©ë¶„í•˜ì§€ ì•Šë‹¤.  

ìµœê·¼ TransformerëŠ” **í¬ì¸íŠ¸ ë‹¨ìœ„ í‘œí˜„(pointwise representation)** ê³¼  
**ìŒë³„ ì—°ê´€ì„±(pairwise association)** ì„ í†µí•©ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°ì—  
ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.  

ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” ê° ì‹œì (time point)ì˜ **ì…€í”„ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë¶„í¬(self-attention weight distribution)** ê°€  
ì „ì²´ ì‹œê³„ì—´ê³¼ì˜ í’ë¶€í•œ ì—°ê´€ì„±(rich association)ì„ ë‹´ì„ ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆë‹¤.  

ìš°ë¦¬ì˜ í•µì‹¬ ê´€ì°°ì€, ì´ìƒ(anomalies)ì´ ë“œë¬¼ë‹¤ëŠ” ì´ìœ ë¡œ  
ë¹„ì •ìƒ ì§€ì (abnormal points)ì—ì„œ ì „ì²´ ì‹œê³„ì—´ë¡œ í–¥í•˜ëŠ”  
ë¹„ìëª…í•œ ì—°ê´€ì„±(nontrivial associations)ì„ êµ¬ì¶•í•˜ê¸°ê°€  
ê·¹ë„ë¡œ ì–´ë µë‹¤ëŠ” ê²ƒì´ë‹¤.  

ë”°ë¼ì„œ ì´ìƒ ì§€ì ë“¤ì˜ ì—°ê´€ì„±ì€ ì£¼ë¡œ  
ê·¸ë“¤ì˜ ì¸ì ‘í•œ ì‹œì (adjacent time point)ì— ì§‘ì¤‘ë  ê²ƒì´ë‹¤.  

ì´ëŸ¬í•œ **ì¸ì ‘ ì§‘ì¤‘ í¸í–¥(adjacent-concentration bias)** ì€  
ì •ìƒ(normal) ì§€ì ê³¼ ë¹„ì •ìƒ(abnormal) ì§€ì ì„ ë³¸ì§ˆì ìœ¼ë¡œ êµ¬ë³„í•  ìˆ˜ ìˆëŠ”  
**ì—°ê´€ì„± ê¸°ë°˜ ê¸°ì¤€(association-based criterion)** ì„ ë‚´í¬í•œë‹¤.  

ìš°ë¦¬ëŠ” ì´ë¥¼ **ì—°ê´€ì„± ë¶ˆì¼ì¹˜(Association Discrepancy)** ë¥¼ í†µí•´ ê°•ì¡°í•œë‹¤.  

ê¸°ìˆ ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” **ì—°ê´€ì„± ë¶ˆì¼ì¹˜(association discrepancy)** ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´  
ìƒˆë¡œìš´ **Anomaly-Attention ë©”ì»¤ë‹ˆì¦˜**ì„ ê°–ì¶˜  
**Anomaly Transformer** ë¥¼ ì œì•ˆí•œë‹¤.  

ì •ìƒ(normal)ê³¼ ë¹„ì •ìƒ(abnormal) ì‚¬ì´ì—ì„œ  
ì—°ê´€ì„± ë¶ˆì¼ì¹˜(association discrepancy)ì˜ êµ¬ë³„ ê°€ëŠ¥ì„±(distinguishability)ì„  
ì¦í­í•˜ê¸° ìœ„í•´ **ë¯¸ë‹ˆë§¥ìŠ¤ ì „ëµ(minimax strategy)** ì´ ê³ ì•ˆë˜ì—ˆë‹¤.  

Anomaly TransformerëŠ” ì„¸ ê°€ì§€ ì‘ìš© ë¶„ì•¼, ì¦‰ **ì„œë¹„ìŠ¤ ëª¨ë‹ˆí„°ë§(service monitoring)**,  
**ìš°ì£¼ ë° ì§€êµ¬ íƒì‚¬(space & earth exploration)**, **ìˆ˜ìì› ê´€ë¦¬(water treatment)** ì—ì„œì˜  
ì—¬ì„¯ ê°€ì§€ ë¹„ì§€ë„ ì‹œê³„ì—´ ì´ìƒ íƒì§€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ  
**ìµœì²¨ë‹¨(state-of-the-art) ì„±ëŠ¥**ì„ ë‹¬ì„±í•˜ì˜€ë‹¤.  

---

## 1 ì„œë¡  (Introduction)  

ì‹¤ì„¸ê³„(real-world) ì‹œìŠ¤í…œë“¤ì€ í•­ìƒ ì—°ì†ì ìœ¼ë¡œ ë™ì‘í•˜ë©°,  
ì‚°ì—… ì¥ë¹„(industrial equipment), ìš°ì£¼ íƒì‚¬ì„ (space probe) ë“±ê³¼ ê°™ì´  
ë‹¤ì¤‘ ì„¼ì„œ(multi-sensors)ì— ì˜í•´ ëª¨ë‹ˆí„°ë§ë˜ëŠ”  
ì—¬ëŸ¬ ì—°ì†ì ì¸ ì¸¡ì •ê°’(successive measurements)ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤.  

ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë°ì´í„°ì—ì„œ ì˜¤ì‘ë™(malfunctions)ì„ ë°œê²¬í•˜ëŠ” ê²ƒì€  
ì‹œê³„ì—´(time series)ì—ì„œ ë¹„ì •ìƒ ì‹œì (abnormal time points)ì„ íƒì§€í•˜ëŠ” ë¬¸ì œë¡œ  
í™˜ì›ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ë³´ì•ˆ(security)ì„ ë³´ì¥í•˜ê³   
ì¬ì •ì  ì†ì‹¤(financial loss)ì„ í”¼í•˜ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤.  

ê·¸ëŸ¬ë‚˜ ì´ìƒ(anomalies)ì€ ë³´í†µ ë“œë¬¼ê³  ë°©ëŒ€í•œ ì •ìƒ ì§€ì (normal points)ì— ì˜í•´ ê°€ë ¤ì§€ê¸° ë•Œë¬¸ì—,  
ë°ì´í„° ë¼ë²¨ë§(data labeling)ì€ ì–´ë µê³  ë¹„ìš©ì´ ë§ì´ ë“ ë‹¤(expensive).  

ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ë¹„ì§€ë„ ì„¤ì •(unsupervised setting)ì—ì„œì˜  
ì‹œê³„ì—´ ì´ìƒ íƒì§€(time series anomaly detection)ì— ì§‘ì¤‘í•œë‹¤.  

ë¹„ì§€ë„ ì‹œê³„ì—´ ì´ìƒ íƒì§€(unsupervised time series anomaly detection)ëŠ”  
ì‹¤ì œ(practice)ì—ì„œ ê·¹ë„ë¡œ ë„ì „ì ì¸ ê³¼ì œì´ë‹¤.  

ëª¨ë¸ì€ ë¹„ì§€ë„ í•™ìŠµ(unsupervised tasks)ì„ í†µí•´  
ë³µì¡í•œ ì‹œê°„ì  ë™ì—­í•™(complex temporal dynamics)ìœ¼ë¡œë¶€í„°  
ìœ ì˜ë¯¸í•œ í‘œí˜„(informative representations)ì„ í•™ìŠµí•´ì•¼ í•œë‹¤.  

ë˜í•œ ëª¨ë¸ì€ í’ë¶€í•œ ì •ìƒ ì‹œì (normal time points) ì†ì—ì„œ  
ë“œë¬¸ ì´ìƒ(rare anomalies)ì„ íƒì§€í•  ìˆ˜ ìˆëŠ”  
êµ¬ë³„ ê°€ëŠ¥í•œ ê¸°ì¤€(distinguishable criterion)ë„ ë„ì¶œí•´ì•¼ í•œë‹¤.  

ë‹¤ì–‘í•œ ê³ ì „ì ì¸ ì´ìƒ íƒì§€(classic anomaly detection) ë°©ë²•ë“¤ì€  
ë§ì€ ë¹„ì§€ë„ í•™ìŠµ(unsupervised) íŒ¨ëŸ¬ë‹¤ì„ì„ ì œê³µí•´ì™”ë‹¤.  

ì˜ˆë¥¼ ë“¤ì–´, **ì§€ì—­ ì´ìƒì¹˜ ìš”ì¸(Local Outlier Factor, LOF, Breunig et al., 2000)** ì—ì„œ ì œì•ˆëœ  
ë°€ë„ ì¶”ì •(density-estimation) ê¸°ë°˜ ë°©ë²•,  

**ì›í´ë˜ìŠ¤ SVM(One-Class SVM, OC-SVM, SchÃ¶lkopf et al., 2001)** ê³¼  
**ì„œí¬íŠ¸ ë²¡í„° ë°ì´í„° ê¸°ìˆ (Support Vector Data Description, SVDD, Tax & Duin, 2004)** ì—ì„œ ì œì‹œëœ  
í´ëŸ¬ìŠ¤í„°ë§(clustering) ê¸°ë°˜ ë°©ë²• ë“±ì´ ìˆë‹¤.  

ì´ëŸ¬í•œ ê³ ì „ì  ë°©ë²•ë“¤ì€ ì‹œê°„ì  ì •ë³´(temporal information)ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šìœ¼ë©°,  
ë³´ì§€ ëª»í•œ ì‹¤ì œ ìƒí™©(unseen real scenarios)ìœ¼ë¡œ ì¼ë°˜í™”í•˜ê¸° ì–´ë µë‹¤.  

ì‹ ê²½ë§(neural networks)ì˜ í‘œí˜„ í•™ìŠµ ëŠ¥ë ¥(representation learning capability)ì— í˜ì…ì–´,  
ìµœê·¼ì˜ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ëª¨ë¸ë“¤(Su et al., 2019; Shen et al., 2020; Li et al., 2021)ì€  
ìš°ìˆ˜í•œ ì„±ëŠ¥(superior performance)ì„ ë‹¬ì„±í•˜ì˜€ë‹¤.  

ì£¼ìš”í•œ ë°©ë²• ë²”ì£¼ ì¤‘ í•˜ë‚˜ëŠ”, ì˜ ì„¤ê³„ëœ ìˆœí™˜ ì‹ ê²½ë§(recurrent networks)ì„ í†µí•´  
**í¬ì¸íŠ¸ ë‹¨ìœ„ í‘œí˜„(pointwise representations)** ì„ í•™ìŠµí•˜ëŠ” ë° ì§‘ì¤‘í•œë‹¤.  

ê·¸ë¦¬ê³  ì´ë“¤ì€ ì¬êµ¬ì„±(reconstruction) ë˜ëŠ” ìê¸°íšŒê·€(autoregressive) ê³¼ì œë¥¼ í†µí•´  
ìê¸°ì§€ë„(self-supervised) ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœë‹¤.  

ì—¬ê¸°ì—ì„œ ìì—°ìŠ¤ëŸ½ê³  ì‹¤ìš©ì ì¸ ì´ìƒ ê¸°ì¤€(anomaly criterion)ì€  
í¬ì¸íŠ¸ ë‹¨ìœ„(pointwise) **ì¬êµ¬ì„± ì˜¤ì°¨(reconstruction error)** ë˜ëŠ”  
**ì˜ˆì¸¡ ì˜¤ì°¨(prediction error)** ì´ë‹¤.  

ê·¸ëŸ¬ë‚˜ ì´ìƒ(anomalies)ì´ ë“œë¬¼ê¸° ë•Œë¬¸ì—,  
í¬ì¸íŠ¸ ë‹¨ìœ„ í‘œí˜„(pointwise representation)ì€ ë³µì¡í•œ ì‹œê°„ì  íŒ¨í„´(complex temporal patterns)ì— ëŒ€í•´  
ì •ë³´ëŸ‰ì´ ë¶€ì¡±í•˜ë©°(less informative),  

ì •ìƒ ì‹œì (normal time points)ì— ì˜í•´ ì§€ë°°ë˜ì–´  
ì´ìƒì´ ëœ êµ¬ë³„ ê°€ëŠ¥(less distinguishable)í•˜ê²Œ ë  ìˆ˜ ìˆë‹¤.  

ë˜í•œ ì¬êµ¬ì„± ì˜¤ì°¨(reconstruction error)ë‚˜ ì˜ˆì¸¡ ì˜¤ì°¨(prediction error)ëŠ”  
í¬ì¸íŠ¸ ë‹¨ìœ„(point by point)ë¡œ ê³„ì‚°ë˜ê¸° ë•Œë¬¸ì—,  
ì‹œê°„ì  ë§¥ë½(temporal context)ì— ëŒ€í•œ í¬ê´„ì ì¸ ì„¤ëª…ì„ ì œê³µí•  ìˆ˜ ì—†ë‹¤.  

ë˜ ë‹¤ë¥¸ ì£¼ìš” ë°©ë²• ë²”ì£¼ëŠ”  
ëª…ì‹œì  ì—°ê´€ì„± ëª¨ë¸ë§(explicit association modeling)ì— ê¸°ë°˜í•˜ì—¬  
ì´ìƒì„ íƒì§€í•˜ëŠ” ê²ƒì´ë‹¤.  

ë²¡í„° ìê¸°íšŒê·€(vector autoregression)ì™€  
ìƒíƒœ ê³µê°„ ëª¨ë¸(state space models)ì´  
ì´ ë²”ì£¼ì— ì†í•œë‹¤.  

ê·¸ë˜í”„(graph) ë˜í•œ ëª…ì‹œì ìœ¼ë¡œ ì—°ê´€ì„±ì„ í¬ì°©í•˜ëŠ” ë° ì‚¬ìš©ë˜ì—ˆë‹¤.  

ì¦‰, ì„œë¡œ ë‹¤ë¥¸ ì‹œì (time points)ì„ ì •ì (vertices)ìœ¼ë¡œ í•˜ì—¬  
ì‹œê³„ì—´(time series)ì„ í‘œí˜„í•˜ê³ ,  
ëœë¤ ì›Œí¬(random walk)ë¥¼ í†µí•´ ì´ìƒì„ íƒì§€í•˜ëŠ” ë°©ì‹ì´ë‹¤  
(Cheng et al., 2008; 2009).  

---

---

> **(ë¸”ë¡œê·¸ ì¶”ê°€ ì„¤ëª…) ê·¸ë˜í”„ ê¸°ë°˜ ì´ìƒ íƒì§€ (Graph-based Anomaly Detection)**  
> 
> **1. ê·¸ë˜í”„ êµ¬ì„±í•˜ê¸° (How to build the graph)**  
> - ì‹œê³„ì—´(time series)ì˜ ê° ì‹œì (time point)ì„ **ì •ì (vertex)** ìœ¼ë¡œ ë‘”ë‹¤.  
> - ë‘ ì‹œì  ì‚¬ì´ì˜ **ê°„ì„ (edge)** ì€ ì—°ê´€ì„±(association)ì´ë‚˜ ìœ ì‚¬ì„±(similarity)ìœ¼ë¡œ ì •ì˜í•œë‹¤.  
>   - **ì‹œê°„ ì¸ì ‘ ê¸°ë°˜(Local adjacency)**: $t$ ì‹œì ì€ ë³´í†µ $t-1$, $t+1$ê³¼ ì—°ê²°.  
>   - **K-ìµœê·¼ì ‘ ì´ì›ƒ(K-NN) ê¸°ë°˜**: ê° ì‹œì ì„ ê°€ì¥ ìœ ì‚¬í•œ Kê°œì˜ ì‹œì ê³¼ ì—°ê²°.  
>   - **ì™„ì „ ì—°ê²°(Fully connected)**: ëª¨ë“  ì‹œì ì„ ì—°ê²°í•˜ë˜, ê°„ì„  ê°€ì¤‘ì¹˜(weight)ëŠ” ê±°ë¦¬/ìœ ì‚¬ë„ í•¨ìˆ˜ë¡œ ì¡°ì •.  
> - ê°„ì„ ì˜ ê°€ì¤‘ì¹˜ $w_{ij}$ ëŠ” ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤:  
>   $$
>   w_{ij} = \exp\!\left(-\|x_i - x_j\|^2\right)
>   $$  
>   ì—¬ê¸°ì„œ $x_i$ëŠ” $i$ë²ˆì§¸ ì‹œì ì˜ ê´€ì¸¡ê°’(feature)ì´ë‹¤.  
> 
> **2. ëœë¤ ì›Œí¬(Random Walk)ë¡œ ì •ìƒ/ì´ìƒ êµ¬ë¶„í•˜ê¸°**  
> - ì „ì´ í™•ë¥ (transition probability)ì€ ê°„ì„  ê°€ì¤‘ì¹˜ì— ë¹„ë¡€í•œë‹¤:  
>   $$
>   P_{ij} = \frac{w_{ij}}{\sum_k w_{ik}}
>   $$  
>   ì¦‰, $i$ ì‹œì ì—ì„œ $j$ ì‹œì ìœ¼ë¡œ ì´ë™í•  í™•ë¥ ì€ ë‘ ì ì˜ ìœ ì‚¬ì„±ì´ í´ìˆ˜ë¡ ë†’ë‹¤.  
> - **ì •ìƒ(normal) ì‹œì **:  
>   - ì—¬ëŸ¬ ë‹¤ë¥¸ ì‹œì ê³¼ ê°•í•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆìŒ.  
>   - ëœë¤ ì›Œí¬ê°€ ì´ ì •ì ì„ ë°©ë¬¸í•  í™•ë¥ ì´ ì•ˆì •ì ì´ê³ , ì£¼ë³€ ì •ì ë“¤ë¡œ ë¶„í¬ê°€ ê· ì¼í•˜ê²Œ í¼ì§„ë‹¤.  
>   - ë”°ë¼ì„œ ë¶„í¬ $\pi_t$ëŠ” ì‹œê°„ì´ ì§€ë‚˜ë©° **ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´**í•œë‹¤.  
> - **ì´ìƒ(anomaly) ì‹œì **:  
>   - ë‹¤ë¥¸ ì‹œì ë“¤ê³¼ì˜ ì—°ê²°ì´ ì•½í•˜ê±°ë‚˜ íŠ¹ì •í•œ ë°©í–¥ìœ¼ë¡œë§Œ ì¹˜ìš°ì¹¨.  
>   - ëœë¤ ì›Œí¬ê°€ ì´ ì •ì ì„ ê±°ì˜ ë°©ë¬¸í•˜ì§€ ì•Šê±°ë‚˜, ë¨¸ë¬¼ì§€ ëª»í•˜ê³  ê³§ ì´íƒˆí•œë‹¤.  
>   - ê²°ê³¼ì ìœ¼ë¡œ $\pi_t$ ë¶„í¬ê°€ **ë¶ˆê· í˜•í•˜ê²Œ ì™œê³¡**ë˜ì–´ ì •ìƒ íŒ¨í„´ê³¼ í™•ì—°íˆ êµ¬ë¶„ëœë‹¤.  
> 
> **3. ìš”ì•½**  
> ê·¸ë˜í”„ëŠ” ì‹œê³„ì—´ ë°ì´í„°ì˜ "ì—°ê²° êµ¬ì¡°"ë¥¼ ì œê³µí•˜ê³ ,  
> ëœë¤ ì›Œí¬ëŠ” ê·¸ êµ¬ì¡° ìœ„ì—ì„œ ì •ìƒê³¼ ì´ìƒì„ êµ¬ë¶„í•˜ëŠ” "íƒìƒ‰ ì ˆì°¨" ì—­í• ì„ í•œë‹¤.  
> ì´ ì¡°í•© ë•ë¶„ì— ë‹¨ìˆœí•œ í¬ì¸íŠ¸ ë‹¨ìœ„ ì˜¤ì°¨ ê³„ì‚°ë³´ë‹¤  
> ë” **êµ¬ì¡°ì ì´ê³  ì „ì—­ì ì¸ ê´€ì (global perspective)** ì—ì„œ ì´ìƒ íƒì§€ê°€ ê°€ëŠ¥í•˜ë‹¤.  

---

ì¼ë°˜ì ìœ¼ë¡œ ì´ëŸ¬í•œ ê³ ì „ì  ë°©ë²•ë“¤ì€  
ìœ ì˜ë¯¸í•œ í‘œí˜„(informative representations)ì„ í•™ìŠµí•˜ê³   
ì„¸ë°€í•œ ì—°ê´€ì„±(fine-grained associations)ì„ ëª¨ë¸ë§í•˜ê¸° ì–´ë µë‹¤.  

ìµœê·¼ì—ëŠ” ê·¸ë˜í”„ ì‹ ê²½ë§(Graph Neural Network, GNN)ì´  
ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´(multivariate time series)ì—ì„œ  
ì—¬ëŸ¬ ë³€ìˆ˜ë“¤ ê°„ì˜ ë™ì  ê·¸ë˜í”„(dynamic graph)ë¥¼ í•™ìŠµí•˜ëŠ” ë° ì ìš©ë˜ì—ˆë‹¤  
(Zhao et al., 2020; Deng & Hooi, 2021).  

ë” í’ë¶€í•œ í‘œí˜„ë ¥ì„ ê°€ì§€ê¸°ëŠ” í•˜ì§€ë§Œ,  
ì´ë ‡ê²Œ í•™ìŠµëœ ê·¸ë˜í”„ëŠ” ì—¬ì „íˆ **ë‹¨ì¼ ì‹œì (single time point)** ì— í•œì •ë˜ì–´ ìˆìœ¼ë©°,  
ì´ëŠ” ë³µì¡í•œ ì‹œê°„ì  íŒ¨í„´(complex temporal patterns)ì„ ë‹¤ë£¨ê¸°ì—ëŠ” ë¶ˆì¶©ë¶„í•˜ë‹¤.  

ë˜í•œ ë¶€ë¶„ ì‹œí€€ìŠ¤(subsequence) ê¸°ë°˜ ë°©ë²•ë“¤ì€  
ë¶€ë¶„ ì‹œí€€ìŠ¤ë“¤ ê°„ì˜ ìœ ì‚¬ì„±(similarity)ì„ ê³„ì‚°í•˜ì—¬  
ì´ìƒì„ íƒì§€í•œë‹¤ (Boniol & Palpanas, 2020).  

ì´ëŸ¬í•œ ë°©ë²•ë“¤ì€ ë” ë„“ì€ ì‹œê°„ì  ë§¥ë½(wider temporal context)ì„ íƒìƒ‰í•  ìˆ˜ëŠ” ìˆì§€ë§Œ,  
ê° ì‹œì (time point)ê³¼ ì „ì²´ ì‹œê³„ì—´(whole series) ê°„ì˜  
ì„¸ë°€í•œ ì‹œê°„ì  ì—°ê´€ì„±(fine-grained temporal association)ì€ í¬ì°©í•˜ì§€ ëª»í•œë‹¤.  

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Transformer (Vaswani et al., 2017)ë¥¼  
ë¹„ì§€ë„ í™˜ê²½(unsupervised regime)ì—ì„œì˜  
ì‹œê³„ì—´ ì´ìƒ íƒì§€(time series anomaly detection)ì— ì ìš©í•˜ì˜€ë‹¤.  

TransformerëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í° ì§„ì „ì„ ì´ë£¨ì–´ì™”ë‹¤.  
ì˜ˆë¥¼ ë“¤ì–´, ìì—°ì–´ ì²˜ë¦¬(natural language processing, Brown et al., 2020),  
ì»´í“¨í„° ë¹„ì „(machine vision, Liu et al., 2021),  
ê·¸ë¦¬ê³  ì‹œê³„ì—´(time series, Zhou et al., 2021) ë“±ì´ ìˆë‹¤.  

ì´ëŸ¬í•œ ì„±ê³µì€ ì „ì—­ í‘œí˜„(global representation)ê³¼  
ì¥ê¸° ê´€ê³„(long-range relation)ë¥¼ í†µí•©ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ”  
Transformerì˜ ê°•ë ¥í•œ ëŠ¥ë ¥ì— ê¸°ì¸í•œë‹¤.  

ê° ì‹œì (time point)ì˜ ì—°ê´€ì„± ë¶„í¬(association distribution)ëŠ”  
ì‹œê°„ì  ë§¥ë½(temporal context)ì— ëŒ€í•´ ë” ìœ ì˜ë¯¸í•œ ì„¤ëª…ì„ ì œê³µí•  ìˆ˜ ìˆë‹¤.  

ì´ëŠ” ì‹œê³„ì—´(time series)ì˜ ì£¼ê¸°(period)ë‚˜ ì¶”ì„¸(trend)ì™€ ê°™ì€  
ë™ì  íŒ¨í„´(dynamic patterns)ì„ ë“œëŸ¬ë‚¸ë‹¤.  

ìš°ë¦¬ëŠ” ìœ„ì—ì„œ ì„¤ëª…í•œ ì—°ê´€ì„± ë¶„í¬(association distribution)ë¥¼  
**ì‹œë¦¬ì¦ˆ-ì—°ê´€ì„±(series-association)** ì´ë¼ê³  ëª…ëª…í•œë‹¤.  

ì´ëŠ” Transformerë¥¼ í†µí•´ ì›ì‹œ ì‹œê³„ì—´(raw series)ë¡œë¶€í„°  
ë°œê²¬ë  ìˆ˜ ìˆë‹¤.  

ë” ë‚˜ì•„ê°€ ìš°ë¦¬ëŠ”, ì´ìƒ(anomalies)ì€ ë“œë¬¼ê³  ì •ìƒ íŒ¨í„´(normal patterns)ì´ ì§€ë°°ì ì´ê¸° ë•Œë¬¸ì—  
ì´ìƒ(anomalies)ì´ ì „ì²´ ì‹œê³„ì—´(whole series)ê³¼ ê°•í•œ ì—°ê´€ì„±(strong associations)ì„  
í˜•ì„±í•˜ê¸° ì–´ë µë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•˜ì˜€ë‹¤.  

ì´ìƒ(anomalies)ì˜ ì—°ê´€ì„±ì€ ì¸ì ‘í•œ ì‹œì (adjacent time points)ì— ì§‘ì¤‘ë˜ëŠ”ë°,  
ì´ëŠ” ì‹œê³„ì—´ì˜ ì—°ì†ì„±(continuity)ìœ¼ë¡œ ì¸í•´  
ì´ì›ƒí•œ ì‹œì ë“¤ì´ ìœ ì‚¬í•œ ë¹„ì •ìƒ íŒ¨í„´(abnormal patterns)ì„  
í¬í•¨í•  ê°€ëŠ¥ì„±ì´ ë” ë†’ê¸° ë•Œë¬¸ì´ë‹¤.  

ì´ëŸ¬í•œ ì¸ì ‘ ì§‘ì¤‘(adjacent-concentration) ê·€ë‚©ì  í¸í–¥(inductive bias)ì„  
**ì‚¬ì „ ì—°ê´€ì„±(prior-association)** ì´ë¼ê³  í•œë‹¤.  

ëŒ€ì¡°ì ìœ¼ë¡œ, ì§€ë°°ì ì¸ ì •ìƒ ì‹œì (normal time points)ì€  
ì¸ì ‘í•œ ì˜ì—­ì— êµ­í•œë˜ì§€ ì•Šê³ ,  
ì „ì²´ ì‹œê³„ì—´(whole series)ê³¼ì˜ ìœ ì˜ë¯¸í•œ ì—°ê´€ì„±(informative associations)ì„  
ë°œê²¬í•  ìˆ˜ ìˆë‹¤.  

ì´ëŸ¬í•œ ê´€ì°°ì— ê¸°ë°˜í•˜ì—¬, ìš°ë¦¬ëŠ” ì—°ê´€ì„± ë¶„í¬(association distribution)ê°€ ì§€ë‹ˆëŠ”  
ì •ìƒ(normal)ê³¼ ì´ìƒ(abnormal)ì˜ ê³ ìœ í•œ êµ¬ë³„ ê°€ëŠ¥ì„±(distinguishability)ì„  
í™œìš©í•˜ê³ ì í•œë‹¤.  

ì´ë¡œë¶€í„° ê° ì‹œì (time point)ì— ëŒ€í•´ ìƒˆë¡œìš´ ì´ìƒ ê¸°ì¤€(anomaly criterion)ì„ ì •ì˜í•  ìˆ˜ ìˆëŠ”ë°,  
ì´ëŠ” ê° ì‹œì ì˜ **ì‚¬ì „ ì—°ê´€ì„±(prior-association)** ê³¼  
**ì‹œë¦¬ì¦ˆ ì—°ê´€ì„±(series-association)** ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì •ëŸ‰í™”í•˜ì—¬ ì–»ì–´ì§„ë‹¤.  

ìš°ë¦¬ëŠ” ì´ë¥¼ **ì—°ê´€ì„± ë¶ˆì¼ì¹˜(Association Discrepancy)** ë¼ê³  ëª…ëª…í•œë‹¤.  

ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´, ì´ìƒ(anomalies)ì˜ ì—°ê´€ì„±ì€  
ì¸ì ‘ ì§‘ì¤‘(adjacent-concentrating)ë  ê°€ëŠ¥ì„±ì´ ë” í¬ê¸° ë•Œë¬¸ì—,  
ì´ìƒì€ ì •ìƒ ì‹œì (normal time points)ë³´ë‹¤  
ë” ì‘ì€ ì—°ê´€ì„± ë¶ˆì¼ì¹˜(association discrepancy)ë¥¼ ë³´ì´ê²Œ ëœë‹¤.  

ì´ì „ ë°©ë²•ë“¤ì„ ë„˜ì–´, ìš°ë¦¬ëŠ” Transformerë¥¼  
ë¹„ì§€ë„ ì‹œê³„ì—´ ì´ìƒ íƒì§€(unsupervised time series anomaly detection)ì— ë„ì…í•˜ê³ ,  
ì—°ê´€ì„± í•™ìŠµ(association learning)ì„ ìœ„í•œ **Anomaly Transformer** ë¥¼ ì œì•ˆí•œë‹¤.  

ì—°ê´€ì„± ë¶ˆì¼ì¹˜(Association Discrepancy)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´,  
ìš°ë¦¬ëŠ” ì…€í”„ ì–´í…ì…˜(self-attention) ë©”ì»¤ë‹ˆì¦˜ì„  
**ì–´ë…¸ë§ë¦¬ ì–´í…ì…˜(Anomaly-Attention)** ìœ¼ë¡œ ìƒˆë¡­ê²Œ ì„¤ê³„í•˜ì˜€ë‹¤.  

ì´ ë©”ì»¤ë‹ˆì¦˜ì€ **ì´ì¤‘ ë¶„ê¸°(two-branch) êµ¬ì¡°**ë¥¼ ê°€ì§€ë©°,  
ê° ì‹œì (time point)ì˜ **ì‚¬ì „ ì—°ê´€ì„±(prior-association)** ê³¼  
**ì‹œë¦¬ì¦ˆ ì—°ê´€ì„±(series-association)** ì„ ê°ê° ëª¨ë¸ë§í•œë‹¤.  

ì‚¬ì „ ì—°ê´€ì„±(prior-association)ì€ í•™ìŠµ ê°€ëŠ¥í•œ ê°€ìš°ì‹œì•ˆ ì»¤ë„(learnable Gaussian kernel)ì„ ì‚¬ìš©í•˜ì—¬  
ê° ì‹œì (time point)ì˜ ì¸ì ‘ ì§‘ì¤‘(adjacent-concentration) ê·€ë‚©ì  í¸í–¥(inductive bias)ì„ í‘œí˜„í•œë‹¤.  

ë°˜ë©´ ì‹œë¦¬ì¦ˆ ì—°ê´€ì„±(series-association)ì€  
ì›ì‹œ ì‹œê³„ì—´(raw series)ë¡œë¶€í„° í•™ìŠµëœ  
ì…€í”„ ì–´í…ì…˜ ê°€ì¤‘ì¹˜(self-attention weights)ì— í•´ë‹¹í•œë‹¤.  

ë˜í•œ ë‘ ë¶„ê¸°(branch) ì‚¬ì´ì—ëŠ” **ë¯¸ë‹ˆë§¥ìŠ¤ ì „ëµ(minimax strategy)** ì´ ì ìš©ë˜ë©°,  
ì´ë¥¼ í†µí•´ ì—°ê´€ì„± ë¶ˆì¼ì¹˜(Association Discrepancy)ì˜  
ì •ìƒ(normal)ê³¼ ì´ìƒ(abnormal) ê°„ êµ¬ë³„ ê°€ëŠ¥ì„±(distinguishability)ì„ ì¦í­ì‹œí‚¨ë‹¤.  

ë‚˜ì•„ê°€ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ **ì—°ê´€ì„± ê¸°ë°˜ ê¸°ì¤€(association-based criterion)** ì„ ë„ì¶œí•  ìˆ˜ ìˆë‹¤.  

Anomaly TransformerëŠ” ì„¸ ê°€ì§€ ì‹¤ì œ ì‘ìš©(real applications)ì„ í¬í•¨í•œ  
ì—¬ì„¯ ê°€ì§€ ë²¤ì¹˜ë§ˆí¬(benchmarks)ì—ì„œ  
ê°•ë ¥í•œ ì„±ëŠ¥(strong results)ì„ ë‹¬ì„±í•˜ì˜€ë‹¤.  

ë³¸ ë…¼ë¬¸ì˜ ê¸°ì—¬(contributions)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìš”ì•½ëœë‹¤:  

- ì—°ê´€ì„± ë¶ˆì¼ì¹˜(Association Discrepancy)ì— ëŒ€í•œ í•µì‹¬ ê´€ì°°ì— ê¸°ë°˜í•˜ì—¬,  
  ìš°ë¦¬ëŠ” **Anomaly-Attention ë©”ì»¤ë‹ˆì¦˜**ì„ ê°–ì¶˜ **Anomaly Transformer** ë¥¼ ì œì•ˆí•œë‹¤.  
  ì´ ëª¨ë¸ì€ ì‚¬ì „ ì—°ê´€ì„±(prior-association)ê³¼ ì‹œë¦¬ì¦ˆ ì—°ê´€ì„±(series-association)ì„  
  ë™ì‹œì— ëª¨ë¸ë§í•˜ì—¬ ì—°ê´€ì„± ë¶ˆì¼ì¹˜(Association Discrepancy)ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.  

- ìš°ë¦¬ëŠ” ì—°ê´€ì„± ë¶ˆì¼ì¹˜(Association Discrepancy)ì˜  
  ì •ìƒ(normal)ê³¼ ì´ìƒ(abnormal) ê°„ êµ¬ë³„ ê°€ëŠ¥ì„±(distinguishability)ì„ ê°•í™”í•˜ê¸° ìœ„í•´  
  **ë¯¸ë‹ˆë§¥ìŠ¤ ì „ëµ(minimax strategy)** ì„ ì œì•ˆí•œë‹¤.  
  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ **ì—°ê´€ì„± ê¸°ë°˜ íƒì§€ ê¸°ì¤€(association-based detection criterion)** ì„  
  ì¶”ê°€ì ìœ¼ë¡œ ë„ì¶œí•œë‹¤.  

- Anomaly TransformerëŠ” ì„¸ ê°€ì§€ ì‹¤ì œ ì‘ìš©(real applications)ì— ëŒ€í•œ  
  ì—¬ì„¯ ê°€ì§€ ë²¤ì¹˜ë§ˆí¬(benchmarks)ì—ì„œ  
  **ìµœì²¨ë‹¨(state-of-the-art) ì´ìƒ íƒì§€ ì„±ëŠ¥**ì„ ë‹¬ì„±í•˜ì˜€ë‹¤.  
  ì´ëŠ” ê´‘ë²”ìœ„í•œ ì œê±° ì‹¤í—˜(extensive ablations)ê³¼  
  í†µì°°ë ¥ ìˆëŠ” ì‚¬ë¡€ ì—°êµ¬(insightful case studies)ë¥¼ í†µí•´ ì…ì¦ë˜ì—ˆë‹¤.  

---

## 2 ê´€ë ¨ ì—°êµ¬ (Related Work)  

### 2.1 ë¹„ì§€ë„ ì‹œê³„ì—´ ì´ìƒ íƒì§€ (Unsupervised Time Series Anomaly Detection)  

ì¤‘ìš”í•œ ì‹¤ì œ ë¬¸ì œ(real-world problem)ë¡œì„œ,  
ë¹„ì§€ë„ ì‹œê³„ì—´ ì´ìƒ íƒì§€(unsupervised time series anomaly detection)ëŠ”  
ê´‘ë²”ìœ„í•˜ê²Œ ì—°êµ¬ë˜ì–´ ì™”ë‹¤.  

ì´ìƒ(anomaly) íŒë³„ ê¸°ì¤€(determination criterion)ì— ë”°ë¼ ë¶„ë¥˜í•˜ë©´,  
í•´ë‹¹ íŒ¨ëŸ¬ë‹¤ì„(paradigms)ì€ ëŒ€ì²´ë¡œ  
**ë°€ë„ ì¶”ì •(density-estimation)**,  
**í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜(clustering-based)**,  
**ì¬êµ¬ì„± ê¸°ë°˜(reconstruction-based)**,  
**ìê¸°íšŒê·€ ê¸°ë°˜(autoregression-based)** ë°©ë²•ë“¤ì„ í¬í•¨í•œë‹¤.  

ë°€ë„ ì¶”ì •(density-estimation) ê¸°ë°˜ ë°©ë²•ì—ì„œëŠ”,  
ëŒ€í‘œì ì¸ ê³ ì „ì  ê¸°ë²•ì¸ **ì§€ì—­ ì´ìƒì¹˜ ìš”ì¸(Local Outlier Factor, LOF, Breunig et al., 2000)** ê³¼  
**ì—°ê²°ì„± ì´ìƒì¹˜ ìš”ì¸(Connectivity Outlier Factor, COF, Tang et al., 2002)** ì´ ìˆë‹¤.  

ì´ë“¤ì€ ê°ê° **ì§€ì—­ ë°€ë„(local density)** ì™€  
**ì§€ì—­ ì—°ê²°ì„±(local connectivity)** ì„ ê³„ì‚°í•˜ì—¬  
ì´ìƒì¹˜(outlier)ë¥¼ íŒë³„í•œë‹¤.  

**DAGMM (Zong et al., 2018)** ê³¼ **MPPCACD (Yairi et al., 2017)** ëŠ”  
ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸(Gaussian Mixture Model, GMM)ì„ ê²°í•©í•˜ì—¬  
í‘œí˜„(representations)ì˜ ë°€ë„(density)ë¥¼ ì¶”ì •í•œë‹¤.  

í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜(clustering-based) ë°©ë²•ì—ì„œëŠ”,  
ì´ìƒ ì ìˆ˜(anomaly score)ê°€ í•­ìƒ **í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬(cluster center)ê¹Œì§€ì˜ ê±°ë¦¬(distance)** ë¡œ  
ì •ì‹í™”ëœë‹¤.  

**SVDD (Tax & Duin, 2004)** ì™€ **Deep SVDD (Ruff et al., 2018)** ëŠ”  
ì •ìƒ ë°ì´í„°ì—ì„œ ì–»ì–´ì§„ í‘œí˜„ë“¤(representations)ì„  
í•˜ë‚˜ì˜ ë°€ì§‘ëœ í´ëŸ¬ìŠ¤í„°(compact cluster)ë¡œ ëª¨ì€ë‹¤.  

**THOC (Shen et al., 2020)** ëŠ”  
ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ë©”ì»¤ë‹ˆì¦˜(hierarchical clustering mechanism)ì„ í†µí•´  
ì¤‘ê°„ ì¸µ(intermediate layers)ì—ì„œì˜ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹œê°„ì  íŠ¹ì§•(multi-scale temporal features)ì„ ìœµí•©(fuse)í•œë‹¤.  

ê·¸ë¦¬ê³  ë‹¤ì¸µ ê±°ë¦¬(multi-layer distances)ë¥¼ ì´ìš©í•˜ì—¬  
ì´ìƒì„ íƒì§€í•œë‹¤.  

**ITAD (Shin et al., 2020)** ëŠ”  
ë¶„í•´ëœ í…ì„œ(decomposed tensors)ì— ëŒ€í•´  
í´ëŸ¬ìŠ¤í„°ë§(clustering)ì„ ìˆ˜í–‰í•œë‹¤.  

ì¬êµ¬ì„± ê¸°ë°˜(reconstruction-based) ëª¨ë¸ë“¤ì€  
ì¬êµ¬ì„± ì˜¤ì°¨(reconstruction error)ë¥¼ í†µí•´  
ì´ìƒì„ íƒì§€í•˜ë ¤ê³  ì‹œë„í•œë‹¤.  

**Park et al. (2018)** ì€ **LSTM-VAE ëª¨ë¸**ì„ ì œì•ˆí–ˆëŠ”ë°,  
ì´ëŠ” ì‹œê°„ì  ëª¨ë¸ë§(temporal modeling)ì„ ìœ„í•´ LSTMì„ ê¸°ë°˜(backbone)ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ ,  
ì¬êµ¬ì„±ì„ ìœ„í•´ ë³€ë¶„ ì˜¤í† ì¸ì½”ë”(Variational AutoEncoder, VAE)ë¥¼ í™œìš©í•œë‹¤.  

**OmniAnomaly (Su et al., 2019)** ëŠ”  
LSTM-VAE ëª¨ë¸ì„ ì •ê·œí™” íë¦„(normalizing flow)ìœ¼ë¡œ í™•ì¥í•˜ê³ ,  
ì¬êµ¬ì„± í™•ë¥ (reconstruction probabilities)ì„ ì´ìš©í•˜ì—¬  
ì´ìƒì„ íƒì§€í•œë‹¤.  

**InterFusion (Li et al., 2021)** ì€  
ë°±ë³¸(backbone)ì„ **ê³„ì¸µì  VAE(hierarchical VAE)** ë¡œ ìƒˆë¡­ê²Œ ì„¤ê³„í•˜ì—¬,  
ì—¬ëŸ¬ ì‹œê³„ì—´(multiple series) ê°„ì˜ **ìƒí˜¸ ì˜ì¡´ì„±(inter-dependency)** ê³¼  
ë‚´ë¶€ ì˜ì¡´ì„±(intra-dependency)ì„ ë™ì‹œì— ëª¨ë¸ë§í•œë‹¤.  

**GANs (Goodfellow et al., 2014)** ì—­ì‹œ  
ì¬êµ¬ì„± ê¸°ë°˜ ì´ìƒ íƒì§€(reconstruction-based anomaly detection)ì— í™œìš©ë˜ë©°  
(Schlegl et al., 2019; Li et al., 2019a; Zhou et al., 2019),  
ì ëŒ€ì  ì •ê·œí™”(adversarial regularization)ë¡œ ì‘ë™í•œë‹¤.  

ìê¸°íšŒê·€ ê¸°ë°˜(autoregression-based) ëª¨ë¸ë“¤ì€  
ì˜ˆì¸¡ ì˜¤ì°¨(prediction error)ë¥¼ í†µí•´  
ì´ìƒì„ íƒì§€í•œë‹¤.  

**VAR** ëŠ” **ARIMA (Anderson & Kendall, 1976)** ë¥¼ í™•ì¥í•œ ëª¨ë¸ë¡œ,  
ì‹œì°¨ ì˜ì¡´ ê³µë¶„ì‚°(lag-dependent covariance)ì— ê¸°ë°˜í•˜ì—¬  
ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•œë‹¤.  

ìê¸°íšŒê·€ ëª¨ë¸(autoregressive model)ì€  
LSTMìœ¼ë¡œ ëŒ€ì²´ë  ìˆ˜ë„ ìˆë‹¤ (Hundman et al., 2018; Tariq et al., 2019).  

ë³¸ ë…¼ë¬¸ì˜ íŠ¹ì§•ì€ ìƒˆë¡œìš´ **ì—°ê´€ì„± ê¸°ë°˜ ê¸°ì¤€(association-based criterion)** ì— ìˆë‹¤.  

ëœë¤ ì›Œí¬(random walk)ë‚˜ ë¶€ë¶„ ì‹œí€€ìŠ¤(subsequence) ê¸°ë°˜ ë°©ë²•ë“¤  
(Cheng et al., 2008; Boniol & Palpanas, 2020)ê³¼ ë‹¬ë¦¬,  
ìš°ë¦¬ì˜ ê¸°ì¤€(criterion)ì€ ë³´ë‹¤ ìœ ì˜ë¯¸í•œ ì‹œì  ê°„ ì—°ê´€ì„±(time-point associations)ì„ í•™ìŠµí•˜ê¸° ìœ„í•´  
ì‹œê°„ì  ëª¨ë¸(temporal models)ì˜ **ê³µë™ ì„¤ê³„(co-design)** ë¥¼ í†µí•´ êµ¬í˜„ëœë‹¤.  

---

---

> **(ë¸”ë¡œê·¸ ì¶”ê°€ ì„¤ëª…) ì‹œê°„ì  ëª¨ë¸ì˜ ê³µë™ ì„¤ê³„ (Co-design of Temporal Models)**  
> "ê³µë™ ì„¤ê³„(co-design)"ë€ ë‹¨ì¼í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ ,  
> ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§„ ë‘ ê°œ ì´ìƒì˜ ëª¨ë¸ì„ **í•¨ê»˜ ì„¤ê³„í•˜ê³  ìƒí˜¸ ë³´ì™„ì ìœ¼ë¡œ í•™ìŠµ**ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì˜ë¯¸í•œë‹¤.  
> 
> Anomaly Transformerì—ì„œëŠ”  
> - **ì‚¬ì „ ì—°ê´€ì„±(prior-association)** ì„ í‘œí˜„í•˜ëŠ” ëª¨ë¸ â†’ ì¸ì ‘ ì‹œì (adjacent points)ì— ì§‘ì¤‘í•˜ë„ë¡ ì„¤ê³„  
> - **ì‹œë¦¬ì¦ˆ ì—°ê´€ì„±(series-association)** ì„ í‘œí˜„í•˜ëŠ” ëª¨ë¸ â†’ ì „ì²´ ì‹œê³„ì—´(global context)ê³¼ì˜ ì—°ê´€ì„±ì„ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„  
> 
> ì´ë ‡ê²Œ ë‘ ê°€ì§€ ê´€ì ì„ **ë™ì‹œì— í•™ìŠµ(co-design)** í•¨ìœ¼ë¡œì¨,  
> ë‹¨ì¼ ëª¨ë¸ë¡œëŠ” ì¡ì•„ë‚´ê¸° ì–´ë ¤ìš´ **ì„¸ë°€í•˜ê³  í’ë¶€í•œ ì‹œì  ê°„ ì—°ê´€ì„±**ì„ í¬ì°©í•  ìˆ˜ ìˆë‹¤.  
> 
> ì¦‰, ê³µë™ ì„¤ê³„ëŠ” "ì§€ì—­ì  íŒ¨í„´(local patterns)"ê³¼ "ì „ì—­ì  íŒ¨í„´(global patterns)"ì„  
> í•¨ê»˜ ë°˜ì˜í•˜ë„ë¡ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ëŠ” ì ‘ê·¼ì´ë‹¤.  

---

### 2.2 ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•œ Transformer (Transformers for Time Series Analysis)  

ìµœê·¼ Transformer (Vaswani et al., 2017)ëŠ”  
ìì—°ì–´ ì²˜ë¦¬(natural language processing, Devlin et al., 2019; Brown et al., 2020),  
ì˜¤ë””ì˜¤ ì²˜ë¦¬(audio processing, Huang et al., 2019),  
ì»´í“¨í„° ë¹„ì „(computer vision, Dosovitskiy et al., 2021; Liu et al., 2021) ë“±  
ìˆœì°¨ ë°ì´í„°(sequential data) ì²˜ë¦¬ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.  

ì‹œê³„ì—´ ë¶„ì„(time series analysis)ì—ì„œëŠ”  
ì…€í”„ ì–´í…ì…˜(self-attention) ë©”ì»¤ë‹ˆì¦˜ì˜ ì¥ì ì— í˜ì…ì–´,  
Transformerê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¥ê¸° ì‹œê°„ ì˜ì¡´ì„±(long-range temporal dependencies)ì„  
ë°œê²¬í•˜ëŠ” ë° ì‚¬ìš©ë˜ê³  ìˆë‹¤  
(Kitaev et al., 2020; Li et al., 2019b; Zhou et al., 2021; Wu et al., 2021).  

íŠ¹íˆ ì‹œê³„ì—´ ì´ìƒ íƒì§€(time series anomaly detection)ì—ì„œëŠ”,  
**GTA (Chen et al., 2021)** ê°€ ì œì•ˆë˜ì—ˆëŠ”ë°,  
ì´ëŠ” ê·¸ë˜í”„ êµ¬ì¡°(graph structure)ë¥¼ í™œìš©í•˜ì—¬  
ì—¬ëŸ¬ IoT ì„¼ì„œ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³ ,  

Transformerë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ì  ëª¨ë¸ë§(temporal modeling)ì„ ìˆ˜í–‰í•˜ë©°,  
ì¬êµ¬ì„± ê¸°ì¤€(reconstruction criterion)ì„ í†µí•´  
ì´ìƒì„ íƒì§€í•œë‹¤.  

ê¸°ì¡´ Transformer í™œìš© ë°©ì‹ê³¼ ë‹¬ë¦¬,  
**Anomaly Transformer** ëŠ” ì—°ê´€ì„± ë¶ˆì¼ì¹˜(association discrepancy)ì— ëŒ€í•œ í•µì‹¬ ê´€ì°°ì— ê¸°ë°˜í•˜ì—¬  
ì…€í”„ ì–´í…ì…˜(self-attention) ë©”ì»¤ë‹ˆì¦˜ì„  
**ì–´ë…¸ë§ë¦¬ ì–´í…ì…˜(Anomaly-Attention)** ìœ¼ë¡œ ìƒˆë¡­ê²Œ ì„¤ê³„í•˜ì˜€ë‹¤.  
