---
layout: post
title: "[ë…¼ë¬¸] Variational Inference with Normalizing Flows"
date: 2025-11-02 18:00:00 +0900
categories:
  - "ë…¼ë¬¸"
tags: []
---
> ë…¼ë¬¸ ì¶œì²˜  
> Rezende, D. J., & Mohamed, S.  
> Variational Inference with Normalizing Flows.  
> Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.  
> <a href="https://arxiv.org/abs/1505.05770" target="_blank">ğŸ”— ì›ë¬¸ ë§í¬ (arXiv: 1505.05770)</a>

ì €ì  
- Danilo Jimenez Rezende: DANILOR@GOOGLE.COM
- Shakir Mohamed: SHAKIR@GOOGLE.COM
- Google DeepMind, London  

ì œ32íšŒ êµ­ì œ ë¨¸ì‹ ëŸ¬ë‹ í•™ìˆ ëŒ€íšŒ(International Conference on Machine Learning, ICML)  
í”„ë‘ìŠ¤ ë¦´(Lille)ì—ì„œ 2015ë…„ì— ê°œìµœ.  
JMLR: ì›Œí¬ìˆ ë° ì»¨í¼ëŸ°ìŠ¤ ì ˆì°¨ì§‘(Workshop and Conference Proceedings) ì œ37ê¶Œ ìˆ˜ë¡.  
ì €ì‘ê¶Œ Â© 2015, ì €ì ì†Œìœ .

## ì´ˆë¡ (Abstract)

ë³€ë¶„ ì¶”ë¡ (variational inference)ì—ì„œ  
ê·¼ì‚¬ ì‚¬í›„ë¶„í¬(approximate posterior distribution)ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì€  
í•µì‹¬ì ì¸ ë¬¸ì œ ì¤‘ í•˜ë‚˜ì´ë‹¤.  

ëŒ€ë¶€ë¶„ì˜ ë³€ë¶„ ì¶”ë¡  ì‘ìš©ì—ì„œëŠ”  
íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ìœ„í•´ í‰ê· ì¥(mean-field) ë˜ëŠ”  
ê¸°íƒ€ ë‹¨ìˆœí•œ êµ¬ì¡°ì  ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•˜ëŠ”  
ë‹¨ìˆœí•œ í˜•íƒœì˜ ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ë¥¼ ì±„íƒí•œë‹¤.  
í•˜ì§€ë§Œ ì´ëŸ¬í•œ ì œì•½ì€ ë³€ë¶„ ì¶”ë¡ ì˜ ì •í™•ë„ì™€ í‘œí˜„ë ¥ì—  
ì‹¬ê°í•œ í•œê³„ë¥¼ ì´ˆë˜í•œë‹¤.  

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ìœ ì—°í•˜ê³ , ì„ì˜ë¡œ ë³µì¡í•˜ë©°,  
í™•ì¥ ê°€ëŠ¥í•œ ê·¼ì‚¬ ì‚¬í›„ë¶„í¬ë¥¼ ì •ì˜í•˜ê¸° ìœ„í•œ  
ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì‹œí•œë‹¤.  

ìš°ë¦¬ì˜ ê·¼ì‚¬ëŠ” ì •ê·œí™” íë¦„(normalizing flow)ì„ í†µí•´ êµ¬ì„±ëœ ë¶„í¬ë¡œ,  
ë‹¨ìˆœí•œ ì´ˆê¸° ë°€ë„(initial density)ë¥¼ ì¼ë ¨ì˜ ê°€ì—­ ë³€í™˜(invertible transformations)ì—  
ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•¨ìœ¼ë¡œì¨ ì ì°¨ ë³µì¡í•œ ë¶„í¬ë¡œ ë³€í™˜í•œë‹¤.  
ì´ ê³¼ì •ì„ í†µí•´ ì›í•˜ëŠ” ìˆ˜ì¤€ì˜ ë³µì¡ì„±ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.  

ì´ëŸ¬í•œ ê´€ì ì—ì„œ ì •ê·œí™” íë¦„ì„ ì´ìš©í•˜ì—¬  
ìœ í•œ(normalizing flow of finite length) ë°  
ë¬´í•œ(infinitesimal flow) ë³€í™˜ì˜ ë²”ì£¼ë¥¼ ë°œì „ì‹œí‚¤ê³ ,  
í’ë¶€í•œ ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•œ  
í†µí•©ëœ ì ‘ê·¼ë²•ì„ ì œì‹œí•œë‹¤.  

ë˜í•œ, ì§„ì§œ ì‚¬í›„ë¶„í¬ì— ë” ì˜ ë¶€í•©í•˜ëŠ” ì‚¬í›„ë¶„í¬ë¥¼ ê°€ì§€ëŠ” ì´ë¡ ì  ì´ì ê³¼,  
ìƒê°ëœ(amortized) ë³€ë¶„ ì¶”ë¡  ì ‘ê·¼ë²•ì˜ í™•ì¥ì„±(scalability)ì„ ê²°í•©í•¨ìœ¼ë¡œì¨,  
ë³€ë¶„ ì¶”ë¡ ì˜ ì„±ëŠ¥ê³¼ ì ìš© ê°€ëŠ¥ì„± ëª¨ë‘ì—ì„œ  
ëšœë ·í•œ í–¥ìƒì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤€ë‹¤.  

> ì „í†µì ì¸ ë³€ë¶„ ì¶”ë¡ ì—ì„œëŠ”  
> ê° ë°ì´í„° í¬ì¸íŠ¸ë§ˆë‹¤ ê³ ìœ í•œ ë³€ë¶„ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•´ì•¼ í•˜ë¯€ë¡œ  
> ê³„ì‚° ë¹„ìš©ì´ ë§¤ìš° í¬ë‹¤.  
>  
> ë°˜ë©´, ìƒê°(amortized) ë³€ë¶„ ì¶”ë¡ ì€  
> ê³µìœ ëœ ì¸í¼ëŸ°ìŠ¤ ë„¤íŠ¸ì›Œí¬(inference network) ë˜ëŠ” ì¸ì½”ë”(encoder)ë¥¼ ì‚¬ìš©í•˜ì—¬  
> ê´€ì¸¡ê°’ $x$ ë¡œë¶€í„° ë³€ë¶„ íŒŒë¼ë¯¸í„°(ì˜ˆ: $\mu(x), \sigma(x)$)ë¥¼ ì§ì ‘ ì˜ˆì¸¡í•œë‹¤.  
>  
> ì¦‰, ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ë³„ë„ì˜ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ëŒ€ì‹ ,  
> í•˜ë‚˜ì˜ ì‹ ê²½ë§ì„ í•™ìŠµì‹œì¼œ ì¶”ë¡  ë¹„ìš©ì„ ì „ì²´ ë°ì´í„°ì— ê±¸ì³ â€˜ìƒê°(amortize)â€™ì‹œí‚¤ëŠ” ë°©ì‹ì´ë‹¤.  
>  
> ì´ ì ‘ê·¼ë²•ì€ ë³€ë¶„ ì¶”ë¡ ì˜ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°,  
> íŠ¹íˆ ë³€ë¶„ ì˜¤í† ì¸ì½”ë”(VAE) ì™€ ê°™ì€ ëª¨ë¸ì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œë¡œ ì‚¬ìš©ëœë‹¤.

---

## 1. ì„œë¡  (Introduction)

ë³€ë¶„ ì¶”ë¡ (variational inference)ì€  
í™•ë¥ ì  ëª¨ë¸ë§(probabilistic modeling)ì„ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— ë§ê²Œ í™•ì¥(scaling)í•˜ê¸° ìœ„í•œ ìˆ˜ë‹¨ìœ¼ë¡œì„œ  
ìµœê·¼ ë‹¤ì‹œ í° ê´€ì‹¬ì„ ë°›ê³  ìˆë‹¤.  

ë³€ë¶„ ì¶”ë¡ ì€ ì´ì œ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ì£¼ì œ ëª¨ë¸(large-scale topic models of text)ì˜ í•µì‹¬ì— ìˆìœ¼ë©°  
(Hoffman et al., 2013),  
ì¤€ì§€ë„ ë¶„ë¥˜(semi-supervised classification)ì—ì„œ  
ìµœì²¨ë‹¨ ì„±ëŠ¥(state-of-the-art)ì„ ì œê³µí•˜ê³   
(Kingma et al., 2014),  
í˜„ì¬ ê°€ì¥ ì‚¬ì‹¤ì ì¸ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸(generative models of images)ì„ êµ¬ë™í•˜ëŠ”  
ëª¨ë¸ë“¤ì˜ ê¸°ë°˜ì´ ë˜ê³  ìˆë‹¤  
(Gregor et al., 2014; 2015; Rezende et al., 2014; Kingma & Welling, 2014).  

ë˜í•œ ë³€ë¶„ ì¶”ë¡ ì€  
ë‹¤ì–‘í•œ ë¬¼ë¦¬ì  ë° í™”í•™ì  ì‹œìŠ¤í…œ(physical and chemical systems)ì„ ì´í•´í•˜ê¸° ìœ„í•œ  
ê¸°ë³¸ ë„êµ¬(default tool)ë¡œ ìë¦¬ ì¡ì•˜ë‹¤.  

ì´ëŸ¬í•œ ì„±ê³µê³¼ ì§€ì†ì ì¸ ë°œì „ì—ë„ ë¶ˆêµ¬í•˜ê³ ,  
ë³€ë¶„ ë°©ë²•(variational methods)ì—ëŠ” ê·¸ ì„±ëŠ¥ì„ ì œí•œí•˜ê³   
í†µê³„ì  ì¶”ë¡ (statistical inference)ì˜ ê¸°ë³¸ ë°©ë²•ìœ¼ë¡œì„œ  
ë” ë„ë¦¬ ì±„íƒë˜ëŠ” ê²ƒì„ ë°©í•´í•˜ëŠ” ì—¬ëŸ¬ ë‹¨ì (disadvantages)ì´ ì¡´ì¬í•œë‹¤.  

ì´ ë…¼ë¬¸ì—ì„œ ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ê²ƒì€  
ê·¸ëŸ¬í•œ í•œê³„ë“¤ ì¤‘ í•˜ë‚˜ì¸  
ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ì˜ ì„ íƒ ë¬¸ì œì´ë‹¤.

---

ë³€ë¶„ ì¶”ë¡ (variational inference)ì€  
ê³„ì‚° ë¶ˆê°€ëŠ¥í•œ(intractable) ì‚¬í›„ë¶„í¬(posterior distribution)ë¥¼  
ì•Œë ¤ì§„ í™•ë¥ ë¶„í¬(known probability distributions)ì˜ í•œ ê³„ì—´(class)ë¡œ  
ê·¼ì‚¬(approximate)í•  ê²ƒì„ ìš”êµ¬í•˜ë©°,  
ê·¸ ê³„ì—´ ë‚´ì—ì„œ ìš°ë¦¬ëŠ” ì§„ì§œ ì‚¬í›„ë¶„í¬(true posterior)ì—  
ê°€ì¥ ì˜ ê·¼ì‚¬í•˜ëŠ” ë¶„í¬ë¥¼ íƒìƒ‰í•œë‹¤.  

ì‚¬ìš©ë˜ëŠ” ê·¼ì‚¬ ë¶„í¬ì˜ ê³„ì—´(class of approximations)ì€  
ì¢…ì¢… ì œí•œì ì´ë©°, ì˜ˆë¥¼ ë“¤ì–´ í‰ê· ì¥(mean-field) ê·¼ì‚¬ì™€ ê°™ì€ í˜•íƒœê°€ ìˆë‹¤.  
ì´ê²ƒì€ ì–´ë–¤ í•´(solution)ë„ ì§„ì§œ ì‚¬í›„ë¶„í¬ë¥¼ ë‹®ì„ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.  

ì´ ì ì€ ë³€ë¶„ ë°©ë²•(variational methods)ì— ëŒ€í•´  
ë„ë¦¬ ì œê¸°ë˜ì–´ ì˜¨ ë¹„íŒ(widely raised objection) ì¤‘ í•˜ë‚˜ë¡œ,  
ë‹¤ë¥¸ ì¶”ë¡  ë°©ë²•ë“¤ â€” ì˜ˆë¥¼ ë“¤ì–´ MCMC(Markov Chain Monte Carlo) â€” ê³¼ëŠ” ë‹¬ë¦¬,  
ë³€ë¶„ ì¶”ë¡ ì—ì„œëŠ” ë¹„ë¡ ì ê·¼ì (asymptotic) ìƒí™©ì— ì´ë¥´ë”ë¼ë„  
ì§„ì§œ ì‚¬í›„ë¶„í¬ë¥¼ ë³µì›(recover)í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤.

---

ë” í’ë¶€í•˜ê³ (richer), ë” ì¶©ì‹¤í•œ(more faithful) ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ê°€  
ë” ë‚˜ì€ ì„±ëŠ¥ì„ ê°€ì ¸ì˜¨ë‹¤ëŠ” ê²ƒì€ ë§ì€ ì¦ê±°ë“¤ì— ì˜í•´ ë’·ë°›ì¹¨ëœë‹¤.  

ì˜ˆë¥¼ ë“¤ì–´, í‰ê· ì¥(mean-field) ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ ì‹ ë…ë§(sigmoid belief networks)ê³¼ ë¹„êµí•  ë•Œ,  
ì‹¬ì¸µ ìê¸°íšŒê·€ ì‹ ê²½ë§(deep auto-regressive networks)ì€  
ìê¸°íšŒê·€ì  ì¢…ì† êµ¬ì¡°(autoregressive dependency structure)ë¥¼ ê°€ì§„  
ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì„±ëŠ¥ì´ ëª…í™•íˆ í–¥ìƒë¨ì„ ë³´ì¸ë‹¤ (Mnih & Gregor, 2014).  

ë˜í•œ ì œí•œëœ ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ê°€ ì´ˆë˜í•˜ëŠ”  
ë¶€ì •ì  ì˜í–¥(detrimental effect)ì„ ì„¤ëª…í•˜ëŠ” ë§ì€ ì—°êµ¬ ê²°ê³¼ë“¤ë„ ì¡´ì¬í•œë‹¤.  
Turner & Sahani (2011)ì€ ì¼ë°˜ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ì„¤ëª…í•œë‹¤.  

ì²«ì§¸ëŠ”, ì‚¬í›„ë¶„í¬ì˜ ë¶„ì‚°(variance)ì´ ê³¼ì†Œì¶”ì •(under-estimation)ë˜ëŠ”  
ë„ë¦¬ ê´€ì°°ëœ ë¬¸ì œë¡œ, ì´ëŠ” ì„ íƒëœ ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ì— ê¸°ë°˜í•œ  
ë¶€ì •í™•í•œ ì˜ˆì¸¡(poor predictions)ê³¼ ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ê²°ì •(unreliable decisions)ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤.  

ë‘˜ì§¸ëŠ”, ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ì˜ ì œí•œëœ í‘œí˜„ë ¥(limited capacity)ì´  
ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ MAP(Maximum A Posteriori) ì¶”ì •ì¹˜ì— í¸í–¥(bias)ì„ ìœ ë°œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.  
(ì´ëŠ” ì˜ˆë¥¼ ë“¤ì–´ ì‹œê³„ì—´ ëª¨ë¸(time-series models)ì—ì„œë„ í”íˆ ë°œìƒí•˜ëŠ” í˜„ìƒì´ë‹¤.)

---

í’ë¶€í•œ(rich) ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ë¥¼ ìœ„í•œ  
ì—¬ëŸ¬ ì œì•ˆë“¤ì´ íƒêµ¬ë˜ì–´ ì™”ìœ¼ë©°,  
ì´ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ê·¼ì‚¬ ì‚¬í›„ë¶„í¬ ë‚´ì—ì„œ  
ì–´ë–¤ ê¸°ë³¸ì ì¸ í˜•íƒœì˜ ì¢…ì†ì„±(dependency)ì„ í¬í•¨í•˜ëŠ”  
êµ¬ì¡°ì  í‰ê· ì¥(structured mean-field) ê·¼ì‚¬ì— ê¸°ë°˜í•˜ê³  ìˆë‹¤.  

ë˜ ë‹¤ë¥¸ ì ì¬ì ìœ¼ë¡œ ê°•ë ¥í•œ ëŒ€ì•ˆ(alternative)ì€  
ê·¼ì‚¬ ì‚¬í›„ë¶„í¬ë¥¼ í˜¼í•©ëª¨í˜•(mixtâ€‹ure model)ìœ¼ë¡œ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ,  
ì´ëŠ” Jaakkola & Jordan (1998); Jordan et al. (1999); Gershman et al. (2012)  
ì— ì˜í•´ ì œì•ˆëœ ë°©ë²•ë“¤ê³¼ ìœ ì‚¬í•˜ë‹¤.  

ê·¸ëŸ¬ë‚˜ í˜¼í•©ëª¨í˜• ì ‘ê·¼ë²•ì€ ë³€ë¶„ ì¶”ë¡ ì˜  
ì ì¬ì ì¸ í™•ì¥ì„±(scalability)ì„ ì œí•œí•œë‹¤.  
ê·¸ ì´ìœ ëŠ” ê° í˜¼í•© êµ¬ì„±ìš”ì†Œ(mixtâ€‹ure component)ë§ˆë‹¤  
íŒŒë¼ë¯¸í„°ê°€ ê°±ì‹ ë  ë•Œë§ˆë‹¤  
ë¡œê·¸ ê°€ëŠ¥ë„(log-likelihood)ì™€ ê·¸ ê·¸ë˜ë””ì–¸íŠ¸(gradient)ë¥¼  
í‰ê°€í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.  
ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“œëŠ”(computationally expensive) ì‘ì—…ì´ë‹¤.

---

ì´ ë…¼ë¬¸ì€ ë³€ë¶„ ì¶”ë¡ (variational inference)ì„ ìœ„í•œ  
ê·¼ì‚¬ ì‚¬í›„ë¶„í¬(approximate posterior distributions)ë¥¼  
ì •ì˜í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì‹œí•œë‹¤.  

ìš°ë¦¬ëŠ” ë¨¼ì € ìƒê° ë³€ë¶„ ì¶”ë¡ (amortized variational inference)ê³¼  
íš¨ìœ¨ì ì¸ ëª¬í…Œì¹´ë¥¼ë¡œ(Monte Carlo) ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì •ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ”  
ì¼ë°˜ì ì¸ ìœ í–¥ ê·¸ë˜í”„ ëª¨ë¸(directed graphical models)ì—ì„œì˜  
í˜„ì¬ ìµœì„ ì˜ ì¶”ë¡  ë°©ë²•ì„ ê²€í† í•˜ë©°(section 2),  
ê·¸ í›„ ë‹¤ìŒê³¼ ê°™ì€ ê³µí—Œ(contributions)ì„ ì œì‹œí•œë‹¤.  

- ìš°ë¦¬ëŠ” ì •ê·œí™” íë¦„(normalizing flows) ì„ ì‚¬ìš©í•˜ì—¬  
  ê·¼ì‚¬ ì‚¬í›„ë¶„í¬ë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤.  
  ì •ê·œí™” íë¦„ì€ ì¼ë ¨ì˜ ê°€ì—­ì  ë³€í™˜(invertible mappings)ì„ í†µí•´  
  í™•ë¥ ë°€ë„(probability density)ë¥¼ ë³€í™˜í•¨ìœ¼ë¡œì¨  
  ë³µì¡í•œ ë¶„í¬(complex distributions)ë¥¼ êµ¬ì„±í•˜ëŠ” ë„êµ¬ì´ë‹¤(section 3).  
  ì •ê·œí™” íë¦„ì„ ì´ìš©í•œ ì¶”ë¡ ì€  
  ì¶”ê°€ì ì¸ í•­ë“¤ì„ í¬í•¨í•˜ëŠ” ë” ì¡°ë°€í•˜ê³  ìˆ˜ì •ëœ ë³€ë¶„ í•˜í•œ(modified variational lower bound)ì„ ì œê³µí•˜ë©°,  
  ì´ëŸ¬í•œ ì¶”ê°€ í•­ë“¤ì€ ì„ í˜• ì‹œê°„ ë³µì¡ë„(linear time complexity)ë§Œì„ ê°€ì§„ë‹¤(section 4).  

- ìš°ë¦¬ëŠ” ì •ê·œí™” íë¦„(normalizing flows)ì´  
  ë¬´í•œì†Œ íë¦„(infinitesimal flows)ì„ í¬í•¨í•œë‹¤ëŠ” ê²ƒì„ ë³´ì¸ë‹¤.  
  ì´ ë¬´í•œì†Œ íë¦„ì€ ì ê·¼ì (asymptotic) ì˜ì—­ì—ì„œ  
  ì§„ì§œ ì‚¬í›„ë¶„í¬(true posterior distribution)ë¥¼ ë³µì›í•  ìˆ˜ ìˆëŠ”  
  ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ì˜ í•œ ê³„ì—´(class)ì„  
  ì •ì˜í•  ìˆ˜ ìˆë„ë¡ í•´ì¤€ë‹¤.  
  ì´ëŠ” ë³€ë¶„ ì¶”ë¡ (variational inference)ì˜  
  ìì£¼ ì¸ìš©ë˜ëŠ” í•œê³„(one oft-quoted limitation)ë¥¼ ê·¹ë³µí•œë‹¤.  

- ìš°ë¦¬ëŠ” ê°œì„ ëœ ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ë¥¼ ìœ„í•œ ê´€ë ¨ ì ‘ê·¼ë²•ë“¤ì„  
  íŠ¹ìˆ˜í•œ í˜•íƒœì˜ ì •ê·œí™” íë¦„(normalizing flows)ì˜ ì ìš©ìœ¼ë¡œ í†µí•©ì ìœ¼ë¡œ ë°”ë¼ë³´ëŠ”  
  í†µì¼ëœ ê´€ì (unified view)ì„ ì œì‹œí•œë‹¤(section 5).  

- ìš°ë¦¬ëŠ” ì‹¤í—˜ì ìœ¼ë¡œ ì¼ë°˜ì ì¸ ì •ê·œí™” íë¦„(general normalizing flows)ì˜ ì‚¬ìš©ì´  
  ë‹¤ë¥¸ ê²½ìŸì ì¸ ì‚¬í›„ë¶„í¬ ê·¼ì‚¬ ë°©ë²•ë“¤ë³´ë‹¤  
  ì²´ê³„ì ìœ¼ë¡œ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‚¸ë‹¤ëŠ” ê²ƒì„ ë³´ì¸ë‹¤.

---

## 2. ìƒê° ë³€ë¶„ ì¶”ë¡  (Amortized Variational Inference)

ì¶”ë¡ (inference)ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ”  
í™•ë¥ ì  ëª¨ë¸(probabilistic model)ì˜ ì£¼ë³€ê°€ëŠ¥ë„(marginal likelihood)ë¥¼ ì´ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶©ë¶„í•˜ë©°,  
ì´ëŠ” ëª¨ë¸ ë‚´ì—ì„œ ëˆ„ë½ë˜ì—ˆê±°ë‚˜(latent) ì ì¬(latent)ëœ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ì ë¶„(marginalization)ì„ ìš”êµ¬í•œë‹¤.  

ì´ ì ë¶„ì€ ì¼ë°˜ì ìœ¼ë¡œ ê³„ì‚° ë¶ˆê°€ëŠ¥(intractable)í•˜ë¯€ë¡œ,  
ëŒ€ì‹  ìš°ë¦¬ëŠ” ì£¼ë³€ê°€ëŠ¥ë„ì— ëŒ€í•œ í•˜í•œ(lower bound)ì„ ìµœì í™”í•œë‹¤.  

ê´€ì¸¡ê°’ $\mathbf{x}$, ì ë¶„í•´ì•¼ í•˜ëŠ”(latent) ì ì¬ ë³€ìˆ˜ $\mathbf{z}$,  
ê·¸ë¦¬ê³  ëª¨ë¸ íŒŒë¼ë¯¸í„° $\boldsymbol{\theta}$ë¥¼ ê°€ì§„  
ì¼ë°˜ì ì¸ í™•ë¥ ì  ëª¨ë¸(probabilistic model)ì„ ê³ ë ¤í•˜ë¼.    

ìš°ë¦¬ëŠ” ì ì¬ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ê·¼ì‚¬ ì‚¬í›„ë¶„í¬ $q_\phi(\mathbf{z} \mid \mathbf{x})$ë¥¼ ë„ì…í•˜ê³ ,  
Jordan et al. (1999)ì´ ì œì‹œí•œ ë³€ë¶„ ì›ë¦¬(variational principle)ë¥¼ ë”°ë¦„ìœ¼ë¡œì¨  
ì£¼ë³€ê°€ëŠ¥ë„ì— ëŒ€í•œ í•˜í•œ(bound)ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.  

$$
\begin{aligned}
\log p_\theta(\mathbf{x}) 
&= \log \int p_\theta(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, d\mathbf{z} \\
&= \log \int \frac{q_\phi(\mathbf{z} \mid \mathbf{x})}{q_\phi(\mathbf{z} \mid \mathbf{x})} \, p_\theta(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, d\mathbf{z} \\
&\geq - D_{\mathrm{KL}}\!\left[q_\phi(\mathbf{z} \mid \mathbf{x}) \, \| \, p(\mathbf{z})\right] 
+ \mathbb{E}_{q}[\log p_\theta(\mathbf{x} \mid \mathbf{z})] 
= -\mathcal{F}(\mathbf{x})
\end{aligned}
$$

ì—¬ê¸°ì„œ Jensen ë¶€ë“±ì‹(Jensenâ€™s inequality)ì„ ì´ìš©í•˜ì—¬  
ë§ˆì§€ë§‰ ì‹ì„ ì–»ì—ˆë‹¤.  

> Jensen ë¶€ë“±ì‹ì€ ë³¼ë¡(convex) í•¨ìˆ˜ì˜ ê¸°ëŒ“ê°’ì€ í•¨ìˆ˜ì— ê¸°ëŒ“ê°’ì„ ì ìš©í•œ ê°’ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ë‹¤ëŠ” ì„±ì§ˆì„ ë‚˜íƒ€ë‚¸ë‹¤.  
>  
> ìˆ˜í•™ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤.  
>  
> $$
> f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]
> $$
>  
> ë§Œì•½ $f$ê°€ ì˜¤ëª©(concave) í•¨ìˆ˜ë¼ë©´ ë¶€ë“±í˜¸ì˜ ë°©í–¥ì´ ë°˜ëŒ€ê°€ ëœë‹¤.  
>  
> ë³€ë¶„ ì¶”ë¡ ì—ì„œëŠ” ë¡œê·¸ í•¨ìˆ˜ê°€ ì˜¤ëª©í•¨ìˆ˜ì´ë¯€ë¡œ,  
> Jensen ë¶€ë“±ì‹ì„ ì ìš©í•˜ì—¬  
>
> $$\log \mathbb{E}[X] \geq \mathbb{E}[\log X]$$  
>
> í˜•íƒœì˜ ë¶€ë“±ì‹ì„ ì‚¬ìš©í•œë‹¤.  
>  
> ì´ ë¶€ë“±ì‹ì„ í†µí•´ ë¡œê·¸ ê°€ëŠ¥ë„(log-likelihood)ì˜ í•˜í•œ(lower bound) ì„ ìœ ë„í•  ìˆ˜ ìˆìœ¼ë©°,  
> ë°”ë¡œ ê·¸ ê²°ê³¼ê°€ ELBO (Evidence Lower Bound) ì´ë‹¤.

$p_\theta(\mathbf{x} \mid \mathbf{z})$ ëŠ” ê°€ëŠ¥ë„ í•¨ìˆ˜(likelihood function)ì´ê³ ,  
$p(\mathbf{z})$ ëŠ” ì ì¬ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ì‚¬ì „ë¶„í¬ì´ë‹¤.  

ì´ ì‹ì€ íŒŒë¼ë¯¸í„° $\boldsymbol{\theta}$ì— ëŒ€í•œ ì‚¬í›„ ì¶”ë¡ (posterior inference)ìœ¼ë¡œë„ ì‰½ê²Œ í™•ì¥í•  ìˆ˜ ìˆì§€ë§Œ,  
ì—¬ê¸°ì„œëŠ” ì ì¬ ë³€ìˆ˜(latent variables)ì— ëŒ€í•œ ì¶”ë¡ ì—ë§Œ ì§‘ì¤‘í•œë‹¤.  

ì´ í•˜í•œ(bound)ì€ ì¢…ì¢… ìŒì˜ ììœ  ì—ë„ˆì§€(negative free energy)  
ë˜ëŠ” ì¦ê±° í•˜í•œ(Evidence Lower Bound, ELBO) ìœ¼ë¡œ ë¶ˆë¦°ë‹¤.  

ì´ ì‹ì€ ë‘ í•­ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.  
ì²« ë²ˆì§¸ í•­ì€ ê·¼ì‚¬ ì‚¬í›„ë¶„í¬(approximate posterior)ì™€  
ì‚¬ì „ë¶„í¬(prior distribution) ê°„ì˜ KL ë°œì‚°(KL divergence)ìœ¼ë¡œ,  
ì •ê·œí™” í•­(regularizer) ì—­í• ì„ í•œë‹¤.  
ë‘ ë²ˆì§¸ í•­ì€ ì¬êµ¬ì„± ì˜¤ì°¨(reconstruction error)ì´ë‹¤.  

ì´ í•˜í•œ ì‹ (3)ì€ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° $\boldsymbol{\theta}$ì™€  
ë³€ë¶„ ê·¼ì‚¬ì˜ íŒŒë¼ë¯¸í„° $\boldsymbol{\phi}$ ëª¨ë‘ë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•œ  
í†µí•©ëœ(unified) ëª©ì í•¨ìˆ˜(objective function)ë¥¼ ì œê³µí•œë‹¤.

---

ë³€ë¶„ ì¶”ë¡ (variational inference)ì˜ í˜„ì¬ ìµœì„ ì˜ êµ¬í˜„(current best practice)ì€  
ë¯¸ë‹ˆë°°ì¹˜(mini-batch)ì™€ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(stochastic gradient descent)ì„ ì‚¬ìš©í•˜ì—¬  
ì´ ìµœì í™”(optimization)ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤.  

ì´ ì ‘ê·¼ë²•ì´ ë°”ë¡œ ë³€ë¶„ ì¶”ë¡ ì´  
ë§¤ìš° í° ë°ì´í„°ì…‹ì„ ê°€ì§„ ë¬¸ì œë“¤ë¡œ í™•ì¥ë  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ì´ìœ ì´ë‹¤.  

ë³€ë¶„ ì ‘ê·¼ë²•(variational approach)ì„ ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´  
í•´ê²°í•´ì•¼ í•˜ëŠ” ë‘ ê°€ì§€ ë¬¸ì œê°€ ìˆë‹¤.  

(1) ê¸°ëŒ€ ë¡œê·¸ ê°€ëŠ¥ë„(expected log-likelihood)  
$\nabla_\phi \mathbb{E}_{q_\phi(\mathbf{z})}[\log p_\theta(\mathbf{x} \mid \mathbf{z})]$ ì˜  
ë„í•¨ìˆ˜ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ë¬¸ì œ,  

ê·¸ë¦¬ê³  (2) ê³„ì‚°ì ìœ¼ë¡œ ê°€ëŠ¥í•œ(computationally-feasible) ë²”ìœ„ ë‚´ì—ì„œ  
ê°€ì¥ í’ë¶€í•œ(richest) ê·¼ì‚¬ ì‚¬í›„ë¶„í¬(approximate posterior distribution)  
$q(\cdot)$ ë¥¼ ì„ íƒí•˜ëŠ” ë¬¸ì œì´ë‹¤.

ë‘ ë²ˆì§¸ ë¬¸ì œê°€ ë°”ë¡œ ë³¸ ë…¼ë¬¸ì˜ í•µì‹¬ ì£¼ì œì´ë‹¤.  

ì²« ë²ˆì§¸ ë¬¸ì œë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´,  
ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ ë„êµ¬ë¥¼ ì‚¬ìš©í•œë‹¤:  
ëª¬í…Œì¹´ë¥¼ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì •(Monte Carlo gradient estimation)ê³¼  
ì¶”ë¡  ë„¤íŠ¸ì›Œí¬(inference networks)ì´ë‹¤.  

ì´ ë‘ ê°€ì§€ë¥¼ í•¨ê»˜ ì‚¬ìš©í•  ë•Œ,  
ìš°ë¦¬ëŠ” ì´ë¥¼ ìƒê° ë³€ë¶„ ì¶”ë¡ (amortized variational inference) ì´ë¼ê³  ë¶€ë¥¸ë‹¤.

---

### 2.1 í™•ë¥ ì  ì—­ì „íŒŒ (Stochastic Backpropagation)

ì§€ë‚œ ìˆ˜ë…„ê°„ ë³€ë¶„ ì¶”ë¡ (variational inference)ì— ê´€í•œ ì—°êµ¬ì˜ ëŒ€ë¶€ë¶„ì€  
ê¸°ëŒ€ ë¡œê·¸ ê°€ëŠ¥ë„(expected log-likelihood)ì˜ ê·¸ë˜ë””ì–¸íŠ¸(gradient)  
$$\nabla_\phi \mathbb{E}_{q_\phi(\mathbf{z})}[\log p_\theta(\mathbf{x} \mid \mathbf{z})]$$ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ì´ˆì ì„ ë§ì¶”ì–´ ì™”ë‹¤.  

ì´ì „ì— ìš°ë¦¬ëŠ” ì§€ì—­ ë³€ë¶„ ë°©ë²•(local variational methods)  
(Bishop, 2006)ì— ì˜ì¡´í–ˆì„ ê²ƒì´ì§€ë§Œ,  
ì¼ë°˜ì ìœ¼ë¡œ ì´ì œ ìš°ë¦¬ëŠ” ê·¸ëŸ¬í•œ ê¸°ëŒ“ê°’(expectations)ì„  
ëª¬í…Œì¹´ë¥¼ë¡œ ê·¼ì‚¬(Monte Carlo approximations)ë¥¼ ì‚¬ìš©í•˜ì—¬ í•­ìƒ ê³„ì‚°í•œë‹¤.  
(ì´ëŠ” KL í•­(KL term)ì´ ë¶„ì„ì ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆì§€ ì•Šì€ ê²½ìš°,  
í•˜í•œ(bound)ì— í¬í•¨ë˜ëŠ” ê²½ìš°ë¥¼ í¬í•¨í•œë‹¤.)  

> ì§€ì—­ ë³€ë¶„ ë°©ë²•ì€ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì „ì—­(global) ê·¼ì‚¬ ëŒ€ì‹ ,  
> ê° ë°ì´í„° í¬ì¸íŠ¸ë‚˜ ê°œë³„ ë³€ìˆ˜ì— ëŒ€í•´ ì§€ì—­ì ìœ¼ë¡œ(local)  
> ë³€ë¶„ íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ëŠ” ì ‘ê·¼ë²•ì´ë‹¤.  
>  
> ì´ëŸ¬í•œ ë°©ë²•ì€ ê³„ì‚°ëŸ‰ì´ ë§ê³  í™•ì¥ì„±ì´ ë–¨ì–´ì§€ê¸° ë•Œë¬¸ì—,  
> í˜„ì¬ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ ê·¼ì‚¬ ê¸°ë°˜ì˜ í™•ë¥ ì  ì¶”ë¡ (stochastic inference) ìœ¼ë¡œ ëŒ€ì²´ë˜ê³  ìˆë‹¤.

ì´ ì ‘ê·¼ë²•ì€ ì ì ˆí•˜ê²Œ ì´ë¦„ ë¶™ì—¬ì§„  
ì´ì¤‘ í™•ë¥  ì¶”ì •(doubly-stochastic estimation) ì„ í˜•ì„±í•œë‹¤  
(Titsias & Lazaro-Gredilla, 2014).  

ì´ëŠ” ìš°ë¦¬ê°€ ë¯¸ë‹ˆë°°ì¹˜(mini-batch)ì—ì„œ ì˜¤ëŠ”  
ì²« ë²ˆì§¸ í™•ë¥ ì„±(stochasticity) ì›ì²œê³¼,  
ê¸°ëŒ“ê°’ì˜ ëª¬í…Œì¹´ë¥¼ë¡œ ê·¼ì‚¬(Monte Carlo approximation)ì—ì„œ ì˜¤ëŠ”  
ë‘ ë²ˆì§¸ í™•ë¥ ì„± ì›ì²œì„ ëª¨ë‘ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤.

---

ìš°ë¦¬ëŠ” ì—°ì†ì ì¸ ì ì¬ ë³€ìˆ˜(continuous latent variables)ë¥¼ ê°€ì§„ ëª¨ë¸ë“¤ì— ì´ˆì ì„ ë§ì¶˜ë‹¤.  
ì´ ì ‘ê·¼ë²•ì—ì„œ í•„ìš”í•œ ê·¸ë˜ë””ì–¸íŠ¸(gradients)ëŠ”  
ë¹„ì¤‘ì‹¬(non-centered) ì¬ë§¤ê°œë³€ìˆ˜í™”(reparameterization)ëœ ê¸°ëŒ“ê°’(expectation)ì˜ í˜•íƒœë¡œ ê³„ì‚°ë˜ë©°  
(Papaspiliopoulos et al., 2003; Williams, 1992),  
ëª¬í…Œì¹´ë¥¼ë¡œ ê·¼ì‚¬(Monte Carlo approximation)ì™€ ê²°í•©ëœë‹¤.  
ì´ ë°©ë²•ì€ í™•ë¥ ì  ì—­ì „íŒŒ(stochastic backpropagation) (Rezende et al., 2014)ë¼ ë¶ˆë¦°ë‹¤.  

ì´ ì ‘ê·¼ë²•ì€ ë˜í•œ í™•ë¥ ì  ê·¸ë˜ë””ì–¸íŠ¸ ë³€ë¶„ ë² ì´ì¦ˆ(stochastic gradient variational Bayes, SGVB)  
(Kingma & Welling, 2014) ë˜ëŠ”  
ì•„í•€ ë³€ë¶„ ì¶”ë¡ (affine variational inference) (Challis & Barber, 2012)ì´ë¼ ë¶ˆë¦¬ê¸°ë„ í•œë‹¤.  

í™•ë¥ ì  ì—­ì „íŒŒ(stochastic backpropagation)ëŠ” ë‘ ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§„ë‹¤.  

- **ì¬ë§¤ê°œë³€ìˆ˜í™” (Reparameterization)**  
   ìš°ë¦¬ëŠ” ì•Œë ¤ì§„ ê¸°ë³¸ ë¶„í¬(base distribution)ì™€  
   ë¯¸ë¶„ ê°€ëŠ¥í•œ ë³€í™˜(differentiable transformation, ì˜ˆ: ìœ„ì¹˜-ìŠ¤ì¼€ì¼(location-scale) ë³€í™˜   
   ë˜ëŠ” ëˆ„ì  ë¶„í¬ í•¨ìˆ˜(cumulative distribution function))ì„ ì´ìš©í•˜ì—¬  
   ì ì¬ ë³€ìˆ˜(latent variable)ë¥¼ ì¬ë§¤ê°œë³€ìˆ˜í™”í•œë‹¤.  

   ì˜ˆë¥¼ ë“¤ì–´, ë§Œì•½ $q_\phi(\mathbf{z})$ ê°€  
   $\mathcal{N}(\mathbf{z} \mid \mu, \sigma^2)$ ì¸ ê°€ìš°ì‹œì•ˆ ë¶„í¬(Gaussian distribution)ì´ê³   
   $\phi = \lbrace \mu, \sigma^2\rbrace$ ë¼ë©´,  
   ìœ„ì¹˜-ìŠ¤ì¼€ì¼(location-scale) ë³€í™˜ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:  

   $$
   \mathbf{z} \sim \mathcal{N}(\mathbf{z} \mid \mu, \sigma^2)
   \quad \Leftrightarrow \quad
   \mathbf{z} = \mu + \sigma \boldsymbol{\epsilon}, 
   \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, 1)
   $$

- **ëª¬í…Œì¹´ë¥¼ë¡œë¥¼ ì´ìš©í•œ ì—­ì „íŒŒ (Backpropagation with Monte Carlo)**  
   ì´ì œ ìš°ë¦¬ëŠ” ê¸°ë³¸ ë¶„í¬(base distribution)ë¡œë¶€í„°ì˜ ìƒ˜í”Œ(draws)ì„ ì‚¬ìš©í•œ  
   ëª¬í…Œì¹´ë¥¼ë¡œ ê·¼ì‚¬(Monte Carlo approximation)ë¥¼ í†µí•´,  
   ë³€ë¶„ ë¶„í¬(variational distribution)ì˜ íŒŒë¼ë¯¸í„° $\phi$ ì— ëŒ€í•´  
   ë¯¸ë¶„(ì—­ì „íŒŒ, backpropagation)ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤:   

   ì¦‰,

   $$
   \nabla_\phi \mathbb{E}_{q_\phi(\mathbf{z})}[f_\theta(\mathbf{z})]
   \quad \Leftrightarrow \quad
   \mathbb{E}_{\mathcal{N}(\epsilon \mid 0,1)}
   \big[\nabla_\phi f_\theta(\mu + \sigma \epsilon)\big]
   $$

   ìœ¼ë¡œ í‘œí˜„ëœë‹¤.

---

ëª¬í…Œì¹´ë¥¼ë¡œ ì œì–´ ë³€ìˆ˜(Monte Carlo control variate, MCCV) ì¶”ì •ê¸°(estimator)ì— ê¸°ë°˜í•œ  
ì—¬ëŸ¬ ë²”ìš©(general purpose) ì ‘ê·¼ë²•ë“¤ì´  
í™•ë¥ ì  ì—­ì „íŒŒ(stochastic backpropagation)ì˜ ëŒ€ì•ˆìœ¼ë¡œ ì¡´ì¬í•˜ë©°,  
ì—°ì†ì ì´ê±°ë‚˜(discrete) ë¶ˆì—°ì†ì ì¸(latent variables that may be continuous or discrete)  
ì ì¬ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤  
(Williams, 1992; Mnih & Gregor, 2014; Ranganath et al., 2013; Wingate & Weber, 2013).  

í™•ë¥ ì  ì—­ì „íŒŒì˜ ì¤‘ìš”í•œ ì¥ì  ì¤‘ í•˜ë‚˜ëŠ”,  
ì—°ì†ì ì¸ ì ì¬ ë³€ìˆ˜ë¥¼ ê°€ì§„ ëª¨ë¸ì˜ ê²½ìš°,  
ê²½ìŸí•˜ëŠ” ë‹¤ë¥¸ ì¶”ì •ê¸°ë“¤ ì¤‘ì—ì„œ ë¶„ì‚°(variance)ì´ ê°€ì¥ ë‚®ë‹¤ëŠ” ê²ƒì´ë‹¤.

---

### 2.2 ì¶”ë¡  ë„¤íŠ¸ì›Œí¬ (Inference Networks)

ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”í•œ êµ¬í˜„ ì‚¬í•­ì€,  
ê·¼ì‚¬ ì‚¬í›„ë¶„í¬(approximate posterior distribution) $q_\phi(\cdot)$ ê°€  
ì¸ì‹ ëª¨ë¸(recognition model) ë˜ëŠ” ì¶”ë¡  ë„¤íŠ¸ì›Œí¬(inference network)ë¥¼ ì‚¬ìš©í•˜ì—¬  
í‘œí˜„ëœë‹¤ëŠ” ê²ƒì´ë‹¤ (Rezende et al., 2014; Dayan, 2000; Gershman & Goodman, 2014; Kingma & Welling, 2014).  

ì¶”ë¡  ë„¤íŠ¸ì›Œí¬ë€ ê´€ì¸¡ê°’(observations)ìœ¼ë¡œë¶€í„° ì ì¬ ë³€ìˆ˜(latent variables)ë¡œì˜  
ì—­í•¨ìˆ˜(inverse map)ë¥¼ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì´ë‹¤.  

ì¶”ë¡  ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨,  
ê° ë°ì´í„° í¬ì¸íŠ¸ë§ˆë‹¤ ê°œë³„ì ì¸ ë³€ë¶„ íŒŒë¼ë¯¸í„°(variational parameters)ë¥¼  
ê³„ì‚°í•  í•„ìš”ê°€ ì—†ê²Œ ë˜ë©°,  
ëŒ€ì‹  í•™ìŠµ(training)ê³¼ í…ŒìŠ¤íŠ¸(test) ëª¨ë‘ì—ì„œ ìœ íš¨í•œ  
ê¸€ë¡œë²Œ ë³€ë¶„ íŒŒë¼ë¯¸í„°(global variational parameters) $\phi$ ì˜ ì§‘í•©ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.  

ì´ ì ‘ê·¼ë²•ì€ ì¶”ë¡  ë„¤íŠ¸ì›Œí¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´  
ëª¨ë“  ì ì¬ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ì‚¬í›„ë¶„í¬ ì¶”ì •(posterior estimates)ì„ ì¼ë°˜í™”í•¨ìœ¼ë¡œì¨,  
ì¶”ë¡ (inference)ì˜ ë¹„ìš©ì„ ìƒê°(amortize)í•  ìˆ˜ ìˆë„ë¡ í•´ì¤€ë‹¤.  

ìš°ë¦¬ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ì¶”ë¡  ëª¨ë¸ì€  
ëŒ€ê° ê°€ìš°ì‹œì•ˆ ë¶„í¬(diagonal Gaussian densities)ì´ë‹¤.  

$$
q_\phi(\mathbf{z} \mid \mathbf{x}) = 
\mathcal{N}\!\big(\mathbf{z} \mid 
\mu_\phi(\mathbf{x}), 
\mathrm{diag}(\sigma_\phi^2(\mathbf{x}))\big)
$$

ì—¬ê¸°ì„œ í‰ê· (mean) í•¨ìˆ˜ $\mu_\phi(\mathbf{x})$ ì™€  
í‘œì¤€í¸ì°¨(standard deviation) í•¨ìˆ˜ $\sigma_\phi(\mathbf{x})$ ëŠ”  
ì‹¬ì¸µ ì‹ ê²½ë§(deep neural networks)ì„ ì´ìš©í•˜ì—¬ ì •ì˜ëœë‹¤.

---

## 2.3 ì‹¬ì¸µ ì ì¬ ê°€ìš°ì‹œì•ˆ ëª¨ë¸ (Deep Latent Gaussian Models)

ì´ ë…¼ë¬¸ì—ì„œëŠ” ì‹¬ì¸µ ì ì¬ ê°€ìš°ì‹œì•ˆ ëª¨ë¸(Deep Latent Gaussian Models, DLGM)ì„ ì—°êµ¬í•œë‹¤.  
DLGMì€ ì‹¬ì¸µ ìœ í–¥ ê·¸ë˜í”„ ëª¨ë¸(deep directed graphical models)ì˜ ì¼ë°˜ì ì¸ ë²”ì£¼ë¡œ,  
ê°€ìš°ì‹œì•ˆ ì ì¬ ë³€ìˆ˜(Gaussian latent variables)ì˜ $L$ ê°œ ì¸µ(layer)ìœ¼ë¡œ ì´ë£¨ì–´ì§„  
ê³„ì¸µì  êµ¬ì¡°(hierarchy)ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.  
ê° ì¸µ $l$ ì—ëŠ” ì ì¬ ë³€ìˆ˜ $\mathbf{z}_l$ ì´ ì¡´ì¬í•œë‹¤.  

ê° ê³„ì¸µì˜ ì ì¬ ë³€ìˆ˜ëŠ” ê·¸ ìœ„ ê³„ì¸µì˜ ë³€ìˆ˜ì— ë¹„ì„ í˜•ì ìœ¼ë¡œ(non-linearly) ì˜ì¡´í•˜ë©°,  
DLGMì˜ ê²½ìš° ì´ëŸ¬í•œ ë¹„ì„ í˜• ì˜ì¡´ì„±ì€ ì‹¬ì¸µ ì‹ ê²½ë§(deep neural networks)ì— ì˜í•´ ì •ì˜ëœë‹¤.  

ê²°í•© í™•ë¥  ëª¨ë¸(joint probability model)ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
p(\mathbf{x}, \mathbf{z}_1, \ldots, \mathbf{z}_L)
= p(\mathbf{x} \mid f_0(\mathbf{z}_1)) 
\prod_{l=1}^{L} p(\mathbf{z}_l \mid f_l(\mathbf{z}_{l+1}))
\tag{4}
$$  

> ì‹ (4)ëŠ” ì‹¬ì¸µ ì ì¬ ê°€ìš°ì‹œì•ˆ ëª¨ë¸(DLGM)ì˜ ê²°í•© í™•ë¥  êµ¬ì¡°ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.  
> ì´ë¥¼ í’€ì–´ ì“°ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
>  
> $$
> p(\mathbf{x}, \mathbf{z}_1, \ldots, \mathbf{z}_L)
> = p(\mathbf{x} \mid f_0(\mathbf{z}_1)) 
> \cdot p(\mathbf{z}_1 \mid f_1(\mathbf{z}_2))
> \cdot \ldots
> \cdot p(\mathbf{z}_{L-1} \mid f_{L-1}(\mathbf{z}_L))
> \cdot p(\mathbf{z}_L)
> $$  
>  
> ì¦‰, ìµœìƒìœ„ ê³„ì¸µì˜ ì ì¬ ë³€ìˆ˜ $$\mathbf{z}_L$$ ì€ ì‚¬ì „ë¶„í¬ $$p(\mathbf{z}_L)$$ ë¡œë¶€í„° ìƒ˜í”Œë§ë˜ë©°,  
> ê° í•˜ìœ„ ê³„ì¸µì˜ ì ì¬ ë³€ìˆ˜ $$\mathbf{z}_l$$ ì€  
> ìƒìœ„ ê³„ì¸µì˜ ì ì¬ ë³€ìˆ˜ $$\mathbf{z}_{l+1}$$ ì˜ ë¹„ì„ í˜• ë³€í™˜ $$f_l(\mathbf{z}_{l+1})$$ ì— ì¡°ê±´ë¶€ë¡œ ìƒì„±ëœë‹¤.  
>  
> ë§ˆì§€ë§‰ìœ¼ë¡œ ê´€ì¸¡ ë°ì´í„° $$\mathbf{x}$$ ëŠ”  
> ê°€ì¥ í•˜ìœ„ ì ì¬ ë³€ìˆ˜ $$\mathbf{z}_1$$ ì˜ ë³€í™˜ $$f_0(\mathbf{z}_1)$$ ì— ì˜í•´ ìƒì„±ëœë‹¤.  

ì—¬ê¸°ì„œ $L$ë²ˆì§¸ ê°€ìš°ì‹œì•ˆ ë¶„í¬ëŠ” ë‹¤ë¥¸ ì–´ë–¤ í™•ë¥  ë³€ìˆ˜ì—ë„ ì˜ì¡´í•˜ì§€ ì•ŠëŠ”ë‹¤.  
ì ì¬ ë³€ìˆ˜ì— ëŒ€í•œ ì‚¬ì „ë¶„í¬(prior)ëŠ” ë‹¨ìœ„ ê°€ìš°ì‹œì•ˆ(unit Gaussian)ìœ¼ë¡œ,  
$$p(\mathbf{z}_l) = \mathcal{N}(0, \mathbf{I})$$ ì´ë‹¤.  
ê´€ì¸¡ê°’ì— ëŒ€í•œ ìš°ë„í•¨ìˆ˜(likelihood) $$p_\theta(\mathbf{x} \mid \mathbf{z})$$ ëŠ”  
$$\mathbf{z}_1$$ ì— ì¡°ê±´ë¶€ë¡œ ì£¼ì–´ì§€ë©°(conditioned on $$\mathbf{z}_1$$),  
ì‹¬ì¸µ ì‹ ê²½ë§ì— ì˜í•´ íŒŒë¼ë¯¸í„°í™”(parameterized)ëœë‹¤ (ê·¸ë¦¼ 2).  

ì´ ëª¨ë¸ í´ë˜ìŠ¤(model class)ëŠ” ë§¤ìš° ì¼ë°˜ì ì´ë©°,  
ìš”ì¸ ë¶„ì„(factor analysis)ê³¼ ì£¼ì„±ë¶„ ë¶„ì„(PCA),  
ë¹„ì„ í˜• ìš”ì¸ ë¶„ì„(non-linear factor analysis),  
ê·¸ë¦¬ê³  ë¹„ì„ í˜• ê°€ìš°ì‹œì•ˆ ì‹ ë…ë§(non-linear Gaussian belief networks)  
ë“±ì„ íŠ¹ìˆ˜í•œ ê²½ìš°ë¡œ í¬í•¨í•œë‹¤ (Rezende et al., 2014).

> DLGMì€ ì ì¬ ë³€ìˆ˜ $\mathbf{z}$ ë¥¼ ì´ìš©í•´ ê´€ì¸¡ ë°ì´í„° $\mathbf{x}$ ë¥¼ ìƒì„±í•˜ëŠ”  
> ìœ í–¥ í™•ë¥  ëª¨ë¸(directed probabilistic model) ì˜ í•œ í˜•íƒœì´ë‹¤.  
>  
> PCA(ì£¼ì„±ë¶„ ë¶„ì„)ë‚˜ ìš”ì¸ ë¶„ì„(Factor Analysis)ì€  
> ëª¨ë‘ ì ì¬ ë³€ìˆ˜ ëª¨ë¸(latent variable model) ë¡œ,  
> ì„ í˜• ë³€í™˜(linear transformation)ì„ í†µí•´  
> $\mathbf{x}$ ì™€ $\mathbf{z}$ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•œë‹¤.  
>  
> ì˜ˆë¥¼ ë“¤ì–´,  
>
> $$
> \mathbf{x} = \mathbf{Wz} + \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \sigma^2 \mathbf{I})
> $$
>
> ì™€ ê°™ì€ í˜•íƒœê°€ PCA/FAì˜ ê¸°ë³¸ êµ¬ì¡°ì´ë‹¤.  
>  
> ë°˜ë©´, DLGMì—ì„œëŠ” ì´ ì„ í˜• ë³€í™˜ì„ ë¹„ì„ í˜• í•¨ìˆ˜ $f_l(\cdot)$ (ì˜ˆ: ì‹ ê²½ë§)ìœ¼ë¡œ ì¼ë°˜í™”í•˜ê³ ,  
> ì—¬ëŸ¬ ê³„ì¸µ(layer)ì˜ ì ì¬ ë³€ìˆ˜ë¥¼ í¬í•¨í•˜ì—¬  
> ë³µì¡í•œ ë°ì´í„° ë¶„í¬ë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆë‹¤.  
>  
> ë”°ë¼ì„œ DLGMì€  
> - ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” PCA, ìš”ì¸ ë¶„ì„(FA)  
> - ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë¹„ì„ í˜• ìš”ì¸ ë¶„ì„(NLFA), ê°€ìš°ì‹œì•ˆ ì‹ ë…ë§(GBN)  
> ì„ ëª¨ë‘ í•˜ìœ„ íŠ¹ìˆ˜í•œ ê²½ìš°(special cases) ë¡œ í¬í•¨í•˜ëŠ”  
> ë³´ë‹¤ ì¼ë°˜ì ì¸(latent hierarchical & nonlinear) ëª¨ë¸ì´ë‹¤.
