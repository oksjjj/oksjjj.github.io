---
layout: post
title: "[논문] LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS"
date: 2025-10-04 15:00:00 +0900
categories:
  - "논문"
tags: []
---

> **논문 출처**  
> Hu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W.  
> *LoRA: Low-Rank Adaptation of Large Language Models*.  
> arXiv preprint (arXiv:2106.09685), Version 2.  
> <a href="https://arxiv.org/abs/2106.09685" target="_blank">🔗 원문 링크 (arXiv:2106.09685)</a>

**저자**  
- Edward Hu (Microsoft Corporation) – edwardhu@microsoft.com  
- Yelong Shen (Microsoft Corporation) – yeshe@microsoft.com  
- Phillip Wallis (Microsoft Corporation) – phwallis@microsoft.com  
- Zeyuan Allen-Zhu (Microsoft Corporation) – zeyuana@microsoft.com  
- Yuanzhi Li (Carnegie Mellon University) – yuanzhil@andrew.cmu.edu  
- Shean Wang (Microsoft Corporation) – swang@microsoft.com  
- Lu Wang (Microsoft Corporation) – luw@microsoft.com  
- Weizhu Chen (Microsoft Corporation) – wzchen@microsoft.com  

---

**주석**  

∗ 공동 기여(Equal contribution).

---

(Version 2)

---

**주석**  

V1과 비교했을 때, 이번 초안에는 더 나은 **베이스라인(baselines)**,  
**GLUE 실험**, 그리고 **어댑터 지연(adapter latency)** 에 대한  
추가 내용이 포함되어 있다.

---

## 초록 (Abstract)  

자연어 처리(NLP)의 주요 패러다임 중 하나는  
**일반 도메인 데이터에 대한 대규모 사전학습(pre-training)** 과  
**특정 작업이나 도메인에 대한 적응(adaptation)** 으로 구성된다.  

하지만 모델이 커질수록,  
모든 파라미터를 다시 학습시키는 **전체 미세조정(full fine-tuning)** 은  
점점 더 비현실적이 된다.  

예를 들어, **GPT-3 (175B)** 모델의 경우,  
각각 1,750억 개의 파라미터를 가진 미세조정된 모델 인스턴스를  
독립적으로 배포하는 것은 **막대한 비용**이 든다.  

이를 해결하기 위해 우리는 **LoRA (Low-Rank Adaptation)** 를 제안한다.  

LoRA는 **사전학습된 모델의 가중치를 고정(freeze)** 한 채,  
**트랜스포머(Transformer) 아키텍처의 각 층(layer)** 에  
**학습 가능한 저랭크 분해 행렬(rank decomposition matrices)** 을 삽입함으로써,  
**전이 학습(transfer learning)** 시 필요한 **학습 가능한 파라미터 수를 크게 줄인다.**  

---

> **(블로그 추가 설명) 전이 학습(Transfer Learning)이란?**  
> - **전이 학습(Transfer Learning)** 은  
>   하나의 작업(task)이나 도메인(domain)에서 학습된 지식을  
>   **다른 작업으로 이전(transfer)** 하여 활용하는 학습 방법이다.  
> 
> - 일반적으로 **대규모 데이터로 사전학습(pre-training)** 된 모델은  
>   언어, 문맥, 구조 등에 대한 **일반적인 표현(representation)** 을 이미 학습하고 있다.  
>   이러한 모델을 새로운 응용 문제(예: 감정 분석, 질의응답, 번역 등)에 적용할 때는  
>   모델의 **일부 파라미터만 조정**하거나,  
>   **특정 층(layer)만 미세조정(fine-tuning)** 하는 방식으로 수행된다.  
> 
> - 따라서 처음부터 모델을 다시 학습시키지 않고,  
>   **이미 학습된 지식(knowledge)** 을 바탕으로  
>   **새로운 작업에 빠르게 적응(adapt)** 하는 것이  
>   전이 학습의 핵심 개념이다.  
> 
> - LoRA 역시 이러한 전이 학습의 한 형태로,  
>   전체 모델을 다시 학습하지 않고  
>   **필요한 부분만 효율적으로 조정**하여 성능을 유지한다.

---

GPT-3 (175B)를 Adam 옵티마이저로 미세조정했을 때와 비교하면,  
LoRA는 **학습 가능한 파라미터 수를 최대 10,000배까지 줄이고**,  
**GPU 메모리 요구량을 약 3배 절감**할 수 있다.  

그럼에도 불구하고, LoRA는  
RoBERTa, DeBERTa, GPT-2, GPT-3 등의 모델에서  
**미세조정(full fine-tuning)과 동등하거나 더 우수한 성능**을 보인다.  

또한, 학습 가능한 파라미터가 더 적고,  
**학습 처리량(training throughput)** 이 더 높으며,  
**어댑터(adapters)** 방식과 달리  
**추론(inference) 시 지연(latency)** 도 추가되지 않는다.  

---

> **(블로그 추가 설명) 어댑터(Adapter) 방식이란?**  
> - **어댑터(Adapter)** 는 대형 사전학습 모델(pre-trained model)의  
>   모든 파라미터를 다시 학습시키지 않고,  
>   **각 층(layer)** 사이에 **작은 학습 가능한 모듈(small trainable modules)** 을 추가하여  
>   새로운 작업에 맞게 모델을 미세조정(fine-tuning)하는 방법이다.  
> 
> - 이 방식은 원래의 모델 파라미터를 거의 그대로 유지한 채,  
>   **적은 추가 파라미터(parameter-efficient)** 만 학습할 수 있다는 장점이 있다.  
>   따라서 전체 모델을 다시 학습시키는 것보다 훨씬 효율적이며,  
>   서로 다른 작업 간에 **공유(reuse)** 도 용이하다.  
> 
> - 그러나 어댑터는 **추론(inference)** 단계에서  
>   각 층에 추가된 모듈을 통과해야 하므로,  
>   **추가적인 지연(latency)** 이 발생할 수 있다는 단점이 있다.  
>
> 어댑터 방식은 “모델 전체를 다시 학습하지 않고 필요한 부분만 추가 학습하는”  
> **파라미터 효율적 미세조정(Parameter-Efficient Fine-Tuning, PEFT)** 기법이다.  
> 하지만 **LoRA는 어댑터와 달리 추론 시 지연이 거의 없고**,  
> 동일한 효율성을 유지하면서 더 단순한 구조를 제공한다.

---

아울러 우리는  
**언어 모델 적응에서의 랭크 결핍(rank-deficiency)** 현상에 대한  
실증적 분석을 제공하여,  
**LoRA의 효율성에 대한 통찰(insight)** 을 제시한다.  

---

> **(블로그 추가 설명) 왜 ‘랭크 결핍(Rank-Deficiency)’이라고 부를까?**  
> - “랭크 결핍”은 원래 선형대수학에서 **행렬의 랭크(rank)** 가  
>   최대치보다 낮을 때, 즉 **선형 독립 성분이 부족할 때**를 의미하는 **부정적 용어**이다.  
>   예를 들어, $n \times n$ 행렬의 랭크가 $n$보다 작다면  
>   그 행렬은 **정보 손실(loss of information)** 이 있거나,  
>   **역행렬이 존재하지 않는(singular)** 상태로 본다.  
> 
> - 하지만 언어 모델의 관점에서는 이 “결핍”이 반드시 나쁜 것이 아니다.  
>   오히려 모델이 실제로는 **낮은 차원(low-rank)** 의 표현만으로도  
>   충분히 의미 있는 정보를 학습하고,  
>   복잡한 패턴을 표현할 수 있음을 보여주는 **구조적 효율성(structural efficiency)** 의 신호이기도 하다.  
> 
> - LoRA는 바로 이 특성을 적극적으로 이용한다.  
>   즉, 언어 모델이 본질적으로 **랭크 결핍 구조를 가진다**는 점을 활용하여,  
>   전체 파라미터를 모두 학습하지 않고도  
>   **필요한 저차원 방향만 학습함으로써 동일한 효과를 얻는다.**  
> 
> - 따라서 “랭크 결핍”이라는 단어는 수학적으로는 제약(constraint)을 뜻하지만,  
>   LoRA의 맥락에서는 **“낮은 차원 구조로도 충분히 잘 작동하는 현상”** 이라는  
>   **긍정적인 의미로 재해석**된다.

---

마지막으로, 우리는  
**PyTorch 모델에 LoRA를 통합할 수 있는 패키지**를 공개하였으며,  
RoBERTa, DeBERTa, GPT-2용 **구현 및 모델 체크포인트**를  
다음 링크에서 제공한다.  

<a href="https://github.com/microsoft/LoRA" target="_blank">🔗 https://github.com/microsoft/LoRA</a>


## 1. 서론 (Introduction)  

자연어 처리(NLP)의 많은 응용들은  
**하나의 대규모 사전학습 언어 모델(pre-trained language model)** 을  
여러 개의 **전이(또는 하위) 응용 프로그램(downstream applications)** 에  
적응시키는 것에 의존한다.  

이러한 적응(adaptation)은 일반적으로  
**모든 사전학습된 모델의 파라미터를 업데이트하는 미세조정(fine-tuning)** 을 통해 수행된다.  

그러나 미세조정의 주요 단점은,  
**새로운 모델이 원래 모델과 동일한 수의 파라미터를 갖게 된다는 점**이다.  

모델 규모가 몇 달마다 더 커지는 현 상황에서,  
이 문제는 단순히 GPT-2 (Radford et al.)나 RoBERTa large (Liu et al., 2019) 수준에서는  
“약간의 불편함”에 불과할 수 있었지만,  
**1,750억 개의 학습 가능한 파라미터를 가진 GPT-3 (Brown et al., 2020)** 의 경우에는  
**중대한 배포(deployment)상의 도전 과제**로 변하게 되었다.  

---

**주석**  

GPT-3 (175B)는 **퓨샷 학습(few-shot learning)** 만으로도  
일정 수준 이상의 성능을 달성하지만,  
**부록 A(Appendix A)** 에서 보여지듯이  
**미세조정(fine-tuning)** 을 통해 그 성능이 크게 향상된다.

---

많은 연구자들이 이 문제를 완화하기 위해  
**일부 파라미터만 적응(adapt)** 시키거나,  
**새로운 작업(task)** 을 위해 **외부 모듈(external modules)** 을 학습하는 방법을 시도해왔다.  

이러한 방식에서는 각 작업마다  
사전학습된 모델에 더해 **작업별 파라미터(task-specific parameters)** 만  
저장하고 불러오면 되므로,  
배포(deployment) 시 **운영 효율성(operational efficiency)** 이 크게 향상된다.  

그러나 기존 기법들은 종종  
**모델의 깊이(model depth)** 를 확장함으로써 **추론 지연(inference latency)** 을 유발하거나  
(Loulsby et al., 2019; Rebuffi et al., 2017),  
**모델이 처리할 수 있는 시퀀스 길이(sequence length)** 를 줄이는 문제를 발생시킨다  
(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021) (3장 참조).  

더 중요한 점은, 이러한 방법들이 종종  
**미세조정(fine-tuning) 기반의 베이스라인(baseline)** 수준의 성능을 달성하지 못해,  
**효율성과 모델 품질 간의 트레이드오프(trade-off)** 를 초래한다는 것이다.

우리는 Li et al. (2018a) 및 Aghajanyan et al. (2020)에서 제시된 결과로부터 영감을 얻었다.  
이들 연구는 **과도하게 매개변수화된(over-parameterized)** 모델이 실제로는  
**낮은 고유 차원(low intrinsic dimension)** 상에 존재함을 보여준다.  

이에 기반하여 우리는,  
모델 적응(model adaptation) 과정에서 발생하는 **가중치 변화(weight change)** 역시  
낮은 **내재적 랭크(intrinsic rank)** 를 가진다고 가정(hypothesis)한다.  
이 가정으로부터 우리가 제안한 **LoRA (Low-Rank Adaptation)** 방법이 도출된다.  

LoRA는 사전학습된 가중치를 그대로 **고정(freeze)** 시킨 채,  
**적응 과정에서의 밀집층(dense layer) 변화량**을  
**랭크 분해 행렬(rank decomposition matrices)** 형태로 학습하도록 설계된다.  
즉, 밀집층 자체를 직접 학습하는 대신,  
그 변화(change)를 저랭크 행렬 형태로 간접적으로 최적화한다는 것이다  
(그림 1 참고).  

---

그림 1: 우리의 **재매개변수화(reparametrization)** 방식.  
우리는 **A와 B만 학습한다.**

<img src="/assets/img/paper/lora/image_1.png" alt="image" width="360px"> 

---

> **(블로그 추가 설명) 재매개변수화(Reparametrization)와 랭크 분해 행렬 학습이란?**  
> - **재매개변수화(Reparametrization)** 는  
>   기존의 가중치 행렬 $W$ 를 직접 학습하지 않고,  
>   그것을 **다른 형태의 파라미터(예: $A$, $B$)** 로 다시 표현하여 학습하는 방법이다.  
>   즉, $W$ 대신 **$W + \Delta W$** 형태로 모델을 표현하고,  
>   이때 **$\Delta W = B A$** 로 두어 $A$, $B$ 두 행렬만 학습한다.  
> 
> - 이렇게 하면, 원래의 거대한 가중치 행렬 $W \in \mathbb{R}^{d \times d}$ 을  
>   직접 업데이트할 필요가 없다.  
>   대신, **저랭크(rank $r$)** 제약을 가진 두 행렬 $A \in \mathbb{R}^{r \times d}$,  
>   $B \in \mathbb{R}^{d \times r}$ 을 통해  
>   $W$ 의 변화를 근사(approximation)하게 된다.  
> 
> - 그림에서처럼, $A$ 는 초기값을 **정규분포 $\mathcal{N}(0, \sigma^2)$** 로 설정하고,  
>   $B$ 는 **0으로 초기화**된다.  
>   학습 과정에서는 $A$와 $B$만 최적화되고,  
>   원래의 사전학습 가중치 $W$ 는 **고정(freeze)** 되어 유지된다.  
> 
> - 이러한 방식은 모델의 전체 구조를 바꾸지 않으면서도  
>   **적은 수의 학습 파라미터로 동일한 효과를 얻을 수 있게 해주며**,  
>   이를 통해 **저장 공간과 계산량을 크게 절약**할 수 있다.  
> 
> - 요약하자면, LoRA의 재매개변수화는  
>   거대한 파라미터 행렬을 저차원(rank-$r$) 공간으로 분해해  
>   효율적으로 학습하는 **저랭크 근사(low-rank approximation)** 기반의 접근법이다.

---

GPT-3 (175B)를 예로 들면,  
전체 랭크($d$)가 12,288에 달하더라도  
**매우 낮은 랭크(예: $r=1$ 또는 $r=2$)** 만으로 충분함을 보여준다.  
이는 LoRA가 **저장(storage)** 및 **계산(compute)** 양 측면에서  
매우 효율적인 방법임을 의미한다.

LoRA는 여러 가지 핵심적인 이점을 가진다.  

- **사전학습된 모델(pre-trained model)** 은 여러 다른 작업(tasks)을 위한  
  작은 LoRA 모듈들을 구축하는 데 공유되고 사용될 수 있다.  
  우리는 공유된 모델을 고정(freeze)한 상태로 유지하고,  
  그림 1의 행렬 A와 B를 교체함으로써 작업을 효율적으로 전환할 수 있다.  
  이는 저장 공간 요구량(storage requirement)과  
  작업 전환 시 오버헤드(task-switching overhead)를 크게 줄여준다.  

- LoRA는 학습을 더욱 효율적으로 만들고,  
  적응형 옵티마이저(adaptive optimizer)를 사용할 때  
  하드웨어 진입 장벽(hardware barrier to entry)을 최대 3배까지 낮춘다.  
  이는 대부분의 파라미터에 대해 그래디언트를 계산하거나  
  옵티마이저 상태(optimizer state)를 유지할 필요가 없기 때문이다.  
  대신, 우리는 삽입된 **훨씬 작은 저랭크 행렬(low-rank matrices)** 만 최적화한다.  

- 우리의 단순한 선형(linear) 설계는,  
  배포(deployment) 시 학습 가능한 행렬(trainable matrices)을  
  고정된 가중치(frozen weights)와 병합(merge)할 수 있게 하며,  
  완전히 미세조정된 모델(fully fine-tuned model)과 비교했을 때  
  **추론 지연(inference latency)** 을 전혀 초래하지 않는다.  

- LoRA는 많은 기존 기법들과 **직교적(orthogonal)** 이며,  
  **프리픽스 튜닝(prefix-tuning)** 과 같은  
  여러 방법들과 결합될 수 있다.  
  이에 대한 예시는 **부록 E(Appendix E)** 에서 제시한다.

---

> **(블로그 추가 설명) ‘직교적(Orthogonal)’이라는 표현의 의미**  
> - 여기서 **직교적(orthogonal)** 이라는 표현은  
>   수학적 의미의 “서로 독립적(independent)”이라는 개념에서 유래한다.  
>   즉, **LoRA가 다른 방법들과 간섭하지 않고 동시에 사용할 수 있다** 는 뜻이다.  
> 
> - 어떤 기법이 직교적이라는 것은  
>   그것이 다른 학습 기법의 구조나 원리에 의존하지 않으며,  
>   **별도로 적용하거나 함께 결합(combine)** 해도  
>   서로의 작동 방식에 영향을 주지 않는다는 의미이다.  
> 
> - 따라서 LoRA는 **프리픽스 튜닝(prefix-tuning)**, **프로프트 튜닝(prompt tuning)**,  
>   **어댑터(adapter)** 등과 같은 다른 **파라미터 효율적 학습 기법(PEFT)** 들과  
>   함께 사용될 수 있으며, 이러한 결합은  
>   **추가적인 성능 향상이나 효율성 개선**을 가능하게 한다.

---

### **용어 및 표기 규칙 (Terminologies and Conventions)**  

우리는 **트랜스포머(Transformer) 아키텍처**를 자주 참조하며,  
그 구조에서 일반적으로 사용되는 용어와 표기 규칙(conventions)을 따른다.  

트랜스포머 층의 **입력 및 출력 차원 크기**를 $$d_{\text{model}}$$이라고 부른다.  

$$W_q, \; W_k, \; W_v, \; W_o$$는 각각 **셀프-어텐션(self-attention)** 모듈 내의  
**쿼리(query)**, **키(key)**, **밸류(value)**, **출력(output)** 투영 행렬을 의미한다.  

$$W \; \text{또는} \; W_0$$는 **사전학습된(pre-trained)** 가중치 행렬을,  
$$\Delta W$$는 **적응(adaptation)** 중 누적된 **그래디언트 업데이트(accumulated gradient update)** 를 나타낸다.  

$$r$$은 **LoRA 모듈의 랭크(rank)** 를 의미한다.  

우리는 **Vaswani et al. (2017)** 및 **Brown et al. (2020)** 의 표준 규칙을 따르며,  
모델 최적화(model optimization)에는 **Adam 옵티마이저**  
(**Loshchilov & Hutter, 2019; Kingma & Ba, 2017**)를 사용한다.  

또한, 트랜스포머의 **MLP 피드포워드(feedforward) 차원**은  
$$d_{\text{ffn}} = 4 \times d_{\text{model}}$$로 설정한다.


## 2. 문제 정의 (Problem Statement)  

우리의 제안은 **학습 목표(training objective)** 에 구애받지 않지만,  
본 연구에서는 **언어 모델링(language modeling)** 을 주요 동기 사례로 삼는다.  
아래는 언어 모델링 문제에 대한 간단한 설명이며,  
특히 **작업별 프롬프트(task-specific prompt)** 가 주어졌을 때  
**조건부 확률(conditional probability)** 을 최대화하는 과정을 다룬다.  

사전학습된 **자가회귀 언어 모델(autoregressive language model)**  
$P_\Phi(y|x)$ 이 주어졌다고 가정하자.  
이 모델은 매개변수 **Φ**로 정의된다.  

예를 들어, $P_\Phi(y|x)$ 는 **GPT (Radford et al.; Brown et al., 2020)** 과 같은  
**트랜스포머(Transformer)** 기반의 **일반적 다중작업 학습기(generic multi-task learner)** 일 수 있다  
(Vaswani et al., 2017).  

이제 이 사전학습된 모델을  
**요약(summarization)**, **기계 독해(MRC, Machine Reading Comprehension)**,  
**자연어 → SQL 변환(NL2SQL)** 과 같은  
**조건부 텍스트 생성(conditional text generation)** 작업에 적응(adapt)시키는 것을 고려하자.  

각 전이 학습(downstream) 작업은  
**컨텍스트-타깃(context–target)** 쌍으로 이루어진 학습 데이터셋으로 표현된다.  

$$
Z = \{(x_i, y_i)\}_{i=1}^N
$$  

여기서 $x_i$와 $y_i$는 모두 **토큰(token)** 시퀀스이다.  

예를 들어, NL2SQL의 경우 $x_i$는 자연어 질의(natural language query)이고,  
$y_i$는 그에 대응하는 **SQL 명령문(SQL command)** 이다.  
요약(summarization)의 경우 $x_i$는 기사(article)의 본문(content)이고,  
$y_i$는 그 요약(summary)이다.

전체 미세조정(full fine-tuning)에서는  
모델이 **사전학습된 가중치(pre-trained weights)** $\Phi_0$ 로 초기화되고,  
다음의 **조건부 언어 모델링 목표(conditional language modeling objective)** 를  
최대화하기 위해 그래디언트를 반복적으로 따라가며  
$\Phi_0 + \Delta \Phi$ 로 업데이트된다.  

$$
\max_{\Phi} 
\sum_{(x,y) \in Z} 
\sum_{t=1}^{|y|} 
\log \big(P_{\Phi}(y_t \mid x, y_{<t})\big)
\tag{1}
$$  

---

> **(블로그 추가 설명) 전체 미세조정(Full Fine-Tuning)의 의미**  
> - **전체 미세조정(full fine-tuning)** 은 사전학습(pre-training)이 끝난 모델의  
>   **모든 파라미터(parameter)** 를 다시 학습 가능한 상태로 두고,  
>   새로운 작업(task)에 맞게 전체를 재학습시키는 방식이다.  
> 
> - 위 식에서 $\Phi_0$ 는 사전학습(pre-trained)으로 얻은 원래의 가중치(weight)이며,  
>   $\Delta \Phi$ 는 새로운 작업에 맞게 학습을 통해 추가로 얻어지는  
>   **가중치의 변화량(weight update)** 을 의미한다.  
>   즉, 최종 가중치는 $\Phi = \Phi_0 + \Delta \Phi$ 형태로 표현된다.  
> 
> - 모델은 주어진 데이터셋 $Z = \{(x_i, y_i)\}$ 에 대해,  
>   각 토큰 시점 $t$ 에서 **이전 토큰들 $y_{<t}$ 과 입력 $x$ 가 주어졌을 때,  
>   다음 토큰 $y_t$ 가 등장할 확률** $P_{\Phi}(y_t \mid x, y_{<t})$ 을  
>   최대화하는 방향으로 학습된다.  
> 
> - 식 (1)은 이러한 과정을 **확률적 언어 모델링(objective)** 의 형태로 나타낸 것이다.  
>   즉, 모든 학습 데이터 $(x, y)$ 와 각 시점 $t$ 에 대한  
>   로그 확률의 합(log-likelihood sum)을 최대화함으로써,  
>   모델이 주어진 문맥(context)에서 다음 단어를 예측하도록 학습한다.  
> 
> - 이 접근 방식은 성능은 높지만,  
>   파라미터 수가 매우 큰 모델(예: GPT-3)에서는  
>   모든 가중치를 업데이트해야 하므로 **비용(cost)** 과 **메모리(memory)** 측면에서  
>   매우 비효율적이라는 한계를 가진다.

---

전체 미세조정의 주요 단점 중 하나는  
각 전이 학습(downstream) 작업마다  
서로 다른 파라미터 집합 $\Delta \Phi$ 를 학습해야 한다는 점이다.  
이때 그 차원(dimensions) $|\Delta \Phi|$ 는  
원래 모델의 파라미터 수 $|\Phi_0|$ 와 동일하다.  

따라서 **사전학습 모델이 매우 클 경우**  
(예: $|\Phi_0| \approx 175$B인 **GPT-3**)  
많은 개수의 미세조정된 모델 인스턴스를  
저장하거나 배포하는 것은 매우 어렵거나 거의 불가능하다.  

이 논문에서는 보다 **파라미터 효율적(parameter-efficient)** 인 접근을 채택한다.  
즉, 작업별 파라미터 변화량(increment) $\Delta \Phi = \Delta \Phi(\Theta)$ 를  
훨씬 더 작은 크기의 파라미터 집합 $\Theta$ 로 표현한다.  
여기서 $|\Theta| \ll |\Phi_0|$ 이다.  

따라서 $\Delta \Phi$ 를 찾는 문제는  
이제 $\Theta$ 위에서 최적화하는 문제로 바뀐다.  

$$
\max_{\Theta} 
\sum_{(x,y) \in Z} 
\sum_{t=1}^{|y|} 
\log \big(P_{\Phi_0 + \Delta \Phi(\Theta)}(y_t \mid x, y_{<t})\big)
\tag{2}
$$  

---

> **(블로그 추가 설명) $\Theta$의 의미와 식 (2)의 해석**  
> - 식 (2)에서 등장하는 $\Theta$는  
>   **LoRA가 새로 도입한 학습 가능한 파라미터들의 집합(set of trainable parameters)** 이다.  
>   이 파라미터들은 기존의 모델 가중치 $\Phi_0$와는 별도로,  
>   **가중치 변화량(weight update)** $\Delta \Phi$를 저차원 공간에서 표현하기 위해 사용된다.  
> 
> - 구체적으로, LoRA는 기존의 거대한 가중치 행렬 $W \in \mathbb{R}^{d \times d}$ 를  
>   그대로 학습하지 않고,  
>   그 변화량을 두 개의 **저랭크 행렬(low-rank matrices)** $A \in \mathbb{R}^{r \times d}$,  
>   $B \in \mathbb{R}^{d \times r}$ 의 곱으로 근사한다.  
>   $$
>   \Delta W = B A
>   $$  
>   이때 $\Theta$는 바로 이 행렬들 $A$와 $B$ — 즉,  
>   **LoRA 모듈 내에서 새롭게 학습되는 모든 파라미터들의 집합**을 의미한다.  
> 
> - 따라서 $\Delta \Phi(\Theta)$는 “$\Theta$로부터 계산된 전체 가중치 변화량”이다.  
>   다시 말해, $\Theta$를 학습하면 그에 따라 $\Delta \Phi$가 결정된다.  
>   학습의 초점은 이제 $\Phi$ 전체가 아니라,  
>   이 작은 저차원 파라미터 공간 $\Theta$ 위로 옮겨지는 것이다.  
> 
> - 식 (2)는 이러한 과정을 수식으로 표현한 것으로,  
>   모델은 여전히 조건부 확률  
>   $P(y_t \mid x, y_{<t})$을 최대화하지만,  
>   이때의 파라미터는 **사전학습된 가중치 $\Phi_0$** 와  
>   **$\Theta$로부터 유도된 변화량 $\Delta \Phi(\Theta)$** 를 합친 형태로 쓰인다.  
> 
> - 요약하면,  
>   **$\Theta$ = LoRA에서 새로 추가된 학습 가능한 저랭크 행렬 파라미터의 집합**이며,  
>   이 $\Theta$를 학습함으로써  
>   거대한 모델 전체의 가중치를 직접 업데이트하지 않고도  
>   새로운 작업에 적응할 수 있게 되는 것이다.

---

이후의 섹션에서는  
$\Delta \Phi$ 를 계산 및 메모리 효율적으로 표현하기 위한  
**저랭크 표현(low-rank representation)** 방식을 제안한다.  

특히, 사전학습 모델이 **GPT-3 (175B)** 인 경우,  
학습 가능한 파라미터 수 $|\Theta|$ 는  
$|\Phi_0|$ 의 **0.01% 수준까지 줄일 수 있다.**
