---
layout: post
title: "[논문] LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS"
date: 2025-10-04 15:00:00 +0900
categories:
  - "논문"
tags: []
---

> **논문 출처**  
> Hu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W.  
> *LoRA: Low-Rank Adaptation of Large Language Models*.  
> arXiv preprint (arXiv:2106.09685), Version 2.  
> <a href="https://arxiv.org/abs/2106.09685" target="_blank">🔗 원문 링크 (arXiv:2106.09685)</a>

**저자**  
- Edward Hu (Microsoft Corporation) – edwardhu@microsoft.com  
- Yelong Shen (Microsoft Corporation) – yeshe@microsoft.com  
- Phillip Wallis (Microsoft Corporation) – phwallis@microsoft.com  
- Zeyuan Allen-Zhu (Microsoft Corporation) – zeyuana@microsoft.com  
- Yuanzhi Li (Carnegie Mellon University) – yuanzhil@andrew.cmu.edu  
- Shean Wang (Microsoft Corporation) – swang@microsoft.com  
- Lu Wang (Microsoft Corporation) – luw@microsoft.com  
- Weizhu Chen (Microsoft Corporation) – wzchen@microsoft.com  

---

>**주석**  
>
>∗ 공동 기여(Equal contribution).

---

(Version 2)

---

>**주석**  
>
>V1과 비교했을 때, 이번 초안에는 더 나은 **베이스라인(baselines)**,  
>**GLUE 실험**, 그리고 **어댑터 지연(adapter latency)** 에 대한  
>추가 내용이 포함되어 있다.

---

## 초록 (Abstract)  

자연어 처리(NLP)의 주요 패러다임 중 하나는  
**일반 도메인 데이터에 대한 대규모 사전학습(pre-training)** 과  
**특정 작업이나 도메인에 대한 적응(adaptation)** 으로 구성된다.  

하지만 모델이 커질수록,  
모든 파라미터를 다시 학습시키는 **전체 미세조정(full fine-tuning)** 은  
점점 더 비현실적이 된다.  

예를 들어, **GPT-3 (175B)** 모델의 경우,  
각각 1,750억 개의 파라미터를 가진 미세조정된 모델 인스턴스를  
독립적으로 배포하는 것은 **막대한 비용**이 든다.  

이를 해결하기 위해 우리는 **LoRA (Low-Rank Adaptation)** 를 제안한다.  

LoRA는 **사전학습된 모델의 가중치를 고정(freeze)** 한 채,  
**트랜스포머(Transformer) 아키텍처의 각 층(layer)** 에  
**학습 가능한 저랭크 분해 행렬(rank decomposition matrices)** 을 삽입함으로써,  
**전이 학습(transfer learning)** 시 필요한 **학습 가능한 파라미터 수를 크게 줄인다.**  

---

> **(블로그 추가 설명) 전이 학습(Transfer Learning)이란?**  
> - **전이 학습(Transfer Learning)** 은  
>   하나의 작업(task)이나 도메인(domain)에서 학습된 지식을  
>   **다른 작업으로 이전(transfer)** 하여 활용하는 학습 방법이다.  
> 
> - 일반적으로 **대규모 데이터로 사전학습(pre-training)** 된 모델은  
>   언어, 문맥, 구조 등에 대한 **일반적인 표현(representation)** 을 이미 학습하고 있다.  
>   이러한 모델을 새로운 응용 문제(예: 감정 분석, 질의응답, 번역 등)에 적용할 때는  
>   모델의 **일부 파라미터만 조정**하거나,  
>   **특정 층(layer)만 미세조정(fine-tuning)** 하는 방식으로 수행된다.  
> 
> - 따라서 처음부터 모델을 다시 학습시키지 않고,  
>   **이미 학습된 지식(knowledge)** 을 바탕으로  
>   **새로운 작업에 빠르게 적응(adapt)** 하는 것이  
>   전이 학습의 핵심 개념이다.  
> 
> - LoRA 역시 이러한 전이 학습의 한 형태로,  
>   전체 모델을 다시 학습하지 않고  
>   **필요한 부분만 효율적으로 조정**하여 성능을 유지한다.

---

GPT-3 (175B)를 Adam 옵티마이저로 미세조정했을 때와 비교하면,  
LoRA는 **학습 가능한 파라미터 수를 최대 10,000배까지 줄이고**,  
**GPU 메모리 요구량을 약 3배 절감**할 수 있다.  

그럼에도 불구하고, LoRA는  
RoBERTa, DeBERTa, GPT-2, GPT-3 등의 모델에서  
**미세조정(full fine-tuning)과 동등하거나 더 우수한 성능**을 보인다.  

또한, 학습 가능한 파라미터가 더 적고,  
**학습 처리량(training throughput)** 이 더 높으며,  
**어댑터(adapters)** 방식과 달리  
**추론(inference) 시 지연(latency)** 도 추가되지 않는다.  

---

> **(블로그 추가 설명) 어댑터(Adapter) 방식이란?**  
> - **어댑터(Adapter)** 는 대형 사전학습 모델(pre-trained model)의  
>   모든 파라미터를 다시 학습시키지 않고,  
>   **각 층(layer)** 사이에 **작은 학습 가능한 모듈(small trainable modules)** 을 추가하여  
>   새로운 작업에 맞게 모델을 미세조정(fine-tuning)하는 방법이다.  
> 
> - 이 방식은 원래의 모델 파라미터를 거의 그대로 유지한 채,  
>   **적은 추가 파라미터(parameter-efficient)** 만 학습할 수 있다는 장점이 있다.  
>   따라서 전체 모델을 다시 학습시키는 것보다 훨씬 효율적이며,  
>   서로 다른 작업 간에 **공유(reuse)** 도 용이하다.  
> 
> - 그러나 어댑터는 **추론(inference)** 단계에서  
>   각 층에 추가된 모듈을 통과해야 하므로,  
>   **추가적인 지연(latency)** 이 발생할 수 있다는 단점이 있다.  
>
> 어댑터 방식은 “모델 전체를 다시 학습하지 않고 필요한 부분만 추가 학습하는”  
> **파라미터 효율적 미세조정(Parameter-Efficient Fine-Tuning, PEFT)** 기법이다.  
> 하지만 **LoRA는 어댑터와 달리 추론 시 지연이 거의 없고**,  
> 동일한 효율성을 유지하면서 더 단순한 구조를 제공한다.

---

아울러 우리는  
**언어 모델 적응에서의 랭크 결핍(rank-deficiency)** 현상에 대한  
실증적 분석을 제공하여,  
**LoRA의 효율성에 대한 통찰(insight)** 을 제시한다.  

---

> **(블로그 추가 설명) 왜 ‘랭크 결핍(Rank-Deficiency)’이라고 부를까?**  
> - “랭크 결핍”은 원래 선형대수학에서 **행렬의 랭크(rank)** 가  
>   최대치보다 낮을 때, 즉 **선형 독립 성분이 부족할 때**를 의미하는 **부정적 용어**이다.  
>   예를 들어, $n \times n$ 행렬의 랭크가 $n$보다 작다면  
>   그 행렬은 **정보 손실(loss of information)** 이 있거나,  
>   **역행렬이 존재하지 않는(singular)** 상태로 본다.  
> 
> - 하지만 언어 모델의 관점에서는 이 “결핍”이 반드시 나쁜 것이 아니다.  
>   오히려 모델이 실제로는 **낮은 차원(low-rank)** 의 표현만으로도  
>   충분히 의미 있는 정보를 학습하고,  
>   복잡한 패턴을 표현할 수 있음을 보여주는 **구조적 효율성(structural efficiency)** 의 신호이기도 하다.  
> 
> - LoRA는 바로 이 특성을 적극적으로 이용한다.  
>   즉, 언어 모델이 본질적으로 **랭크 결핍 구조를 가진다**는 점을 활용하여,  
>   전체 파라미터를 모두 학습하지 않고도  
>   **필요한 저차원 방향만 학습함으로써 동일한 효과를 얻는다.**  
> 
> - 따라서 “랭크 결핍”이라는 단어는 수학적으로는 제약(constraint)을 뜻하지만,  
>   LoRA의 맥락에서는 **“낮은 차원 구조로도 충분히 잘 작동하는 현상”** 이라는  
>   **긍정적인 의미로 재해석**된다.

---

마지막으로, 우리는  
**PyTorch 모델에 LoRA를 통합할 수 있는 패키지**를 공개하였으며,  
RoBERTa, DeBERTa, GPT-2용 **구현 및 모델 체크포인트**를  
다음 링크에서 제공한다.  

<a href="https://github.com/microsoft/LoRA" target="_blank">🔗 https://github.com/microsoft/LoRA</a>


## 1. 서론 (Introduction)  

자연어 처리(NLP)의 많은 응용들은  
**하나의 대규모 사전학습 언어 모델(pre-trained language model)** 을  
여러 개의 **전이(또는 하위) 응용 프로그램(downstream applications)** 에  
적응시키는 것에 의존한다.  

이러한 적응(adaptation)은 일반적으로  
**모든 사전학습된 모델의 파라미터를 업데이트하는 미세조정(fine-tuning)** 을 통해 수행된다.  

그러나 미세조정의 주요 단점은,  
**새로운 모델이 원래 모델과 동일한 수의 파라미터를 갖게 된다는 점**이다.  

모델 규모가 몇 달마다 더 커지는 현 상황에서,  
이 문제는 단순히 GPT-2 (Radford et al.)나 RoBERTa large (Liu et al., 2019) 수준에서는  
“약간의 불편함”에 불과할 수 있었지만,  
**1,750억 개의 학습 가능한 파라미터를 가진 GPT-3 (Brown et al., 2020)**<sup>1</sup> 의 경우에는  
**중대한 배포(deployment)상의 도전 과제**로 변하게 되었다.  

---

>**주석**  
>
><sup>1</sup>GPT-3 (175B)는 **퓨샷 학습(few-shot learning)** 만으로도  
>일정 수준 이상의 성능을 달성하지만,  
>**부록 A(Appendix A)** 에서 보여지듯이  
>**미세조정(fine-tuning)** 을 통해 그 성능이 크게 향상된다.

---

많은 연구자들이 이 문제를 완화하기 위해  
**일부 파라미터만 적응(adapt)** 시키거나,  
**새로운 작업(task)** 을 위해 **외부 모듈(external modules)** 을 학습하는 방법을 시도해왔다.  

이러한 방식에서는 각 작업마다  
사전학습된 모델에 더해 **작업별 파라미터(task-specific parameters)** 만  
저장하고 불러오면 되므로,  
배포(deployment) 시 **운영 효율성(operational efficiency)** 이 크게 향상된다.  

그러나 기존 기법들은 종종  
**모델의 깊이(model depth)** 를 확장함으로써 **추론 지연(inference latency)** 을 유발하거나  
(Loulsby et al., 2019; Rebuffi et al., 2017),  
**모델이 처리할 수 있는 시퀀스 길이(sequence length)** 를 줄이는 문제를 발생시킨다  
(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021) (3장 참조).  

더 중요한 점은, 이러한 방법들이 종종  
**미세조정(fine-tuning) 기반의 베이스라인(baseline)** 수준의 성능을 달성하지 못해,  
**효율성과 모델 품질 간의 트레이드오프(trade-off)** 를 초래한다는 것이다.

우리는 Li et al. (2018a) 및 Aghajanyan et al. (2020)에서 제시된 결과로부터 영감을 얻었다.  
이들 연구는 **과도하게 매개변수화된(over-parameterized)** 모델이 실제로는  
**낮은 고유 차원(low intrinsic dimension)** 상에 존재함을 보여준다.  

이에 기반하여 우리는,  
모델 적응(model adaptation) 과정에서 발생하는 **가중치 변화(weight change)** 역시  
낮은 **내재적 랭크(intrinsic rank)** 를 가진다고 가정(hypothesis)한다.  
이 가정으로부터 우리가 제안한 **LoRA (Low-Rank Adaptation)** 방법이 도출된다.  

LoRA는 사전학습된 가중치를 그대로 **고정(freeze)** 시킨 채,  
**적응 과정에서의 밀집층(dense layer) 변화량**을  
**랭크 분해 행렬(rank decomposition matrices)** 형태로 학습하도록 설계된다.  
즉, 밀집층 자체를 직접 학습하는 대신,  
그 변화(change)를 저랭크 행렬 형태로 간접적으로 최적화한다는 것이다  
(그림 1 참고).  

---

그림 1: 우리의 **재매개변수화(reparametrization)** 방식.  
우리는 **A와 B만 학습한다.**

<img src="/assets/img/paper/lora/image_1.png" alt="image" width="360px"> 

---

> **(블로그 추가 설명) 재매개변수화(Reparametrization)와 랭크 분해 행렬 학습이란?**  
> - **재매개변수화(Reparametrization)** 는  
>   기존의 가중치 행렬 $W$ 를 직접 학습하지 않고,  
>   그것을 **다른 형태의 파라미터(예: $A$, $B$)** 로 다시 표현하여 학습하는 방법이다.  
>   즉, $W$ 대신 **$W + \Delta W$** 형태로 모델을 표현하고,  
>   이때 **$\Delta W = B A$** 로 두어 $A$, $B$ 두 행렬만 학습한다.  
> 
> - 이렇게 하면, 원래의 거대한 가중치 행렬 $W \in \mathbb{R}^{d \times d}$ 을  
>   직접 업데이트할 필요가 없다.  
>   대신, **저랭크(rank $r$)** 제약을 가진 두 행렬 $A \in \mathbb{R}^{r \times d}$,  
>   $B \in \mathbb{R}^{d \times r}$ 을 통해  
>   $W$ 의 변화를 근사(approximation)하게 된다.  
> 
> - 그림에서처럼, $A$ 는 초기값을 **정규분포 $\mathcal{N}(0, \sigma^2)$** 로 설정하고,  
>   $B$ 는 **0으로 초기화**된다.  
>   학습 과정에서는 $A$와 $B$만 최적화되고,  
>   원래의 사전학습 가중치 $W$ 는 **고정(freeze)** 되어 유지된다.  
> 
> - 이러한 방식은 모델의 전체 구조를 바꾸지 않으면서도  
>   **적은 수의 학습 파라미터로 동일한 효과를 얻을 수 있게 해주며**,  
>   이를 통해 **저장 공간과 계산량을 크게 절약**할 수 있다.  
> 
> - 요약하자면, LoRA의 재매개변수화는  
>   거대한 파라미터 행렬을 저차원(rank-$r$) 공간으로 분해해  
>   효율적으로 학습하는 **저랭크 근사(low-rank approximation)** 기반의 접근법이다.

---

GPT-3 (175B)를 예로 들면,  
전체 랭크($d$)가 12,288에 달하더라도  
**매우 낮은 랭크(예: $r=1$ 또는 $r=2$)** 만으로 충분함을 보여준다.  
이는 LoRA가 **저장(storage)** 및 **계산(compute)** 양 측면에서  
매우 효율적인 방법임을 의미한다.

LoRA는 여러 가지 핵심적인 이점을 가진다.  

- **사전학습된 모델(pre-trained model)** 은 여러 다른 작업(tasks)을 위한  
  작은 LoRA 모듈들을 구축하는 데 공유되고 사용될 수 있다.  
  우리는 공유된 모델을 고정(freeze)한 상태로 유지하고,  
  그림 1의 행렬 A와 B를 교체함으로써 작업을 효율적으로 전환할 수 있다.  
  이는 저장 공간 요구량(storage requirement)과  
  작업 전환 시 오버헤드(task-switching overhead)를 크게 줄여준다.  

- LoRA는 학습을 더욱 효율적으로 만들고,  
  적응형 옵티마이저(adaptive optimizer)를 사용할 때  
  하드웨어 진입 장벽(hardware barrier to entry)을 최대 3배까지 낮춘다.  
  이는 대부분의 파라미터에 대해 그래디언트를 계산하거나  
  옵티마이저 상태(optimizer state)를 유지할 필요가 없기 때문이다.  
  대신, 우리는 삽입된 **훨씬 작은 저랭크 행렬(low-rank matrices)** 만 최적화한다.  

- 우리의 단순한 선형(linear) 설계는,  
  배포(deployment) 시 학습 가능한 행렬(trainable matrices)을  
  고정된 가중치(frozen weights)와 병합(merge)할 수 있게 하며,  
  완전히 미세조정된 모델(fully fine-tuned model)과 비교했을 때  
  **추론 지연(inference latency)** 을 전혀 초래하지 않는다.  

- LoRA는 많은 기존 기법들과 **직교적(orthogonal)** 이며,  
  **프리픽스 튜닝(prefix-tuning)** 과 같은  
  여러 방법들과 결합될 수 있다.  
  이에 대한 예시는 **부록 E(Appendix E)** 에서 제시한다.

---

> **(블로그 추가 설명) ‘직교적(Orthogonal)’이라는 표현의 의미**  
> - 여기서 **직교적(orthogonal)** 이라는 표현은  
>   수학적 의미의 “서로 독립적(independent)”이라는 개념에서 유래한다.  
>   즉, **LoRA가 다른 방법들과 간섭하지 않고 동시에 사용할 수 있다** 는 뜻이다.  
> 
> - 어떤 기법이 직교적이라는 것은  
>   그것이 다른 학습 기법의 구조나 원리에 의존하지 않으며,  
>   **별도로 적용하거나 함께 결합(combine)** 해도  
>   서로의 작동 방식에 영향을 주지 않는다는 의미이다.  
> 
> - 따라서 LoRA는 **프리픽스 튜닝(prefix-tuning)**, **프로프트 튜닝(prompt tuning)**,  
>   **어댑터(adapter)** 등과 같은 다른 **파라미터 효율적 학습 기법(PEFT)** 들과  
>   함께 사용될 수 있으며, 이러한 결합은  
>   **추가적인 성능 향상이나 효율성 개선**을 가능하게 한다.

---

### **용어 및 표기 규칙 (Terminologies and Conventions)**  

우리는 **트랜스포머(Transformer) 아키텍처**를 자주 참조하며,  
그 구조에서 일반적으로 사용되는 용어와 표기 규칙(conventions)을 따른다.  

트랜스포머 층의 **입력 및 출력 차원 크기**를 $$d_{\text{model}}$$이라고 부른다.  

$$W_q, \; W_k, \; W_v, \; W_o$$는 각각 **셀프-어텐션(self-attention)** 모듈 내의  
**쿼리(query)**, **키(key)**, **밸류(value)**, **출력(output)** 투영 행렬을 의미한다.  

$$W \; \text{또는} \; W_0$$는 **사전학습된(pre-trained)** 가중치 행렬을,  
$$\Delta W$$는 **적응(adaptation)** 중 누적된 **그래디언트 업데이트(accumulated gradient update)** 를 나타낸다.  

$$r$$은 **LoRA 모듈의 랭크(rank)** 를 의미한다.  

우리는 **Vaswani et al. (2017)** 및 **Brown et al. (2020)** 의 표준 규칙을 따르며,  
모델 최적화(model optimization)에는 **Adam 옵티마이저**  
(**Loshchilov & Hutter, 2019; Kingma & Ba, 2017**)를 사용한다.  

또한, 트랜스포머의 **MLP 피드포워드(feedforward) 차원**은  
$$d_{\text{ffn}} = 4 \times d_{\text{model}}$$로 설정한다.


## 2. 문제 정의 (Problem Statement)  

우리의 제안은 **학습 목표(training objective)** 에 구애받지 않지만,  
본 연구에서는 **언어 모델링(language modeling)** 을 주요 동기 사례로 삼는다.  

아래는 언어 모델링 문제에 대한 간단한 설명이며,  
특히 **작업별 프롬프트(task-specific prompt)** 가 주어졌을 때  
**조건부 확률(conditional probability)** 을 최대화하는 과정을 다룬다.  

사전학습된 **자기회귀 언어 모델(autoregressive language model)**  
$P_\Phi(y|x)$ 이 주어졌다고 가정하자.  
이 모델은 매개변수 **Φ**로 정의된다.  

예를 들어, $P_\Phi(y|x)$ 는 **GPT (Radford et al.; Brown et al., 2020)** 과 같은  
**트랜스포머(Transformer)** 기반의 **일반적 다중작업 학습기(generic multi-task learner)** 일 수 있다  
(Vaswani et al., 2017).  

이제 이 사전학습된 모델을  
**요약(summarization)**, **기계 독해(MRC, Machine Reading Comprehension)**,  
**자연어 → SQL 변환(NL2SQL)** 과 같은  
**조건부 텍스트 생성(conditional text generation)** 작업에 적응(adapt)시키는 것을 고려하자.  

각 전이 학습(downstream) 작업은  
**컨텍스트-타깃(context–target)** 쌍으로 이루어진 학습 데이터셋으로 표현된다.  

$$
Z = \{(x_i, y_i)\}_{i=1}^N
$$  

여기서 $x_i$와 $y_i$는 모두 **토큰(token)** 시퀀스이다.  

예를 들어, NL2SQL의 경우 $x_i$는 자연어 질의(natural language query)이고,  
$y_i$는 그에 대응하는 **SQL 명령문(SQL command)** 이다.  

요약(summarization)의 경우 $x_i$는 기사(article)의 본문(content)이고,  
$y_i$는 그 요약(summary)이다.

전체 미세조정(full fine-tuning)에서는  
모델이 **사전학습된 가중치(pre-trained weights)** $\Phi_0$ 로 초기화되고,  
다음의 **조건부 언어 모델링 목표(conditional language modeling objective)** 를  
최대화하기 위해 그래디언트를 반복적으로 따라가며  
$\Phi_0 + \Delta \Phi$ 로 업데이트된다.  

$$
\max_{\Phi} 
\sum_{(x,y) \in Z} 
\sum_{t=1}^{|y|} 
\log \big(P_{\Phi}(y_t \mid x, y_{<t})\big)
\tag{1}
$$  

---

> **(블로그 추가 설명) 전체 미세조정(Full Fine-Tuning)의 의미**  
> - **전체 미세조정(full fine-tuning)** 은 사전학습(pre-training)이 끝난 모델의  
>   **모든 파라미터(parameter)** 를 다시 학습 가능한 상태로 두고,  
>   새로운 작업(task)에 맞게 전체를 재학습시키는 방식이다.  
> 
> - 위 식에서 $\Phi_0$ 는 사전학습(pre-trained)으로 얻은 원래의 가중치(weight)이며,  
>   $\Delta \Phi$ 는 새로운 작업에 맞게 학습을 통해 추가로 얻어지는  
>   **가중치의 변화량(weight update)** 을 의미한다.  
>   즉, 최종 가중치는 $\Phi = \Phi_0 + \Delta \Phi$ 형태로 표현된다.  
> 
> - 모델은 주어진 데이터셋 $Z = \{(x_i, y_i)\}$ 에 대해,  
>   각 토큰 시점 $t$ 에서 **이전 토큰들 $y_{<t}$ 과 입력 $x$ 가 주어졌을 때,  
>   다음 토큰 $y_t$ 가 등장할 확률** $P_{\Phi}(y_t \mid x, y_{<t})$ 을  
>   최대화하는 방향으로 학습된다.  
> 
> - 식 (1)은 이러한 과정을 **확률적 언어 모델링(objective)** 의 형태로 나타낸 것이다.  
>   즉, 모든 학습 데이터 $(x, y)$ 와 각 시점 $t$ 에 대한  
>   로그 확률의 합(log-likelihood sum)을 최대화함으로써,  
>   모델이 주어진 문맥(context)에서 다음 단어를 예측하도록 학습한다.  
> 
> - 이 접근 방식은 성능은 높지만,  
>   파라미터 수가 매우 큰 모델(예: GPT-3)에서는  
>   모든 가중치를 업데이트해야 하므로 **비용(cost)** 과 **메모리(memory)** 측면에서  
>   매우 비효율적이라는 한계를 가진다.

---

전체 미세조정의 주요 단점 중 하나는  
각 전이 학습(downstream) 작업마다  
서로 다른 파라미터 집합 $\Delta \Phi$ 를 학습해야 한다는 점이다.  

이때 그 차원(dimensions) $\mid\Delta\Phi\mid$ 는  
원래 모델의 파라미터 수 $\mid \Phi_0 \mid$ 와 동일하다.  

따라서 **사전학습 모델이 매우 클 경우**  
(예: $|\Phi_0| \approx 175$B인 **GPT-3**)  
많은 개수의 미세조정된 모델 인스턴스를  
저장하거나 배포하는 것은 매우 어렵거나 거의 불가능하다.  

이 논문에서는 보다 **파라미터 효율적(parameter-efficient)** 인 접근을 채택한다.  

즉, 작업별 파라미터 변화량(increment) $\Delta \Phi = \Delta \Phi(\Theta)$ 를  
훨씬 더 작은 크기의 파라미터 집합 $\Theta$ 로 표현한다.  

여기서 $\mid \Theta \mid \ll \mid \Phi_0 \mid$ 이다.  

따라서 $\Delta \Phi$ 를 찾는 문제는  
이제 $\Theta$ 위에서 최적화하는 문제로 바뀐다.  

$$
\max_{\Theta} 
\sum_{(x,y) \in Z} 
\sum_{t=1}^{|y|} 
\log \big(P_{\Phi_0 + \Delta \Phi(\Theta)}(y_t \mid x, y_{<t})\big)
\tag{2}
$$  

---

> **(블로그 추가 설명) $\Theta$의 의미와 식 (2)의 해석**  
> - 식 (2)에서 등장하는 $\Theta$는  
>   **LoRA가 새로 도입한 학습 가능한 파라미터들의 집합(set of trainable parameters)** 이다.  
>   이 파라미터들은 기존의 모델 가중치 $\Phi_0$와는 별도로,  
>   **가중치 변화량(weight update)** $\Delta \Phi$를 저차원 공간에서 표현하기 위해 사용된다.  
> 
> - 구체적으로, LoRA는 기존의 거대한 가중치 행렬 $W \in \mathbb{R}^{d \times d}$ 를  
>   그대로 학습하지 않고,  
>   그 변화량을 두 개의 **저랭크 행렬(low-rank matrices)** $A \in \mathbb{R}^{r \times d}$,  
>   $B \in \mathbb{R}^{d \times r}$ 의 곱으로 근사한다.  
>   $$
>   \Delta W = B A
>   $$  
>   이때 $\Theta$는 바로 이 행렬들 $A$와 $B$ — 즉,  
>   **LoRA 모듈 내에서 새롭게 학습되는 모든 파라미터들의 집합**을 의미한다.  
> 
> - 따라서 $\Delta \Phi(\Theta)$는 “$\Theta$로부터 계산된 전체 가중치 변화량”이다.  
>   다시 말해, $\Theta$를 학습하면 그에 따라 $\Delta \Phi$가 결정된다.  
>   학습의 초점은 이제 $\Phi$ 전체가 아니라,  
>   이 작은 저차원 파라미터 공간 $\Theta$ 위로 옮겨지는 것이다.  
> 
> - 식 (2)는 이러한 과정을 수식으로 표현한 것으로,  
>   모델은 여전히 조건부 확률  
>   $P(y_t \mid x, y_{<t})$을 최대화하지만,  
>   이때의 파라미터는 **사전학습된 가중치 $\Phi_0$** 와  
>   **$\Theta$로부터 유도된 변화량 $\Delta \Phi(\Theta)$** 를 합친 형태로 쓰인다.  
> 
> - 요약하면,  
>   **$\Theta$ = LoRA에서 새로 추가된 학습 가능한 저랭크 행렬 파라미터의 집합**이며,  
>   이 $\Theta$를 학습함으로써  
>   거대한 모델 전체의 가중치를 직접 업데이트하지 않고도  
>   새로운 작업에 적응할 수 있게 되는 것이다.

---

이후의 섹션에서는  
$\Delta \Phi$ 를 계산 및 메모리 효율적으로 표현하기 위한  
**저랭크 표현(low-rank representation)** 방식을 제안한다.  

특히, 사전학습 모델이 **GPT-3 (175B)** 인 경우,  
학습 가능한 파라미터 수 $|\Theta|$ 는  
$|\Phi_0|$ 의 **0.01% 수준까지 줄일 수 있다.**

---

## 3. 기존 방법들은 충분하지 않은가? (Aren’t Existing Solutions Good Enough?)  

우리가 해결하려는 문제는 결코 새로운 것이 아니다.  
**전이 학습(transfer learning)** 이 등장한 이후,  
수많은 연구들이 모델 적응(model adaptation)을  
더 **파라미터 효율적(parameter-efficient)** 이고  
**계산 효율적(compute-efficient)** 으로 만드는 방법을 제시해 왔다.  
이와 관련된 잘 알려진 연구들에 대해서는 **6장(Section 6)** 에서 요약한다.  

**언어 모델링(language modeling)** 을 예로 들면,  
효율적인 적응을 위한 대표적인 두 가지 전략이 존재한다.  

첫 번째는 **어댑터 층(adapter layers)** 을 추가하는 방식이고,  
(Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2021; Rücklé et al., 2020).  

두 번째는 **입력층 활성화(input layer activations)** 의  
일부 형태를 최적화(optimization)하는 접근이다.  
(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021).  

그러나 두 전략 모두 한계를 가진다.  
특히 **대규모 모델 환경(large-scale setting)** 이나  
**지연(latency)에 민감한 실제 서비스(production scenario)** 에서는  
이러한 기존 방법들이 충분히 효과적이지 않다.

### **어댑터 층(Adapter Layers)은 추론 지연(Inference Latency)을 초래한다**  

어댑터(adapter)에는 여러 가지 변형(variants)이 존재한다.  
우리는 그중 **두 가지 대표적인 설계(design)** 에 초점을 맞춘다.  

첫째, **Houlsby et al. (2019)** 이 제안한 방식으로,  
각 **트랜스포머 블록(Transformer block)** 에 **두 개의 어댑터 층(adapter layers)** 을 포함한다.  

둘째, **Lin et al. (2020)** 의 보다 최근 설계로,  
각 블록에 **하나의 어댑터 층만 포함하지만**,  
**추가적인 LayerNorm (Ba et al., 2016)** 을 사용한다.

전체 지연(latency)을 줄이기 위해  
일부 층을 **가지치기(pruning)** 하거나  
**멀티태스크 환경(multi-task setting)** 을 활용할 수도 있지만  
(Rücklé et al., 2020; Pfeiffer et al., 2021),  
어댑터 층에서 발생하는 **추가 연산량(extra compute)** 자체를  
근본적으로 제거할 방법은 없다.  

처음에는 이것이 큰 문제가 아닌 것처럼 보일 수 있다.  
왜냐하면 어댑터 층은 일반적으로  
**좁은 병목 차원(bottleneck dimension)** 을 사용하여  
**원래 모델의 1% 미만의 파라미터**만 추가하기 때문이다.  
이로 인해 추가되는 연산량(FLOPs)은 제한적이다.  

그러나 **대규모 신경망(large neural networks)** 은  
**하드웨어 병렬 처리(hardware parallelism)** 를 통해  
낮은 지연(latency)을 유지한다.  
문제는 어댑터 층이 **순차적으로(sequentially)** 처리되어야 한다는 점이다.  

따라서 **온라인 추론(online inference)** 환경처럼  
**배치 크기(batch size)** 가 1에 가까운 상황에서는  
이 순차적 처리로 인해 지연이 뚜렷하게 나타난다.  

예를 들어, 모델 병렬화(model parallelism)가 적용되지 않은 일반적인 환경에서,  
단일 GPU 위에서 **GPT-2 (Radford et al.) medium 모델**을  
어댑터와 함께 실행하면,  
병목 차원이 매우 작더라도(표 1 참조)  
**추론 지연(latency)이 눈에 띄게 증가**함을 확인할 수 있다.  

---

**표 1**

GPT-2 medium에서 **단일 순전파(single forward pass)** 시의  
**추론 지연 시간(inference latency)** 을 밀리초 단위로 측정한 결과이다.  
100회의 실험을 평균하여 산출했으며,  
**NVIDIA Quadro RTX8000** GPU를 사용하였다.  

기호 “$|\Theta|$”는 **어댑터 층(adapter layers)** 에서의  
**학습 가능한 파라미터 수(trainable parameters)** 를 나타낸다.  

**Adapter**<sup>L</sup>과 **Adapter**<sup>H</sup>는  
두 가지 **어댑터 튜닝(adapter tuning)** 변형이며,  
이에 대한 설명은 **5.1절(Section 5.1)** 에 제시되어 있다.  

어댑터 층으로 인해 발생하는 추론 지연은  
**온라인 환경(online setting)** 이나  
**짧은 시퀀스 길이(short-sequence-length)** 의 경우 특히 두드러질 수 있다.  
자세한 실험 결과는 **부록 B(Appendix B)** 를 참조하라.

<img src="/assets/img/paper/lora/image_2.png" alt="image" width="720px"> 

---

이 문제는 Shoeybi et al. (2020), Lepikhin et al. (2020)에서처럼  
모델을 여러 GPU에 **샤딩(sharding)** 해야 할 때 더 심각해진다.  

추가된 네트워크 깊이로 인해  
**AllReduce** 및 **Broadcast**와 같은  
**동기식 GPU 연산(synchronous GPU operations)** 이 더 많이 필요하기 때문이다.  

이는 우리가 어댑터 파라미터를 여러 번 **중복 저장(redundantly store)** 하지 않는 한  
피할 수 없는 문제이다.

---

> **(블로그 추가 설명) AllReduce와 Broadcast 연산의 의미**  
> - **AllReduce** 와 **Broadcast** 는  
>   다중 GPU 환경에서 모델 학습이나 추론 시 **데이터를 동기화(synchronize)** 하기 위해 사용되는  
>   대표적인 **통신 연산(communication operations)** 이다.  
> 
> - **AllReduce 연산**은  
>   여러 GPU가 각각 계산한 값을 서로 교환(exchange)하고,  
>   이를 더하거나(예: gradient 합산), 평균내는 등의 연산을 통해  
>   모든 GPU가 동일한 결과를 갖도록 만드는 과정이다.  
>   예를 들어, 모델 병렬 학습 시 각 GPU가 계산한 그래디언트를  
>   전체적으로 합산하여 모든 GPU에 동일하게 반영하는 데 사용된다.  
>   $$
>   \text{AllReduce: } x_i \rightarrow \sum_i x_i \quad \text{(모든 GPU 간 합산 후 동기화)}
>   $$  
> 
> - **Broadcast 연산**은  
>   한 GPU(일반적으로 “마스터 GPU”)에서 계산된 값을  
>   다른 모든 GPU로 복사(copy)하여 전달하는 과정이다.  
>   이는 모델 파라미터나 중간 결과를 여러 장치에 동일하게 배포(distribute)할 때 사용된다.  
>   $$
>   \text{Broadcast: } x_1 \rightarrow (x_1, x_1, x_1, \dots)
>   $$  
> 
> - 이러한 연산들은 모두 **동기식(synchronous)** 으로 수행되기 때문에,  
>   하나의 GPU라도 작업이 지연되면 전체 연산이 멈추게 된다.  
>   따라서 네트워크 깊이가 깊어질수록,  
>   또는 어댑터 층이 추가될수록  
>   **GPU 간 통신 부담(communication overhead)** 이 커지고  
>   결과적으로 **추론 지연(latency)** 도 증가하게 된다.

---

### **프롬프트 직접 최적화의 어려움 (Directly Optimizing the Prompt is Hard)**  

다른 방향의 접근법인 **프리픽스 튜닝(prefix tuning)** (Li & Liang, 2021)은  
다른 형태의 어려움을 겪는다.  

우리는 프리픽스 튜닝이 **최적화(optimization)** 하기 어렵고,  
**학습 가능한 파라미터 수(trainable parameters)** 에 따라  
그 성능이 **단조적으로(non-monotonically)** 변하지 않는다는 것을 관찰했다.  
이는 원 논문(original paper)에서도 보고된 바와 동일한 현상이다.  

보다 근본적으로,  
**시퀀스 길이(sequence length)** 의 일부를  
적응(adaptation)을 위해 예약(reserve)해야 한다는 점이 문제이다.  

이로 인해 전이 학습(downstream task)을 처리할 수 있는  
**실제 시퀀스 길이(available sequence length)** 가 줄어들게 된다.  

우리는 이러한 제약이  
프롬프트 튜닝(prompt tuning)의 성능을  
다른 방법들에 비해 떨어뜨리는 요인 중 하나라고 추정한다.  

작업별 성능(task performance)에 대한 자세한 연구는  
**5절(Section 5)** 에서 다룬다.

---

> **(블로그 추가 설명) 시퀀스 길이 예약이 의미하는 것**  
> - 프리픽스 튜닝(prefix tuning)에서는  
>   모델이 새로운 작업(task)에 적응할 수 있도록  
>   입력 시퀀스의 맨 앞부분에 **학습 가능한 토큰들(trainable tokens)**,  
>   즉 **프리픽스(prefix)** 를 추가한다.  
> 
> - 이 프리픽스는 모델의 파라미터를 직접 수정하지 않고도  
>   입력에 “작업별 힌트(task-specific context)”를 주는 역할을 한다.  
>   하지만 이 추가된 프리픽스 토큰들이  
>   전체 시퀀스 길이 안에 포함되기 때문에,  
>   모델이 실제 입력 데이터(예: 문장, 문단 등)를 처리할 수 있는  
>   **토큰 수(token capacity)** 가 줄어들게 된다.  
> 
> - 예를 들어, 모델의 최대 시퀀스 길이가 512라면  
>   프리픽스로 20개의 토큰을 사용했을 때  
>   실제 작업 입력에는 492개의 토큰만 사용할 수 있다.  
>   즉, **적응을 위한 공간이 늘어날수록  
>   입력 데이터에 할당할 수 있는 공간이 줄어드는 구조적 제약**이 생긴다.  
> 
> - 이러한 이유로 프리픽스 튜닝은  
>   특히 긴 문맥(long-context)이나 복잡한 입력을 처리해야 하는 작업에서  
>   성능이 저하될 가능성이 있다.

---

## 4. 우리의 방법 (Our Method)  

이 절에서는 **LoRA의 간단한 설계(simple design)** 와  
그 **실용적 장점(practical benefits)** 에 대해 설명한다.  

여기서 제시하는 원칙(principles)은  
딥러닝 모델의 모든 **밀집층(dense layers)** 에 적용 가능하지만,  
본 연구에서는 **트랜스포머(Transformer) 기반 언어 모델**의  
특정 가중치(weights)에만 초점을 맞춘다.  
이는 LoRA의 주요 동기 사례(motivating use case)이기 때문이다.

### **4.1 저랭크 기반의 파라미터화된 갱신 행렬 (Low-Rank-Parametrized Update Matrices)**  

신경망(neural network)은 여러 개의 **밀집층(dense layer)** 으로 구성되어 있으며,  
각 층은 **행렬 곱(matrix multiplication)** 연산을 수행한다.  
이 층들의 가중치 행렬(weight matrix)은 일반적으로 **풀 랭크(full-rank)** 구조를 가진다.  

---

> **(블로그 추가 설명) 풀 랭크(Full-Rank)란 무엇인가?**  
> - **풀 랭크(full-rank)** 행렬이란,  
>   행렬이 가질 수 있는 **최대 차수(rank)** 를 모두 가진 경우를 의미한다.  
>   즉, $d \times k$ 크기의 행렬이라면  
>   그 랭크(rank)가 $\min(d, k)$ 인 상태를 말한다.  
> 
> - 행렬의 **랭크(rank)** 는  
>   행 또는 열이 서로 **선형적으로 독립(linearly independent)** 한 정도를 나타내며,  
>   이는 곧 행렬이 표현할 수 있는 **정보의 다양성(information diversity)** 을 의미한다.  
> 
> - 예를 들어,  
>   어떤 가중치 행렬이 풀 랭크라면  
>   모든 출력 방향(output direction)을  
>   입력 공간(input space)의 선형 조합(linear combination)으로 표현할 수 있다.  
>   즉, **정보 손실 없이 입력을 변환할 수 있는 완전한 표현 능력(full representational capacity)** 을 가진다.  
> 
> - 반대로, 랭크가 낮은 행렬(저랭크 행렬, low-rank matrix)은  
>   일부 방향으로의 변환 능력이 제한되어 있으며,  
>   이는 계산량을 줄이는 대신 **표현력(expressiveness)** 을 희생한다.  
> 
> - LoRA는 이러한 특성을 이용하여,  
>   기존의 풀 랭크 행렬을 직접 학습하는 대신  
>   **저랭크 근사(low-rank approximation)** 를 통해  
>   훨씬 적은 파라미터로 유사한 변환을 수행하도록 설계된 것이다.

---

특정 작업(task)에 적응(adaptation)할 때,  
**Aghajanyan et al. (2020)** 은  
사전학습된 언어 모델(pre-trained language model)이  
**낮은 “내재 차원(intrinsic dimension)”** 을 가지고 있으며,  
무작위 투영(random projection)을 통해  
더 작은 부분공간(subspace)으로 축소되더라도  
효율적으로 학습할 수 있음을 보였다.  

이 결과에 영감을 받아,  
우리는 **적응 과정에서의 가중치 변화(weight update)** 역시  
낮은 **내재 랭크(intrinsic rank)** 를 가진다고 가정(hypothesize)한다.  

사전학습된 가중치 행렬이 $W_0 \in \mathbb{R}^{d \times k}$ 일 때,  
그 갱신(update)을 **저랭크 분해(low-rank decomposition)** 형태로 제한한다.  

$$
W_0 + \Delta W = W_0 + BA
$$  

여기서 $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$이며,  
랭크 $r \ll \min(d, k)$ 이다.  

---

> **(블로그 추가 설명) $d$와 $k$의 의미**  
> - 여기서 $d$와 $k$는 각각 **행렬의 차원(dimension)** 을 나타낸다.  
>   구체적으로, $W_0 \in \mathbb{R}^{d \times k}$ 는  
>   **입력 벡터(input vector)** 를 **출력 벡터(output vector)** 로 변환하는  
>   **가중치 행렬(weight matrix)** 이다.  
> 
> - $k$는 **입력 차원(input dimension)**,  
>   즉 한 데이터 샘플(토큰 등)을 표현하는 벡터의 크기이다.  
>   반면 $d$는 **출력 차원(output dimension)** 으로,  
>   변환 후 벡터가 표현되는 공간의 크기를 의미한다.  
> 
> - 예를 들어,  
>   트랜스포머의 셀프-어텐션(self-attention) 구조에서  
>   쿼리(query)나 밸류(value) 투영 행렬이  
>   $W_q, W_v \in \mathbb{R}^{d \times k}$ 라면,  
>   입력 임베딩의 차원이 $k$이고,  
>   어텐션 공간의 출력 차원이 $d$가 된다.  
> 
> - LoRA에서는 이 가중치 행렬의 업데이트를  
>   $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$ 로 분해하여  
>   **저랭크 근사(low-rank approximation)** 를 수행한다.  
>   여기서 $r$은 $d$와 $k$보다 훨씬 작은 값으로 설정되어,  
>   계산량과 파라미터 수를 크게 줄이면서도  
>   원래의 표현력을 최대한 보존할 수 있도록 한다.

---

학습 과정 동안 $W_0$는 **고정(frozen)** 되어  
그래디언트 업데이트를 받지 않으며,  
$A$와 $B$만이 **학습 가능한 파라미터(trainable parameters)** 를 포함한다.  

$W_0$와 $\Delta W = BA$ 는  
모두 동일한 입력(input)을 곱하고,  
그 결과(output) 벡터는 **좌표 단위로 합산(coordinate-wise summation)** 된다.  

$h = W_0 x$ 라고 할 때, 수정된 순전파(modified forward pass)는 다음과 같다:

$$
h = W_0 x + \Delta W x = W_0 x + BAx
\tag{3}
$$

이 재매개변수화(reparametrization)는 **그림 1(Figure 1)** 에 시각화되어 있다.  

초기화 시, $A$는 **정규분포(random Gaussian)** 로 $B$는 **0(zero)** 으로 설정하여,  
학습 초기에 $\Delta W = BA = 0$ 이 되도록 한다.  

그 후, $\Delta W x$를 $\dfrac{\alpha}{r}$로 스케일링(scale)한다.  
여기서 $\alpha$는 랭크 $r$에 따라 설정되는 상수(constant)이다.  

**Adam 옵티마이저**를 사용할 때, $\alpha$ 값을 조정하는 것은  
초기화 스케일을 적절히 설정했을 경우 **학습률(learning rate)** 을 조정하는 것과 유사하다.  
따라서 우리는 시도하는 첫 번째 랭크 $r$에 대해 $\alpha$를 설정하고 이후 추가 조정은 하지 않는다.  

이러한 스케일링은 $r$ 값을 변화시킬 때마다 하이퍼파라미터를 다시 조정해야 하는 필요성을 줄여준다  
(Yang & Hu, 2021).

---

> **(블로그 추가 설명) $\alpha$를 고정해도 되는 이유**  
> - LoRA에서는 학습 중 가중치 갱신항 $\Delta W x = BAx$에  
>   $\dfrac{\alpha}{r}$이라는 스케일링 계수를 곱해 사용한다.  
>   따라서 전체 식은 다음과 같이 표현된다.  
>
>   $$
>   h = W_0x + \frac{\alpha}{r}BAx
>   $$  
> 
> - 이렇게 하면 **랭크 $r$이 달라져도**  
>   업데이트의 크기(scale)가 자동으로 보정된다.  
>   예를 들어 $r$이 커지면 자유도가 늘어나고 $\Delta W$의 크기가 커지지만,  
>   동시에 $\dfrac{1}{r}$로 나누어 주기 때문에 변화폭이 일정하게 유지된다.  
> 
> - 논문에서 “$\alpha$를 처음 시도하는 $r$ 값으로 고정한다”고 말하는 이유는  
>   바로 이 자동 보정 덕분이다.  
>   $\alpha$를 한 번 정하면 이후 다른 $r$ 값을 실험하더라도  
>   $\dfrac{\alpha}{r}$ 항이 알아서 균형을 맞춰주므로  
>   **$\alpha$나 학습률을 다시 튜닝할 필요가 없다.**  
> 
> - 즉, $\alpha$는 고정된 상수이지만  
>   실제 적용은 $\dfrac{\alpha}{r}$ 형태로 이루어지기 때문에  
>   **랭크가 달라져도 학습 안정성(stability)** 을 유지할 수 있는 것이다.

---

#### **전체 미세조정의 일반화 (A Generalization of Full Fine-Tuning)**  

보다 일반적인 형태의 **미세조정(fine-tuning)** 은  
사전학습된 파라미터 중 일부만을 학습 대상으로 삼는 방식을 허용한다.  

**LoRA**는 이보다 한 단계 더 나아가,  
적응(adaptation) 과정에서 가중치 행렬(weight matrix)에 적용되는  
**누적 그래디언트 업데이트(accumulated gradient update)** 가  
**풀 랭크(full-rank)** 일 필요가 없다고 가정한다.  

이는 곧 LoRA를 모든 가중치 행렬에 적용하고  
모든 **바이어스(bias)**<sup>2</sup> 를 함께 학습시킬 때,  
**사전학습된 가중치 행렬의 랭크(rank)** 와  
**LoRA의 랭크 $r$** 를 동일하게 설정하면  
**전체 미세조정(full fine-tuning)** 과 거의 동일한 수준의  
**표현력(expressiveness)** 을 복원할 수 있음을 의미한다.  

---

>**주석**  
>
><sup>2</sup> 바이어스는 가중치에 비해 차지하는 파라미터 수가 매우 적다.  

---

즉, 학습 가능한 파라미터의 수를 점진적으로 늘려감에 따라,  
**LoRA의 학습은 원래 모델의 학습과 점점 수렴(converge)** 하게 된다.  

반면,  
**어댑터(adapter) 기반 방법**은  
이론적으로 **MLP(다층 퍼셉트론)** 에 수렴하며,  
**프리픽스(prefix)** 기반 방법은  
**긴 입력 시퀀스(long input sequences)** 를 처리하지 못하는 모델로 수렴한다.<sup>3</sup>

---

>**주석**  
>
><sup>3</sup> 이는 복잡한(hard) 작업에 적응할 때 불가피한 현상이다.

---

#### **추가적인 추론 지연이 없음 (No Additional Inference Latency)**  

프로덕션 환경(production environment)에서 모델을 배포할 때,  
다음과 같이 명시적으로 계산하여 저장할 수 있다.  

$$
W = W_0 + BA
$$  

그 후 **일반적인 방식으로(in the usual way)** 추론(inference)을 수행하면 된다.  
여기서 $W_0$와 $BA$는 모두 $\mathbb{R}^{d \times k}$ 공간에 속한다.  

다른 전이 학습(downstream) 작업으로 전환해야 하는 경우에는,  
$BA$를 빼서 원래의 $W_0$를 복원한 뒤,  
새로운 작업에 해당하는 $B'A'$를 더하면 된다.  
이 연산은 매우 빠르며 **메모리 오버헤드(memory overhead)** 도 거의 없다.  

가장 중요한 것은, 이러한 설계에 의해  
**LoRA는 추론 중(inference) 완전 미세조정(fine-tuned) 모델과 비교했을 때  
어떠한 추가적인 지연(latency)도 유발하지 않는다**는 점이다.


### **4.2 트랜스포머에 LoRA 적용하기 (Applying LoRA to Transformer)**  

이론적으로, **LoRA**는  
신경망(neural network) 내의 **임의의 가중치 행렬(weight matrix) 부분집합**에  
적용하여 학습 가능한 파라미터 수를 줄일 수 있다.  

**트랜스포머(Transformer)** 아키텍처에서는  
**셀프-어텐션(self-attention)** 모듈에  
4개의 가중치 행렬 $W_q, W_k, W_v, W_o$ 가 있고,  
**MLP 모듈**에는 2개의 가중치 행렬이 존재한다.  

우리는 $W_q$ (또는 $W_k$, $W_v$)를  
출력 차원이 일반적으로 **어텐션 헤드(attention heads)** 로 분할되어 있더라도,  
**단일 행렬(single matrix)** $d_{\text{model}} \times d_{\text{model}}$ 크기의 행렬로 간주한다.  

본 연구에서는 단순성과 파라미터 효율성(parameter-efficiency)을 위해  
**전이 학습(downstream task)** 시  
**어텐션 가중치(attention weights)** 만 적응(adapt)시키고,  
**MLP 모듈은 고정(freeze)** 시켜 학습하지 않는다.  

또한, **7.1절(Section 7.1)** 에서  
트랜스포머 내 서로 다른 유형의 어텐션 가중치 행렬에  
LoRA를 적용할 때의 효과를 추가로 분석한다.  

한편, **MLP 층, LayerNorm 층, 바이어스(biases)** 에  
LoRA를 적용하는 것에 대한 실증적 연구(empirical investigation)는  
**향후 연구(future work)** 로 남겨둔다.

#### **실용적 이점과 한계 (Practical Benefits and Limitations)**  

가장 큰 이점은 **메모리(memory)** 와 **저장 공간(storage)** 사용량의 감소이다.  

대규모 트랜스포머(Transformer)를 **Adam 옵티마이저**로 학습할 때,  
$r \ll d_{\text{model}}$ 인 경우  
고정된 파라미터(frozen parameters)에 대한 옵티마이저 상태(optimizer states)를  
저장할 필요가 없으므로 **VRAM 사용량을 최대 2/3까지 줄일 수 있다.**  

예를 들어, **GPT-3 (175B)** 모델의 경우  
학습 중 VRAM 소비량이 **1.2TB에서 350GB로 감소**한다.  

$r = 4$이고 **쿼리(query)** 및 **밸류(value)** 투영 행렬만을 적응시킬 때,  
체크포인트(checkpoint) 크기는 약 **10,000배 감소**한다  
(350GB → 35MB)<sup>4</sup>.  

---

>**주석**  
>
><sup>4</sup> 배포(deployment) 시에는 여전히 **350GB 크기의 기본 모델**이 필요하다.  
>그러나 100개의 적응된 모델(adapted models)을 저장하더라도  
>필요한 용량은 **350GB + (35MB × 100) ≈ 354GB**에 불과하다.  
>이는 각각의 모델을 별도로 저장했을 때 필요한 **100 × 350GB ≈ 35TB**에 비해  
>극적으로 작다.  

---

이로 인해 훨씬 적은 수의 GPU로 학습이 가능해지고,  
**입출력 병목(I/O bottleneck)** 문제를 피할 수 있다.  

또한, LoRA 가중치만 교체하면  
모든 파라미터를 교체할 필요 없이  
**배포 중에도 낮은 비용으로 작업 간 전환(task switching)** 이 가능하다.  

이 덕분에 VRAM에 사전학습된 가중치를 저장한 채로  
다양한 **맞춤형(customized) 모델**을  
**실시간(on the fly)** 으로 교체하여 사용할 수 있다.  

마지막으로, **GPT-3 (175B)** 에서 전체 미세조정(full fine-tuning)과 비교했을 때  
**약 25%의 학습 속도 향상(speedup)** 이 관찰되었다.<sup>5</sup>  

이는 전체 파라미터 중 대다수에 대해  
그래디언트를 계산할 필요가 없기 때문이다.

---

>**주석**  
>
><sup>5</sup> **GPT-3 (175B)** 의 경우,  
>**전체 미세조정(full fine-tuning)** 의 학습 처리 속도(training throughput)는  
>**V100 GPU당 32.5 tokens/s** 이다.  
>동일한 모델 병렬화(model parallelism) 분할 수(weight shards)를 사용했을 때,  
>**LoRA** 의 처리 속도는 **V100 GPU당 43.1 tokens/s** 로 향상된다.

---

LoRA에도 몇 가지 한계점(limitations)이 존재한다.  

예를 들어, 추가적인 추론 지연(inference latency)을 제거하기 위해  
$A$와 $B$를 $W$에 흡수(absorb)하기로 선택한 경우,  
서로 다른 작업(tasks)에 대해 서로 다른 $A$와 $B$를 가진 입력들을  
**하나의 순전파(single forward pass)** 에서 배치(batch) 처리하는 것은 간단하지 않다.  

그러나 지연(latency)이 중요하지 않은 시나리오에서는,  
가중치(weights)를 병합하지 않고  
배치 내의 각 샘플(sample)에 사용할 **LoRA 모듈**을  
**동적으로 선택(dynamically choose)** 하는 것이 가능하다.

---

> **(블로그 추가 설명) 왜 서로 다른 작업을 한 번에 배치 처리하기 어려운가?**  
> - LoRA에서는 각 작업(task)마다 고유한 $A$와 $B$ 행렬을 가진다.  
>   즉, “작업 A용 LoRA”와 “작업 B용 LoRA”는 서로 다른 파라미터 집합을 사용한다.  
> 
> - 그런데 만약 $A$와 $B$를 **기본 가중치 $W$에 미리 흡수(absorb)** 해버리면  
>   LoRA가 원래의 모델 구조에 완전히 통합되어 버린다.  
>   이렇게 되면 한 번의 순전파(forward pass)에서  
>   여러 작업이 서로 다른 $A$와 $B$를 동시에 사용하는 것이 불가능해진다.  
>   (즉, 한 모델이 한 가지 LoRA 설정만 사용할 수 있는 셈이다.)  
> 
> - 반면, **지연(latency)** 이 중요하지 않은 상황이라면  
>   $A$와 $B$를 미리 합치지 않고 그대로 분리해 둔 채로  
>   **배치 내 각 샘플(sample)** 이 서로 다른 LoRA 모듈을  
>   **동적으로 선택(dynamically select)** 하여 사용할 수 있다.  
>   예를 들어, 같은 배치에 “요약 작업용 LoRA”와 “번역 작업용 LoRA”가 섞여 있어도  
>   각 입력마다 대응되는 LoRA 모듈을 지정해 처리할 수 있다는 뜻이다.  
> 
> - 그러나 이 방식은 LoRA 모듈을 매번 선택하고 적용해야 하므로  
>   연산 효율이 떨어지고, 실제 서비스 환경(online inference)에서는  
>   속도 저하(latency increase)가 발생할 수 있다.

---

## 5. 실증 실험 (Empirical Experiments)  

우리는 **RoBERTa** (Liu et al., 2019), **DeBERTa** (He et al., 2021),  
그리고 **GPT-2** (Radford et al., b)에서 **LoRA**의 전이 학습(downstream) 성능을 평가하고,  
그 후 이를 **GPT-3 (175B)** (Brown et al., 2020)로 확장한다.  

본 실험은 **자연어 이해(NLU, Natural Language Understanding)** 부터  
**자연어 생성(NLG, Natural Language Generation)** 까지  
넓은 범위의 작업(tasks)을 포함한다.  

구체적으로,  
**RoBERTa**와 **DeBERTa**는 **GLUE 벤치마크(GLUE benchmark)** (Wang et al., 2019)를 기준으로 평가한다.  

**GPT-2**의 경우, **Li & Liang (2021)** 의 설정을 그대로 따라  
직접적인 비교(direct comparison)를 수행하며,  
대규모 실험(large-scale experiments)을 위해  
**GPT-3**에는 다음 두 작업을 추가한다:  
- **WikiSQL** (Zhong et al., 2017): 자연어 질의(NL)를 SQL 쿼리로 변환  
- **SAMSum** (Gliwa et al., 2019): 대화 요약(conversation summarization)  

사용된 데이터셋에 대한 더 자세한 내용은 **부록 C(Appendix C)** 를 참고한다.  

모든 실험은 **NVIDIA Tesla V100 GPU** 환경에서 수행하였다.

### **5.1 베이스라인 (Baselines)**  

다른 베이스라인(baseline)들과의 폭넓은 비교를 위해,  
우리는 기존 연구에서 사용된 설정(setup)을 재현(replicate)하고,  
가능한 경우 그들이 보고한 수치(reported numbers)를 그대로 활용하였다.  
그러나 이러한 방식은 일부 베이스라인이  
특정 실험에서만 등장할 수도 있음을 의미한다.  

**미세조정(Fine-Tuning, FT)** 은  
적응(adaptation)을 위한 일반적인 접근법이다.  
미세조정 과정에서 모델은  
**사전학습된 가중치(pre-trained weights)** 와 **바이어스(biases)** 로 초기화되고,  
이후 **모든 모델 파라미터(model parameters)** 가  
**그래디언트 업데이트(gradient updates)** 를 받는다.  

단순한 변형으로는,  
일부 층(layers)만 업데이트하고  
나머지 층을 **고정(freeze)** 하는 방식이 있다.  
이 중 하나로, **Li & Liang (2021)** 이 **GPT-2**에서 보고한  
**FT<sup>Top2</sup>** 베이스라인을 포함하였다.  
이는 **마지막 두 개의 층만 적응(adapt)** 시키는 미세조정 방식이다.

**Bias-only (또는 BitFit)** 은  
모델의 **바이어스 벡터(bias vectors)** 만 학습시키고,  
그 외의 모든 파라미터는 **고정(freeze)** 시키는 베이스라인이다.  
이 접근은 **BitFit (Zaken et al., 2021)** 에서도  
동일한 방식으로 연구된 바 있다.  

---

> **(블로그 추가 설명) Bias-only (BitFit) 방식의 의미**  
> - **바이어스(bias)** 는 각 뉴런의 출력값을 조정하는 **상수 항(constant term)** 으로,  
>   모델이 입력값에 관계없이 특정 방향으로 출력을 이동(shift)시키는 역할을 한다.  
> 
> - **Bias-only 학습** 또는 **BitFit** 방식은  
>   가중치(weight)는 그대로 둔 채,  
>   이 바이어스 항만 미세하게 조정(fine-tune)함으로써  
>   새로운 작업(task)에 적응하도록 하는 매우 단순한 접근이다.  
> 
> - 이 방법의 장점은 **학습해야 할 파라미터 수가 극도로 적다**는 점이다.  
>   전체 파라미터 중 바이어스 항은 0.1% 이하에 불과하므로,  
>   적은 자원으로도 빠르게 전이 학습이 가능하다.  
> 
> - 반면, **모델의 표현력(expressive power)** 은 제한된다.  
>   가중치 행렬을 전혀 수정하지 않기 때문에,  
>   복잡한 작업에서는 충분한 적응력을 확보하기 어렵다.  
> 
> - 즉, BitFit은 **효율성과 단순성**을 극대화하지만,  
>   **성능 향상 폭은 제한적**인 파라미터 효율적 미세조정(PEFT) 기법이다.

---

**Prefix-embedding tuning (PreEmbed)** 은  
입력 토큰들 사이에 **특수 토큰(special tokens)** 을 삽입하는 방식이다.  
이 특수 토큰들은 **학습 가능한 단어 임베딩(trainable word embeddings)** 을 가지며,  
일반적으로 모델의 기존 어휘(vocabulary)에는 포함되지 않는다.  

이러한 특수 토큰을 **어디에 위치시킬지** 는  
모델의 성능에 영향을 줄 수 있다.  
본 논문에서는 두 가지 방식에 초점을 맞춘다.  
- **프리픽싱(prefixing)** : 특수 토큰을 **프롬프트(prompt)** 앞에 추가하는 방식  
- **인픽싱(infixing)** : 특수 토큰을 **프롬프트 뒤에 추가하는 방식**  

이 두 방법은 **Li & Liang (2021)** 에서 모두 논의되었다.  

프리픽스(prefix) 토큰의 개수를 $l_p$,  
인픽스(infix) 토큰의 개수를 $l_i$ 라고 할 때,  
학습 가능한 파라미터의 수는 다음과 같다.  

$$
|\Theta| = d_{\text{model}} \times (l_p + l_i)
$$  

---

> **(블로그 추가 설명) Prefix-embedding tuning 방식의 개념**  
> - **Prefix-embedding tuning (PreEmbed)** 은  
>   모델이 입력(prompt)을 해석하기 전에  
>   **학습 가능한 임베딩 벡터(learnable embedding vectors)** 를  
>   인위적으로 삽입해 주는 방식이다.  
>   이 임베딩은 마치 “작업(task)별 가이드 문장”처럼 동작하며,  
>   모델이 어떤 문맥(context)에서 작동해야 하는지를 미리 조정한다.  
> 
> - 이 접근법은 **프롬프트(prompt) 기반 학습(prompt-based learning)** 의 한 형태로,  
>   사전학습된 언어모델이 기존에 학습한 지식을 그대로 유지한 채  
>   새로운 작업에 적응할 수 있도록 돕는다.  
> 
> - 예를 들어, “문장 요약” 작업을 학습할 때  
>   입력 앞에 `[SUMMARIZE]` 같은 토큰을 추가하면,  
>   모델은 이 임베딩을 통해 해당 문맥을 “요약” 문제로 인식하게 된다.  
> 
> - **프리픽싱(prefixing)** 과 **인픽싱(infixing)** 은  
>   이러한 특수 토큰을 어디에 삽입하느냐의 차이에 불과하다.  
>   - 프리픽싱(prefixing): 입력 앞에 붙여 모델이 가장 먼저 인식하도록 함  
>   - 인픽싱(infixing): 입력 뒤쪽에 붙여 문맥 정보를 보완하도록 함  
> 
> - 학습 가능한 파라미터 수는  
>   각 특수 토큰의 임베딩 크기($d_{\text{model}}$)와  
>   토큰의 개수($l_p$, $l_i$)에 비례하며,  
>   수식으로는 다음과 같이 표현된다.  
>   $$
>   |\Theta| = d_{\text{model}} \times (l_p + l_i)
>   $$  
> 
> - 즉, PreEmbed는 **가중치(weight)** 나 **레이어(layer)** 를 바꾸지 않고도  
>   **문맥을 조절(contextual adaptation)** 하는 방법으로,  
>   **추가 파라미터가 매우 적고 구조 변경이 필요 없는 장점**이 있다.

---

**Prefix-layer tuning (PreLayer)** 은  
**Prefix-embedding tuning** 의 확장(extension)이다.  
이 방법은 단지 특수 토큰의 **단어 임베딩(word embeddings)**  
또는 임베딩 층 이후의 **활성값(activations)** 만 학습하는 것이 아니라,  
**각 트랜스포머 층(Transformer layer)** 이후의  
활성값 전체를 학습한다.  

이때 이전 층에서 계산된 활성값들은  
단순히 학습 가능한 값으로 **치환(replace)** 된다.  

그 결과, 학습 가능한 파라미터의 총 수는 다음과 같다.  

$$
|\Theta| = L \times d_{\text{model}} \times (l_p + l_i)
$$  

여기서 $L$은 **트랜스포머 층의 개수(number of Transformer layers)** 를 의미한다.

---

> **(블로그 추가 설명) Prefix-layer tuning 방식의 개념**  
> - **Prefix-layer tuning (PreLayer)** 은  
>   **Prefix-embedding tuning** 보다 한 단계 더 깊은 수준에서  
>   모델의 내부 표현(internal representation)을 조정하는 방법이다.  
> 
> - PreEmbed가 입력 임베딩(input embedding) 수준에서만  
>   “특수 토큰의 표현”을 학습하는 반면,  
>   PreLayer는 **트랜스포머의 각 층(layer)** 마다  
>   새로운 **학습 가능한 활성값(learnable activations)** 을 삽입한다.  
> 
> - 즉, 원래는 이전 층의 출력을 다음 층의 입력으로 사용하지만,  
>   PreLayer에서는 이 출력을 **완전히 교체(replace)** 하여  
>   “학습 가능한 프리픽스(prefix)”로 대체한다.  
>   이로써 모델은 각 층에서 독립적으로  
>   새로운 문맥 정보를 주입(inject)할 수 있게 된다.  
> 
> - 결과적으로, PreLayer는  
>   트랜스포머 전체를 관통하는 **계층적 프리픽스 구조(hierarchical prefixing)** 를 형성하며,  
>   더 강력한 조정 능력을 제공한다.  
>   하지만 동시에, 학습해야 할 파라미터 수도 늘어난다.  
> 
> - 실제로, 전체 학습 가능한 파라미터 수는  
>   트랜스포머 층의 개수 $L$에 비례하여 증가한다.  
>   $$
>   |\Theta| = L \times d_{\text{model}} \times (l_p + l_i)
>   $$  
> 
> - 정리하자면,  
>   PreLayer는 **각 층의 출력 공간에서 직접 적응(adaptation)을 수행**하는 기법으로,  
>   PreEmbed보다 훨씬 유연하지만,  
>   그만큼 **계산량(computation)** 과 **메모리 요구량(memory usage)** 이 커지는 단점이 있다.

---

**어댑터 튜닝(Adapter tuning)** 은 **Houlsby et al. (2019)** 에 의해 제안된 방법으로,  
**셀프-어텐션(self-attention)** 모듈(및 **MLP 모듈**)과  
그 뒤의 **잔차 연결(residual connection)** 사이에  
**어댑터 층(adapter layer)** 을 삽입한다.  

각 어댑터 층은 **두 개의 완전연결층(fully connected layers)** 과  
그 사이에 위치한 **비선형 함수(non-linearity)** 로 구성되며,  
각 층에는 **바이어스(bias)** 도 포함된다.  
이 원래의 설계를 **Adapter<sup>H</sup>** 라고 부른다.  

최근 **Lin et al. (2020)** 은  
더 효율적인 설계를 제안하였는데,  
이는 **MLP 모듈 뒤**와 **LayerNorm 이후**에만  
어댑터 층을 적용하는 방식이다.  
이 설계를 **Adapter<sup>L</sup>** 이라고 한다.  

또한, **Pfeiffer et al. (2021)** 이 제안한  
매우 유사한 또 다른 변형이 있으며,  
이를 **Adapter<sup>P</sup>** 라고 부른다.  

우리는 또한 **Rücklé et al. (2020)** 이 제안한  
**AdapterDrop (Adapter<sup>D</sup>)** 베이스라인도 포함한다.  
이 방법은 일부 어댑터 층을 제거(drop)하여  
효율성을 더욱 높인다.  

비교 가능한 베이스라인의 수를 최대화하기 위해,  
가능한 경우 기존 연구에서 보고된 수치(reported results)를 인용하였으며,  
표에서 별표(*)로 표시된 행이 이에 해당한다.  

모든 경우에서 학습 가능한 파라미터의 수는 다음과 같이 주어진다.  

$$
|\Theta| = 
\hat{L}_{\text{Adpt}} \times (2 \times d_{\text{model}} \times r + r + d_{\text{model}}) 
+ 2 \times \hat{L}_{\text{LN}} \times d_{\text{model}}
$$  

여기서  
$$\hat{L}_{\text{Adpt}}$$ 는 **어댑터 층의 개수(number of adapter layers)**,  
$$\hat{L}_{\text{LN}}$$ 은 **학습 가능한 LayerNorm의 개수(number of trainable LayerNorms)** 를 의미한다.  
(예: AdapterL의 경우 LayerNorm을 학습시킴)  

---

> **(블로그 추가 설명) 어댑터(Adapter) 튜닝의 구조적 특징**  
> - **어댑터(Adapter)** 는 대형 언어모델의 모든 파라미터를 미세조정하지 않고,  
>   **기존 층(layer) 사이에 작은 모듈을 삽입**하여 새로운 작업에 적응하도록 하는 기법이다.  
>   즉, 모델의 주요 가중치는 그대로 두고,  
>   **작고 얕은(subnetwork)** 네트워크만 추가 학습하는 형태이다.  
> 
> - 어댑터 층은 일반적으로 다음과 같은 구조를 가진다.  
>
>   $$
>   h' = h + f(W_{\text{up}} \, \sigma(W_{\text{down}} h))
>   $$  
>
>   여기서  
>   $W_{\text{down}}$ 은 **차원 축소(bottleneck)** 를 위한 작은 행렬,  
>   $W_{\text{up}}$ 은 다시 **확장(projection back)** 하는 행렬이며,  
>   $\sigma$ 는 비선형 함수(예: ReLU, GELU)이다.  
>   이렇게 “입력 → 축소 → 비선형 변환 → 복원” 과정을 거치면서  
>   모델은 작은 파라미터만으로 새로운 표현을 학습할 수 있다.  
> 
> - **Adapter<sup>H</sup> (Houlsby-style)** 는  
>   셀프-어텐션과 MLP 모듈 **모두 뒤에** 어댑터를 삽입하여  
>   비교적 풍부한 적응 표현을 학습하지만, 연산량이 다소 증가한다.  
> 
> - **Adapter<sup>L</sup> (Lin-style)** 은  
>   MLP 뒤쪽과 LayerNorm 이후에만 어댑터를 적용하여  
>   파라미터 수와 연산량을 줄인 효율적인 변형이다.  
> 
> - **Adapter<sup>P</sup> (Pfeiffer-style)** 은 AdapterL과 구조가 매우 유사하며,  
>   위치와 초기화 방식의 세부 차이만 존재한다.  
> 
> - **Adapter<sup>D</sup> (AdapterDrop)** 은  
>   일부 어댑터 층을 제거(drop)함으로써  
>   훈련 효율을 높이는 방법으로,  
>   메모리 사용량과 추론 시간을 줄일 수 있다.  
> 
> - 이처럼 어댑터 계열의 방법들은  
>   **추가되는 파라미터 수가 매우 적고**,  
>   기존 모델의 표현을 크게 손상시키지 않으면서  
>   새로운 작업으로 빠르게 적응할 수 있다는 장점이 있다.  
>   그러나 모든 층을 순차적으로 거치므로,  
>   **추론 시 지연(latency)** 이 발생한다는 한계도 존재한다.

---

한편, **LoRA** 는 기존 가중치 행렬(weight matrix)에  
**랭크 분해(rank decomposition)** 행렬 쌍을 병렬로 추가하여 학습 가능한 구조를 만든다.  
**4.2절** 에서 언급한 바와 같이,  
대부분의 실험에서는 단순화를 위해  
**$W_q$** 와 **$W_v$** 에만 LoRA를 적용하였다.  

LoRA의 학습 가능한 파라미터 수는  
**랭크 $r$** 과 원래 가중치의 형태(shape)에 따라 결정된다.  

$$
|\Theta| = 2 \times \hat{L}_{\text{LoRA}} \times d_{\text{model}} \times r
$$  

여기서  
$\hat{L}_{\text{LoRA}}$ 는 **LoRA가 적용된 가중치 행렬의 개수**를 의미한다.

---

> **(블로그 추가 설명) LoRA의 구조적 특징과 파라미터 효율성**  
> - **LoRA (Low-Rank Adaptation)** 의 핵심 아이디어는  
>   기존의 가중치 행렬 $W$ 를 직접 수정하지 않고,  
>   그 변화량(update)을 **저랭크 행렬 쌍 $A$, $B$** 의 곱으로 근사하는 것이다.  
>   즉,  
>
>   $$
>   W' = W + BA
>   $$  
>
>   와 같이 표현되며, 여기서 $B \in \mathbb{R}^{d_{\text{model}} \times r}$,  
>   $A \in \mathbb{R}^{r \times d_{\text{model}}}$,  
>   그리고 $r \ll d_{\text{model}}$ 이다.  
> 
> - 이 구조 덕분에 LoRA는 **학습해야 할 파라미터 수를 대폭 줄이면서도**,  
>   **원래의 모델 표현력을 유지**할 수 있다.  
>   왜냐하면, 모델의 변화량 $\Delta W$ 를  
>   고차원(full rank) 공간이 아니라  
>   **저차원(low-rank) 부분공간(subspace)** 내에서만 학습하기 때문이다.  
> 
> - 대부분의 실험에서 LoRA는  
>   **쿼리($W_q$)** 와 **밸류($W_v$)** 투영 행렬에만 적용되었다.  
>   이는 어텐션(attention) 계산에서  
>   가장 중요한 정보 흐름을 담당하는 두 행렬에 집중함으로써,  
>   효율성과 성능의 균형을 맞추기 위한 설계이다.  
> 
> - LoRA의 총 학습 파라미터 수는  
>   다음과 같이 간단히 계산된다.  
>
>   $$
>   |\Theta| = 2 \times \hat{L}_{\text{LoRA}} \times d_{\text{model}} \times r
>   $$  
>
>   여기서 $\hat{L}_{\text{LoRA}}$ 는 LoRA가 적용된  
>   가중치 행렬(weight matrices)의 개수를 의미한다.  
> 
> - 즉, LoRA는 **랭크 $r$ 값**을 작게 설정함으로써  
>   전체 파라미터의 0.1% 이하만을 학습하면서도,  
>   완전 미세조정(full fine-tuning) 수준의 성능을 유지할 수 있는  
>   **매우 효율적인 전이 학습(parameter-efficient transfer learning)** 기법이다.

---

### **5.2 RoBERTa Base/Large**  

**RoBERTa (Liu et al., 2019)** 는  
**BERT (Devlin et al., 2019a)** 에서 제안된  
사전학습 방식을 개선한(pre-training recipe optimized) 모델로,  
학습 가능한 파라미터 수를 크게 늘리지 않으면서도 BERT의 성능을 향상시킨 모델이다.  

최근 몇 년간 **RoBERTa**는  
**GLUE 벤치마크 (Wang et al., 2019)** 와 같은  
**NLP 리더보드**에서 훨씬 더 큰 모델들에 의해 추월당했지만,  
그럼에도 불구하고 **모델 크기 대비 경쟁력 있고 인기 있는 사전학습 모델**로  
여전히 많은 연구자들(practitioners) 사이에서 사용되고 있다.  

우리는 **HuggingFace Transformers 라이브러리 (Wolf et al., 2020)** 에서  
사전학습된 **RoBERTa Base (125M)** 와 **RoBERTa Large (355M)** 모델을 사용하였으며,  
**GLUE 벤치마크**의 여러 과제(tasks)에 대해  
다양한 **효율적 적응 기법(efficient adaptation approaches)** 의 성능을 비교 평가하였다.  

또한 **Houlsby et al. (2019)** 과 **Pfeiffer et al. (2021)** 의 설정을 재현(replicate)하여  
동일한 조건에서 비교하였다.  

공정한 비교(fair comparison)를 위해,  
어댑터(Adapter) 방식과 LoRA를 비교할 때  
다음 두 가지 주요 변경 사항을 적용하였다.  

1. 모든 과제에서 동일한 **배치 크기(batch size)** 를 사용하고,  
   **시퀀스 길이(sequence length)** 는 128로 설정하여  
   어댑터 베이스라인(adapter baselines)의 설정과 일치시켰다.  
2. **MRPC, RTE, STS-B** 과제의 경우,  
   **MNLI로 이미 적응된 모델이 아닌**,  
   **사전학습된(pre-trained)** 모델에서 직접 초기화하였다.  
   (이는 미세조정(fine-tuning) 베이스라인의 설정과 다르다.)  

**Houlsby et al. (2019)** 의 이보다 더 제한된 설정을 따른 실행(run)들은  
**†** 기호로 표시된다.    

결과는 **표 2 (상단 3개 구간)** 에 제시되어 있으며,  
사용된 **하이퍼파라미터(hyperparameter)** 의 세부 내용은  
**부록 D.1 (Section D.1)** 에서 확인할 수 있다.

---

**표 2**

**GLUE 벤치마크**에서 서로 다른 적응(adaptation) 방법을 적용한  
**RoBERTa-base**, **RoBERTa-large**, 그리고 **DeBERTa-XXL** 모델의 성능 비교.  

**MNLI**는 전체 정확도(overall accuracy, matched 및 mismatched 포함),  
**CoLA**는 **Matthew’s 상관 계수(Matthew’s correlation)**,  
**STS-B**는 **피어슨 상관 계수(Pearson correlation)**,  
그 외 과제들은 **정확도(accuracy)** 를 기준으로 평가하였다.  

모든 지표(metric)는 값이 높을수록 더 우수하다.  

별표(*)는 **기존 연구에서 발표된 수치(published results)** 를,  
더거 기호(†)는 **공정한 비교(fair comparison)** 를 위해  
**Houlsby et al. (2019)** 과 유사한 설정으로 구성된 실행(run)을 나타낸다.

<img src="/assets/img/paper/lora/image_4.png" alt="image" width="800px"> 

---

### **5.3 DeBERTa XXL**  

**DeBERTa (He et al., 2021)** 는  
**BERT**의 보다 최근 변형(modern variant)으로,  
훨씬 더 대규모 데이터로 학습되었으며,  
**GLUE (Wang et al., 2019)** 와 **SuperGLUE (Wang et al., 2020)** 와 같은  
벤치마크에서 매우 경쟁력 있는 성능을 보인다.  

우리는 **LoRA**가  
**완전 미세조정(full fine-tuning)** 된 **DeBERTa XXL (1.5B)** 모델의  
**GLUE 벤치마크** 성능을 여전히 따라잡을 수 있는지를 평가하였다.  

그 결과는 **표 2 (하단 구간)** 에 제시되어 있으며,  
사용된 **하이퍼파라미터(hyperparameters)** 의 세부 내용은  
**부록 D.2 (Section D.2)** 에서 확인할 수 있다.


### **5.4 GPT-2 Medium/Large**  

**LoRA**가 **자연어 이해(NLU)** 에서  
**전체 미세조정(full fine-tuning)** 의 경쟁력 있는 대안이 될 수 있음을 보여주었으므로,  
우리는 이제 **LoRA**가 **GPT-2 Medium** 및 **Large (Radford et al., b)** 와 같은  
**자연어 생성(NLG)** 모델에서도 여전히 우수한지를 알아보고자 한다.  

직접적인 비교를 위해,  
우리의 설정은 **Li & Liang (2021)** 과 가능한 한 유사하게 유지하였다.  

지면 제약(space constraint)으로 인해,  
이 절에서는 **E2E NLG Challenge** 에 대한 결과만 제시하며,  
그 내용은 **표 3** 에 나와 있다.  

**WebNLG (Gardent et al., 2017)** 및 **DART (Nan et al., 2020)** 의 결과는  
**부록 F.1 (Section F.1)** 에서 확인할 수 있으며,  
사용된 **하이퍼파라미터(hyperparameters)** 의 목록은  
**부록 D.3 (Section D.3)** 에 포함되어 있다.

---

**표 3**

**E2E NLG Challenge**에서 서로 다른 적응(adaptation) 방법을 적용한  
**GPT-2 Medium (M)** 과 **Large (L)** 모델의 성능 비교.  

모든 평가 지표(metric)에 대해 값이 높을수록 성능이 좋다.  
**LoRA**는 **유사하거나 더 적은 학습 가능한 파라미터 수**로  
여러 베이스라인보다 우수한 성능을 보였다.  

우리가 수행한 실험에 대해서는 **신뢰 구간(confidence intervals)** 이 함께 제시되어 있으며,  
별표(*)는 **기존 연구에서 발표된 수치(published results)** 를 나타낸다.

<img src="/assets/img/paper/lora/image_3.png" alt="image" width="720px"> 

---

### **5.5 GPT-3 175B로의 확장 (Scaling Up to GPT-3 175B)**  

**LoRA**의 최종적인 **스트레스 테스트(stress test)** 로서,  
우리는 **1750억 개의 파라미터(parameter)** 를 가진 **GPT-3** 모델로 확장하였다.  

높은 학습 비용(training cost)으로 인해,  
모든 항목마다 개별적인 표준편차(standard deviation)를 제공하는 대신,  
특정 과제(task)에 대해 **무작위 시드(random seeds)** 를 기준으로 한  
**일반적인 표준편차(typical standard deviation)** 만을 보고한다.  
사용된 **하이퍼파라미터(hyperparameters)** 의 세부 사항은  
**부록 D.4 (Section D.4)** 에 제시되어 있다.  

**표 4** 에서 보이듯,  
**LoRA**는 세 개의 데이터셋 모두에서  
**미세조정(fine-tuning) 베이스라인**과 비슷하거나 그 이상의 성능을 보였다.  

그러나 **그림 2(Figure 2)** 에서 볼 수 있듯이,  
모든 방법이 학습 가능한 파라미터 수가 많아질수록  
**단조롭게(monotonically)** 성능이 향상되는 것은 아니다.  

우리는 **Prefix-embedding tuning** 에서  
**256개 이상의 특수 토큰(special tokens)** 을 사용하거나,  
**Prefix-layer tuning** 에서  
**32개 이상의 특수 토큰** 을 사용할 경우,  
성능이 **유의미하게 하락(significant performance drop)** 하는 것을 관찰했다.  
이 결과는 **Li & Liang (2021)** 에서 보고된 유사한 관찰과 일치한다.  

이 현상에 대한 **심층 분석(thorough investigation)** 은  
본 연구의 범위를 벗어나지만,  
우리는 특수 토큰의 수가 많아질수록  
입력 분포(input distribution)가  
**사전학습 데이터 분포(pre-training data distribution)** 에서  
더 멀어지기 때문일 것으로 추정한다.  

또한, 우리는 **Section F.3** 에서  
**적은 데이터 환경(low-data regime)** 에서의  
다양한 적응(adaptation) 방법의 성능을 별도로 분석하였다.

---

**표 4**

**GPT-3 175B** 에서 서로 다른 적응(adaptation) 방법의 성능 비교.  

**WikiSQL** 에 대해서는 **논리 형태(logical form) 검증 정확도(validation accuracy)**,  
**MultiNLI-matched** 에 대해서는 **검증 정확도(validation accuracy)**,  
그리고 **SAMSum** 에 대해서는 **Rouge-1 / Rouge-2 / Rouge-L** 점수를 보고한다.  

**LoRA**는 **전체 미세조정(full fine-tuning)** 을 포함한  
이전 접근 방식들보다 더 우수한 성능을 보였다.  

결과의 변동 범위(fluctuation range)는 다음과 같다.  
- **WikiSQL**: 약 ±0.5%  
- **MNLI-m**: 약 ±0.1%  
- **SAMSum**: 세 지표 각각 약 ±0.2 / ±0.2 / ±0.1  

<img src="/assets/img/paper/lora/image_5.png" alt="image" width="720px"> 

---

**그림 2**

**GPT-3 175B** 에서 **WikiSQL** 및 **MNLI-matched** 데이터셋에 대한  
여러 적응(adaptation) 방법들의 **학습 가능한 파라미터 수(trainable parameters)** 와  
**검증 정확도(validation accuracy)** 간의 관계를 나타낸 그래프.  

**LoRA**는 더 나은 **확장성(scalability)** 과  
**과제 성능(task performance)** 을 보인다.  

도표에 사용된 데이터 포인트의 세부 내용은  
**부록 F.2 (Section F.2)** 에 제시되어 있다.

<img src="/assets/img/paper/lora/image_6.png" alt="image" width="800px"> 

---

## **6. 관련 연구 (Related Works)**  

### **트랜스포머 언어모델 (Transformer Language Models)**  

**트랜스포머(Transformer)** (Vaswani et al., 2017)는  
**셀프-어텐션(self-attention)** 을 적극적으로 활용하는  
**시퀀스-투-시퀀스(sequence-to-sequence)** 구조의 아키텍처이다.  

**Radford et al. (a)** 는 트랜스포머 디코더(decoder)들을  
여러 층으로 쌓은 구조를 이용해  
이를 **자기회귀 언어모델(autoregressive language modeling)** 에 적용하였다.  

그 이후로, 트랜스포머 기반 언어모델들은  
**자연어처리(NLP)** 분야를 지배하게 되었으며,  
다양한 과제에서 **최첨단(state-of-the-art)** 성능을 달성하였다.  

**BERT (Devlin et al., 2019b)** 와 **GPT-2 (Radford et al., b)** 를 계기로  
새로운 패러다임이 등장하였다.  
이 두 모델은 모두 **대규모 텍스트 데이터**로 학습된  
**대형 트랜스포머 언어모델(large Transformer language models)** 로,  
**일반 도메인 데이터(general domain data)** 에 대한 사전학습(pre-training) 후  
**과제별 데이터(task-specific data)** 로 **미세조정(fine-tuning)** 을 수행하면,  
단순히 과제 데이터로만 학습하는 것보다  
훨씬 큰 성능 향상이 가능함을 보여주었다.  

또한, **더 큰 규모의 트랜스포머 모델을 학습시키는 것**은  
일반적으로 성능 향상으로 이어지며,  
지속적인 연구 주제로 남아 있다.  

현재까지 학습된 단일 트랜스포머 언어모델 중  
가장 큰 모델은 **GPT-3 (Brown et al., 2020)** 로,  
**1750억 개의 파라미터(175B parameters)** 를 가지고 있다.

### **프롬프트 엔지니어링과 미세조정 (Prompt Engineering and Fine-Tuning)**  

**GPT-3 (175B)** 는 몇 개의 추가 학습 예시(few-shot examples)만으로도  
그 동작(behavior)을 조정할 수 있지만,  
그 결과는 **입력 프롬프트(input prompt)** 에 크게 의존한다 (Brown et al., 2020).  

이로 인해, 원하는 과제에서 모델의 성능을 극대화하기 위해  
프롬프트를 어떻게 구성하고(format) 작성할지를  
**경험적으로 탐색하는 실천적 기술(empirical art)** 이 필요하게 되었으며,  
이를 **프롬프트 엔지니어링(prompt engineering)**  
또는 **프롬프트 해킹(prompt hacking)** 이라고 부른다.  

반면, **미세조정(fine-tuning)** 은  
**일반 도메인 데이터(general domain data)** 로 사전학습된 모델을  
**특정 과제(specific task)** 에 맞추어 다시 학습시키는 방법이다  
(Devlin et al., 2019b; Radford et al., a).  

그 변형(variant) 중 일부는  
모델의 전체 파라미터가 아니라 **일부 파라미터(subset of parameters)** 만 학습하는 방법도 포함한다  
(Devlin et al., 2019b; Collobert & Weston, 2008).  
그러나 실무자(practitioners)들은  
전이 학습(downstream task)의 성능을 최대화하기 위해  
대부분의 경우 **모든 파라미터를 다시 학습(retrain all parameters)** 한다.  

하지만 **GPT-3 (175B)** 는 규모가 매우 크기 때문에,  
기존 방식의 미세조정(fine-tuning)을 수행하기 어렵다.  
그 이유는 다음과 같다:  
- **체크포인트(checkpoint)** 크기가 매우 크고,  
- **사전학습(pre-training)** 과 동일한 수준의 **메모리 사용량(memory footprint)** 을 요구하며,  
- 이로 인해 **하드웨어 진입 장벽(hardware barrier to entry)** 이 높기 때문이다.

### **파라미터 효율적 적응 (Parameter-Efficient Adaptation)**  

여러 연구에서는 **신경망(neural network)** 내의 기존 층(layers)들 사이에  
**어댑터 층(adapter layers)** 을 삽입하는 방법을 제안하였다  
(Houlsby et al., 2019; Rebuffi et al., 2017; Lin et al., 2020).  

우리의 방법(LoRA)은 이와 유사한 **병목 구조(bottleneck structure)** 를 사용하여,  
가중치 업데이트(weight updates)에 **저랭크 제약(low-rank constraint)** 을 부과한다.  

그러나 핵심적인 기능적 차이(key functional difference)는,  
우리의 학습된 가중치(learned weights)가  
**추론(inference)** 중 **기존의 주요 가중치(main weights)** 와 **병합(merge)** 될 수 있어,  
**추가적인 지연(latency)** 을 발생시키지 않는다는 점이다.  
이는 어댑터 층(adapter layers)에서는 불가능하다 (3절 참조).  

어댑터의 **최근 확장형(comtemporary extension)** 인 **COMPACTER (Mahabadi et al., 2021)** 는,  
**Kronecker 곱(Kronecker products)** 과  
일부 **사전 정의된 가중치 공유 체계(predetermined weight sharing scheme)** 를 사용하여  
어댑터 층(adapter layers)을 매개변수화(parametrize)하는 방식이다.  

---

> **(블로그 추가 설명) COMPACTER와 Kronecker 곱을 이용한 어댑터 효율화**  
> - **COMPACTER (Mahabadi et al., 2021)** 는  
>   기존 **어댑터(Adapter)** 구조의 파라미터 효율성을 한층 더 높이기 위해 제안된  
>   **최근 확장형(modern extension)** 방식이다.  
> 
> - 일반적인 어댑터 층은  
>   두 개의 완전연결층(fully connected layers)을 추가하여  
>   비선형 변환을 수행하지만,  
>   각 층마다 **독립적인 가중치 행렬(weight matrix)** 을 학습해야 한다.  
>   이로 인해 **층 수가 많아질수록 중복된 파라미터(redundant parameters)** 가 급격히 증가한다.  
> 
> - COMPACTER는 이러한 비효율성을 줄이기 위해  
>   가중치 행렬을 **Kronecker 곱(Kronecker product)** 형태로 분해하여 표현한다.  
>   예를 들어,  
>
>   $$
>   W_{\text{adapter}} = A \otimes B
>   $$  
>
>   와 같이 나타낼 수 있으며,  
>   여기서 $\otimes$ 는 **크로네커 곱(Kronecker product)** 연산을 의미한다.  
> 
> - **Kronecker 곱**은  
>   두 행렬 $A$와 $B$를 결합해 더 큰 블록 행렬(block matrix)을 구성하는 연산이다.  
>   예를 들어,  
>
>   $$
>   A = 
>   \begin{bmatrix}
>   a_{11} & a_{12} \\
>   a_{21} & a_{22}
>   \end{bmatrix},
>   \quad
>   B =
>   \begin{bmatrix}
>   b_{11} & b_{12} \\
>   b_{21} & b_{22}
>   \end{bmatrix}
>   $$
>
>   일 때,  
>
>   $$
>   A \otimes B =
>   \begin{bmatrix}
>   a_{11}B & a_{12}B \\
>   a_{21}B & a_{22}B
>   \end{bmatrix}
>   $$
>
>   와 같이 표현된다.  
> 
> - 이를 통해 하나의 큰 행렬을 두 개의 작은 행렬로 표현할 수 있으므로,  
>   **필요한 파라미터 수를 크게 줄이면서도**  
>   **원래의 변환 능력(expressive power)** 을 유지할 수 있다.  
> 
> - 또한 COMPACTER는 **가중치 공유(weight sharing)** 를 적용하여  
>   여러 어댑터 층이 일부 파라미터를 공유하도록 설계되었다.  
>   이를 통해 **저장 공간 절약**과 **학습 안정성 향상**을 동시에 달성한다.  
> 
> - 정리하자면, COMPACTER는  
>   **저랭크 근사(low-rank approximation)** 와  
>   **크로네커 곱 기반 파라미터화(Kronecker parameterization)** 를 결합한  
>   **차세대 어댑터 구조(next-generation adapter architecture)** 로,  
>   기존 어댑터 대비 훨씬 적은 파라미터로도  
>   유사한 성능을 달성할 수 있다.  

---

이와 유사하게,  
**LoRA**를 다른 **텐서 곱(tensor product)** 기반 방법들과 결합하면  
파라미터 효율성(parameter efficiency)을 더욱 향상시킬 수 있을 것으로 예상되며,  
이는 향후 연구로 남긴다.  

보다 최근에는,  
**미세조정(fine-tuning)** 대신  
**입력 단어 임베딩(input word embeddings)** 을 직접 최적화하는 방법이 제안되었다.  
이는 본질적으로 **프롬프트 엔지니어링(prompt engineering)** 의  
**연속적이고 미분 가능한 일반화(continuous and differentiable generalization)** 로 볼 수 있다  
(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021).  

---

> **(블로그 추가 설명) “연속적이고 미분 가능한 일반화”의 의미**  
> - 전통적인 **프롬프트 엔지니어링(prompt engineering)** 은  
>   사람이 직접 단어(토큰)를 조합해 입력 문장을 설계하는 **이산적(discrete)** 과정이다.  
>   예를 들어,  
>   “요약해줘(Summarize this article:)” 같은 문구를 바꾸어가며  
>   모델의 출력을 경험적으로 조정한다.  
> 
> - 반면, **연속적이고 미분 가능한 일반화(continuous and differentiable generalization)** 는  
>   이런 텍스트 기반 프롬프트를  
>   **연속적인 벡터 공간(continuous vector space)** 에서 직접 학습하는 접근이다.  
>   즉, 단어 대신 **임베딩 벡터(embedding vectors)** 자체를  
>   **모델의 학습 대상(trainable parameters)** 으로 두고  
>   역전파(backpropagation)를 통해 자동으로 최적화한다.  
> 
> - 이 접근은 사람이 문장을 바꿔가며 시도하는 대신,  
>   **모델이 스스로 최적의 프롬프트 벡터를 찾아가는 과정**이라 할 수 있다.  
>   따라서 “연속적(continuous)”이라는 말은  
>   입력이 이산적인 단어가 아닌 **실수 벡터로 표현된다**는 뜻이고,  
>   “미분 가능(differentiable)”은  
>   이 벡터가 **경사하강법(gradient descent)** 을 통해  
>   학습될 수 있다는 의미이다.  
> 
> - 결과적으로, 이러한 방법은  
>   사람이 직접 프롬프트를 설계하지 않아도 되며,  
>   모델이 학습 과정에서 **프롬프트를 자동으로 최적화(auto-tune)** 하는  
>   보다 일반적이고 수학적인 확장 형태라고 할 수 있다.  

---

우리의 실험 섹션에서는  
**Li & Liang (2021)** 과의 비교 결과를 포함하였다.  

그러나 이러한 연구 방향(line of works)은  
프롬프트 내에 **더 많은 특수 토큰(special tokens)** 을 사용함으로써만 확장될 수 있으며,  
**위치 임베딩(positional embeddings)** 을 학습하는 경우  
이 특수 토큰들이 과제 토큰(task tokens)을 위한  
**사용 가능한 시퀀스 길이(sequence length)** 를 차지하게 된다.
