---
layout: post
title: "[논문] LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS"
date: 2025-10-04 15:00:00 +0900
categories:
  - "논문"
tags: []
---

> **논문 출처**  
> Hu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W.  
> *LoRA: Low-Rank Adaptation of Large Language Models*.  
> arXiv preprint (arXiv:2106.09685), Version 2.  
> <a href="https://arxiv.org/abs/2106.09685" target="_blank">🔗 원문 링크 (arXiv:2106.09685)</a>

**저자**  
- Edward Hu (Microsoft Corporation) – edwardhu@microsoft.com  
- Yelong Shen (Microsoft Corporation) – yeshe@microsoft.com  
- Phillip Wallis (Microsoft Corporation) – phwallis@microsoft.com  
- Zeyuan Allen-Zhu (Microsoft Corporation) – zeyuana@microsoft.com  
- Yuanzhi Li (Carnegie Mellon University) – yuanzhil@andrew.cmu.edu  
- Shean Wang (Microsoft Corporation) – swang@microsoft.com  
- Lu Wang (Microsoft Corporation) – luw@microsoft.com  
- Weizhu Chen (Microsoft Corporation) – wzchen@microsoft.com  

---

>**주석**  
>
>∗ 공동 기여(Equal contribution).

---

(Version 2)

---

>**주석**  
>
>V1과 비교했을 때, 이번 초안에는 더 나은 **베이스라인(baselines)**,  
>**GLUE 실험**, 그리고 **어댑터 지연(adapter latency)** 에 대한  
>추가 내용이 포함되어 있다.

---

## 초록 (Abstract)  

자연어 처리(NLP)의 주요 패러다임 중 하나는  
**일반 도메인 데이터에 대한 대규모 사전학습(pre-training)** 과  
**특정 작업이나 도메인에 대한 적응(adaptation)** 으로 구성된다.  

하지만 모델이 커질수록,  
모든 파라미터를 다시 학습시키는 **전체 미세조정(full fine-tuning)** 은  
점점 더 비현실적이 된다.  

예를 들어, **GPT-3 (175B)** 모델의 경우,  
각각 1,750억 개의 파라미터를 가진 미세조정된 모델 인스턴스를  
독립적으로 배포하는 것은 **막대한 비용**이 든다.  

이를 해결하기 위해 우리는 **LoRA (Low-Rank Adaptation)** 를 제안한다.  

LoRA는 **사전학습된 모델의 가중치를 고정(freeze)** 한 채,  
**트랜스포머(Transformer) 아키텍처의 각 층(layer)** 에  
**학습 가능한 저랭크 분해 행렬(rank decomposition matrices)** 을 삽입함으로써,  
**전이 학습(transfer learning)** 시 필요한 **학습 가능한 파라미터 수를 크게 줄인다.**  

---

> **(블로그 추가 설명) 전이 학습(Transfer Learning)이란?**  
> - **전이 학습(Transfer Learning)** 은  
>   하나의 작업(task)이나 도메인(domain)에서 학습된 지식을  
>   **다른 작업으로 이전(transfer)** 하여 활용하는 학습 방법이다.  
> 
> - 일반적으로 **대규모 데이터로 사전학습(pre-training)** 된 모델은  
>   언어, 문맥, 구조 등에 대한 **일반적인 표현(representation)** 을 이미 학습하고 있다.  
>   이러한 모델을 새로운 응용 문제(예: 감정 분석, 질의응답, 번역 등)에 적용할 때는  
>   모델의 **일부 파라미터만 조정**하거나,  
>   **특정 층(layer)만 미세조정(fine-tuning)** 하는 방식으로 수행된다.  
> 
> - 따라서 처음부터 모델을 다시 학습시키지 않고,  
>   **이미 학습된 지식(knowledge)** 을 바탕으로  
>   **새로운 작업에 빠르게 적응(adapt)** 하는 것이  
>   전이 학습의 핵심 개념이다.  
> 
> - LoRA 역시 이러한 전이 학습의 한 형태로,  
>   전체 모델을 다시 학습하지 않고  
>   **필요한 부분만 효율적으로 조정**하여 성능을 유지한다.

---

GPT-3 (175B)를 Adam 옵티마이저로 미세조정했을 때와 비교하면,  
LoRA는 **학습 가능한 파라미터 수를 최대 10,000배까지 줄이고**,  
**GPU 메모리 요구량을 약 3배 절감**할 수 있다.  

그럼에도 불구하고, LoRA는  
RoBERTa, DeBERTa, GPT-2, GPT-3 등의 모델에서  
**미세조정(full fine-tuning)과 동등하거나 더 우수한 성능**을 보인다.  

또한, 학습 가능한 파라미터가 더 적고,  
**학습 처리량(training throughput)** 이 더 높으며,  
**어댑터(adapters)** 방식과 달리  
**추론(inference) 시 지연(latency)** 도 추가되지 않는다.  

---

> **(블로그 추가 설명) 어댑터(Adapter) 방식이란?**  
> - **어댑터(Adapter)** 는 대형 사전학습 모델(pre-trained model)의  
>   모든 파라미터를 다시 학습시키지 않고,  
>   **각 층(layer)** 사이에 **작은 학습 가능한 모듈(small trainable modules)** 을 추가하여  
>   새로운 작업에 맞게 모델을 미세조정(fine-tuning)하는 방법이다.  
> 
> - 이 방식은 원래의 모델 파라미터를 거의 그대로 유지한 채,  
>   **적은 추가 파라미터(parameter-efficient)** 만 학습할 수 있다는 장점이 있다.  
>   따라서 전체 모델을 다시 학습시키는 것보다 훨씬 효율적이며,  
>   서로 다른 작업 간에 **공유(reuse)** 도 용이하다.  
> 
> - 그러나 어댑터는 **추론(inference)** 단계에서  
>   각 층에 추가된 모듈을 통과해야 하므로,  
>   **추가적인 지연(latency)** 이 발생할 수 있다는 단점이 있다.  
>
> 어댑터 방식은 “모델 전체를 다시 학습하지 않고 필요한 부분만 추가 학습하는”  
> **파라미터 효율적 미세조정(Parameter-Efficient Fine-Tuning, PEFT)** 기법이다.  
> 하지만 **LoRA는 어댑터와 달리 추론 시 지연이 거의 없고**,  
> 동일한 효율성을 유지하면서 더 단순한 구조를 제공한다.

---

아울러 우리는  
**언어 모델 적응에서의 랭크 결핍(rank-deficiency)** 현상에 대한  
실증적 분석을 제공하여,  
**LoRA의 효율성에 대한 통찰(insight)** 을 제시한다.  

---

> **(블로그 추가 설명) 왜 ‘랭크 결핍(Rank-Deficiency)’이라고 부를까?**  
> - “랭크 결핍”은 원래 선형대수학에서 **행렬의 랭크(rank)** 가  
>   최대치보다 낮을 때, 즉 **선형 독립 성분이 부족할 때**를 의미하는 **부정적 용어**이다.  
>   예를 들어, $n \times n$ 행렬의 랭크가 $n$보다 작다면  
>   그 행렬은 **정보 손실(loss of information)** 이 있거나,  
>   **역행렬이 존재하지 않는(singular)** 상태로 본다.  
> 
> - 하지만 언어 모델의 관점에서는 이 “결핍”이 반드시 나쁜 것이 아니다.  
>   오히려 모델이 실제로는 **낮은 차원(low-rank)** 의 표현만으로도  
>   충분히 의미 있는 정보를 학습하고,  
>   복잡한 패턴을 표현할 수 있음을 보여주는 **구조적 효율성(structural efficiency)** 의 신호이기도 하다.  
> 
> - LoRA는 바로 이 특성을 적극적으로 이용한다.  
>   즉, 언어 모델이 본질적으로 **랭크 결핍 구조를 가진다**는 점을 활용하여,  
>   전체 파라미터를 모두 학습하지 않고도  
>   **필요한 저차원 방향만 학습함으로써 동일한 효과를 얻는다.**  
> 
> - 따라서 “랭크 결핍”이라는 단어는 수학적으로는 제약(constraint)을 뜻하지만,  
>   LoRA의 맥락에서는 **“낮은 차원 구조로도 충분히 잘 작동하는 현상”** 이라는  
>   **긍정적인 의미로 재해석**된다.

---

마지막으로, 우리는  
**PyTorch 모델에 LoRA를 통합할 수 있는 패키지**를 공개하였으며,  
RoBERTa, DeBERTa, GPT-2용 **구현 및 모델 체크포인트**를  
다음 링크에서 제공한다.  

<a href="https://github.com/microsoft/LoRA" target="_blank">🔗 https://github.com/microsoft/LoRA</a>


## 1. 서론 (Introduction)  

자연어 처리(NLP)의 많은 응용들은  
**하나의 대규모 사전학습 언어 모델(pre-trained language model)** 을  
여러 개의 **전이(또는 하위) 응용 프로그램(downstream applications)** 에  
적응시키는 것에 의존한다.  

이러한 적응(adaptation)은 일반적으로  
**모든 사전학습된 모델의 파라미터를 업데이트하는 미세조정(fine-tuning)** 을 통해 수행된다.  

그러나 미세조정의 주요 단점은,  
**새로운 모델이 원래 모델과 동일한 수의 파라미터를 갖게 된다는 점**이다.  

모델 규모가 몇 달마다 더 커지는 현 상황에서,  
이 문제는 단순히 GPT-2 (Radford et al.)나 RoBERTa large (Liu et al., 2019) 수준에서는  
“약간의 불편함”에 불과할 수 있었지만,  
**1,750억 개의 학습 가능한 파라미터를 가진 GPT-3 (Brown et al., 2020)**<sup>1</sup> 의 경우에는  
**중대한 배포(deployment)상의 도전 과제**로 변하게 되었다.  

---

>**주석**  
>
><sup>1</sup>GPT-3 (175B)는 **퓨샷 학습(few-shot learning)** 만으로도  
>일정 수준 이상의 성능을 달성하지만,  
>**부록 A(Appendix A)** 에서 보여지듯이  
>**미세조정(fine-tuning)** 을 통해 그 성능이 크게 향상된다.

---

많은 연구자들이 이 문제를 완화하기 위해  
**일부 파라미터만 적응(adapt)** 시키거나,  
**새로운 작업(task)** 을 위해 **외부 모듈(external modules)** 을 학습하는 방법을 시도해왔다.  

이러한 방식에서는 각 작업마다  
사전학습된 모델에 더해 **작업별 파라미터(task-specific parameters)** 만  
저장하고 불러오면 되므로,  
배포(deployment) 시 **운영 효율성(operational efficiency)** 이 크게 향상된다.  

그러나 기존 기법들은 종종  
**모델의 깊이(model depth)** 를 확장함으로써 **추론 지연(inference latency)** 을 유발하거나  
(Loulsby et al., 2019; Rebuffi et al., 2017),  
**모델이 처리할 수 있는 시퀀스 길이(sequence length)** 를 줄이는 문제를 발생시킨다  
(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021) (3장 참조).  

더 중요한 점은, 이러한 방법들이 종종  
**미세조정(fine-tuning) 기반의 베이스라인(baseline)** 수준의 성능을 달성하지 못해,  
**효율성과 모델 품질 간의 트레이드오프(trade-off)** 를 초래한다는 것이다.

우리는 Li et al. (2018a) 및 Aghajanyan et al. (2020)에서 제시된 결과로부터 영감을 얻었다.  
이들 연구는 **과도하게 매개변수화된(over-parameterized)** 모델이 실제로는  
**낮은 고유 차원(low intrinsic dimension)** 상에 존재함을 보여준다.  

이에 기반하여 우리는,  
모델 적응(model adaptation) 과정에서 발생하는 **가중치 변화(weight change)** 역시  
낮은 **내재적 랭크(intrinsic rank)** 를 가진다고 가정(hypothesis)한다.  
이 가정으로부터 우리가 제안한 **LoRA (Low-Rank Adaptation)** 방법이 도출된다.  

LoRA는 사전학습된 가중치를 그대로 **고정(freeze)** 시킨 채,  
**적응 과정에서의 밀집층(dense layer) 변화량**을  
**랭크 분해 행렬(rank decomposition matrices)** 형태로 학습하도록 설계된다.  
즉, 밀집층 자체를 직접 학습하는 대신,  
그 변화(change)를 저랭크 행렬 형태로 간접적으로 최적화한다는 것이다  
(그림 1 참고).  

---

그림 1: 우리의 **재매개변수화(reparametrization)** 방식.  
우리는 **A와 B만 학습한다.**

<img src="/assets/img/paper/lora/image_1.png" alt="image" width="360px"> 

---

> **(블로그 추가 설명) 재매개변수화(Reparametrization)와 랭크 분해 행렬 학습이란?**  
> - **재매개변수화(Reparametrization)** 는  
>   기존의 가중치 행렬 $W$ 를 직접 학습하지 않고,  
>   그것을 **다른 형태의 파라미터(예: $A$, $B$)** 로 다시 표현하여 학습하는 방법이다.  
>   즉, $W$ 대신 **$W + \Delta W$** 형태로 모델을 표현하고,  
>   이때 **$\Delta W = B A$** 로 두어 $A$, $B$ 두 행렬만 학습한다.  
> 
> - 이렇게 하면, 원래의 거대한 가중치 행렬 $W \in \mathbb{R}^{d \times d}$ 을  
>   직접 업데이트할 필요가 없다.  
>   대신, **저랭크(rank $r$)** 제약을 가진 두 행렬 $A \in \mathbb{R}^{r \times d}$,  
>   $B \in \mathbb{R}^{d \times r}$ 을 통해  
>   $W$ 의 변화를 근사(approximation)하게 된다.  
> 
> - 그림에서처럼, $A$ 는 초기값을 **정규분포 $\mathcal{N}(0, \sigma^2)$** 로 설정하고,  
>   $B$ 는 **0으로 초기화**된다.  
>   학습 과정에서는 $A$와 $B$만 최적화되고,  
>   원래의 사전학습 가중치 $W$ 는 **고정(freeze)** 되어 유지된다.  
> 
> - 이러한 방식은 모델의 전체 구조를 바꾸지 않으면서도  
>   **적은 수의 학습 파라미터로 동일한 효과를 얻을 수 있게 해주며**,  
>   이를 통해 **저장 공간과 계산량을 크게 절약**할 수 있다.  
> 
> - 요약하자면, LoRA의 재매개변수화는  
>   거대한 파라미터 행렬을 저차원(rank-$r$) 공간으로 분해해  
>   효율적으로 학습하는 **저랭크 근사(low-rank approximation)** 기반의 접근법이다.

---

GPT-3 (175B)를 예로 들면,  
전체 랭크($d$)가 12,288에 달하더라도  
**매우 낮은 랭크(예: $r=1$ 또는 $r=2$)** 만으로 충분함을 보여준다.  
이는 LoRA가 **저장(storage)** 및 **계산(compute)** 양 측면에서  
매우 효율적인 방법임을 의미한다.

LoRA는 여러 가지 핵심적인 이점을 가진다.  

- **사전학습된 모델(pre-trained model)** 은 여러 다른 작업(tasks)을 위한  
  작은 LoRA 모듈들을 구축하는 데 공유되고 사용될 수 있다.  
  우리는 공유된 모델을 고정(freeze)한 상태로 유지하고,  
  그림 1의 행렬 A와 B를 교체함으로써 작업을 효율적으로 전환할 수 있다.  
  이는 저장 공간 요구량(storage requirement)과  
  작업 전환 시 오버헤드(task-switching overhead)를 크게 줄여준다.  

- LoRA는 학습을 더욱 효율적으로 만들고,  
  적응형 옵티마이저(adaptive optimizer)를 사용할 때  
  하드웨어 진입 장벽(hardware barrier to entry)을 최대 3배까지 낮춘다.  
  이는 대부분의 파라미터에 대해 그래디언트를 계산하거나  
  옵티마이저 상태(optimizer state)를 유지할 필요가 없기 때문이다.  
  대신, 우리는 삽입된 **훨씬 작은 저랭크 행렬(low-rank matrices)** 만 최적화한다.  

- 우리의 단순한 선형(linear) 설계는,  
  배포(deployment) 시 학습 가능한 행렬(trainable matrices)을  
  고정된 가중치(frozen weights)와 병합(merge)할 수 있게 하며,  
  완전히 미세조정된 모델(fully fine-tuned model)과 비교했을 때  
  **추론 지연(inference latency)** 을 전혀 초래하지 않는다.  

- LoRA는 많은 기존 기법들과 **직교적(orthogonal)** 이며,  
  **프리픽스 튜닝(prefix-tuning)** 과 같은  
  여러 방법들과 결합될 수 있다.  
  이에 대한 예시는 **부록 E(Appendix E)** 에서 제시한다.

---

> **(블로그 추가 설명) ‘직교적(Orthogonal)’이라는 표현의 의미**  
> - 여기서 **직교적(orthogonal)** 이라는 표현은  
>   수학적 의미의 “서로 독립적(independent)”이라는 개념에서 유래한다.  
>   즉, **LoRA가 다른 방법들과 간섭하지 않고 동시에 사용할 수 있다** 는 뜻이다.  
> 
> - 어떤 기법이 직교적이라는 것은  
>   그것이 다른 학습 기법의 구조나 원리에 의존하지 않으며,  
>   **별도로 적용하거나 함께 결합(combine)** 해도  
>   서로의 작동 방식에 영향을 주지 않는다는 의미이다.  
> 
> - 따라서 LoRA는 **프리픽스 튜닝(prefix-tuning)**, **프로프트 튜닝(prompt tuning)**,  
>   **어댑터(adapter)** 등과 같은 다른 **파라미터 효율적 학습 기법(PEFT)** 들과  
>   함께 사용될 수 있으며, 이러한 결합은  
>   **추가적인 성능 향상이나 효율성 개선**을 가능하게 한다.

---

### **용어 및 표기 규칙 (Terminologies and Conventions)**  

우리는 **트랜스포머(Transformer) 아키텍처**를 자주 참조하며,  
그 구조에서 일반적으로 사용되는 용어와 표기 규칙(conventions)을 따른다.  

트랜스포머 층의 **입력 및 출력 차원 크기**를 $$d_{\text{model}}$$이라고 부른다.  

$$W_q, \; W_k, \; W_v, \; W_o$$는 각각 **셀프-어텐션(self-attention)** 모듈 내의  
**쿼리(query)**, **키(key)**, **밸류(value)**, **출력(output)** 투영 행렬을 의미한다.  

$$W \; \text{또는} \; W_0$$는 **사전학습된(pre-trained)** 가중치 행렬을,  
$$\Delta W$$는 **적응(adaptation)** 중 누적된 **그래디언트 업데이트(accumulated gradient update)** 를 나타낸다.  

$$r$$은 **LoRA 모듈의 랭크(rank)** 를 의미한다.  

우리는 **Vaswani et al. (2017)** 및 **Brown et al. (2020)** 의 표준 규칙을 따르며,  
모델 최적화(model optimization)에는 **Adam 옵티마이저**  
(**Loshchilov & Hutter, 2019; Kingma & Ba, 2017**)를 사용한다.  

또한, 트랜스포머의 **MLP 피드포워드(feedforward) 차원**은  
$$d_{\text{ffn}} = 4 \times d_{\text{model}}$$로 설정한다.


## 2. 문제 정의 (Problem Statement)  

우리의 제안은 **학습 목표(training objective)** 에 구애받지 않지만,  
본 연구에서는 **언어 모델링(language modeling)** 을 주요 동기 사례로 삼는다.  

아래는 언어 모델링 문제에 대한 간단한 설명이며,  
특히 **작업별 프롬프트(task-specific prompt)** 가 주어졌을 때  
**조건부 확률(conditional probability)** 을 최대화하는 과정을 다룬다.  

사전학습된 **자가회귀 언어 모델(autoregressive language model)**  
$P_\Phi(y|x)$ 이 주어졌다고 가정하자.  
이 모델은 매개변수 **Φ**로 정의된다.  

예를 들어, $P_\Phi(y|x)$ 는 **GPT (Radford et al.; Brown et al., 2020)** 과 같은  
**트랜스포머(Transformer)** 기반의 **일반적 다중작업 학습기(generic multi-task learner)** 일 수 있다  
(Vaswani et al., 2017).  

이제 이 사전학습된 모델을  
**요약(summarization)**, **기계 독해(MRC, Machine Reading Comprehension)**,  
**자연어 → SQL 변환(NL2SQL)** 과 같은  
**조건부 텍스트 생성(conditional text generation)** 작업에 적응(adapt)시키는 것을 고려하자.  

각 전이 학습(downstream) 작업은  
**컨텍스트-타깃(context–target)** 쌍으로 이루어진 학습 데이터셋으로 표현된다.  

$$
Z = \{(x_i, y_i)\}_{i=1}^N
$$  

여기서 $x_i$와 $y_i$는 모두 **토큰(token)** 시퀀스이다.  

예를 들어, NL2SQL의 경우 $x_i$는 자연어 질의(natural language query)이고,  
$y_i$는 그에 대응하는 **SQL 명령문(SQL command)** 이다.  

요약(summarization)의 경우 $x_i$는 기사(article)의 본문(content)이고,  
$y_i$는 그 요약(summary)이다.

전체 미세조정(full fine-tuning)에서는  
모델이 **사전학습된 가중치(pre-trained weights)** $\Phi_0$ 로 초기화되고,  
다음의 **조건부 언어 모델링 목표(conditional language modeling objective)** 를  
최대화하기 위해 그래디언트를 반복적으로 따라가며  
$\Phi_0 + \Delta \Phi$ 로 업데이트된다.  

$$
\max_{\Phi} 
\sum_{(x,y) \in Z} 
\sum_{t=1}^{|y|} 
\log \big(P_{\Phi}(y_t \mid x, y_{<t})\big)
\tag{1}
$$  

---

> **(블로그 추가 설명) 전체 미세조정(Full Fine-Tuning)의 의미**  
> - **전체 미세조정(full fine-tuning)** 은 사전학습(pre-training)이 끝난 모델의  
>   **모든 파라미터(parameter)** 를 다시 학습 가능한 상태로 두고,  
>   새로운 작업(task)에 맞게 전체를 재학습시키는 방식이다.  
> 
> - 위 식에서 $\Phi_0$ 는 사전학습(pre-trained)으로 얻은 원래의 가중치(weight)이며,  
>   $\Delta \Phi$ 는 새로운 작업에 맞게 학습을 통해 추가로 얻어지는  
>   **가중치의 변화량(weight update)** 을 의미한다.  
>   즉, 최종 가중치는 $\Phi = \Phi_0 + \Delta \Phi$ 형태로 표현된다.  
> 
> - 모델은 주어진 데이터셋 $Z = \{(x_i, y_i)\}$ 에 대해,  
>   각 토큰 시점 $t$ 에서 **이전 토큰들 $y_{<t}$ 과 입력 $x$ 가 주어졌을 때,  
>   다음 토큰 $y_t$ 가 등장할 확률** $P_{\Phi}(y_t \mid x, y_{<t})$ 을  
>   최대화하는 방향으로 학습된다.  
> 
> - 식 (1)은 이러한 과정을 **확률적 언어 모델링(objective)** 의 형태로 나타낸 것이다.  
>   즉, 모든 학습 데이터 $(x, y)$ 와 각 시점 $t$ 에 대한  
>   로그 확률의 합(log-likelihood sum)을 최대화함으로써,  
>   모델이 주어진 문맥(context)에서 다음 단어를 예측하도록 학습한다.  
> 
> - 이 접근 방식은 성능은 높지만,  
>   파라미터 수가 매우 큰 모델(예: GPT-3)에서는  
>   모든 가중치를 업데이트해야 하므로 **비용(cost)** 과 **메모리(memory)** 측면에서  
>   매우 비효율적이라는 한계를 가진다.

---

전체 미세조정의 주요 단점 중 하나는  
각 전이 학습(downstream) 작업마다  
서로 다른 파라미터 집합 $\Delta \Phi$ 를 학습해야 한다는 점이다.  

이때 그 차원(dimensions) $\mid\Delta\Phi\mid$ 는  
원래 모델의 파라미터 수 $\mid \Phi_0 \mid$ 와 동일하다.  

따라서 **사전학습 모델이 매우 클 경우**  
(예: $|\Phi_0| \approx 175$B인 **GPT-3**)  
많은 개수의 미세조정된 모델 인스턴스를  
저장하거나 배포하는 것은 매우 어렵거나 거의 불가능하다.  

이 논문에서는 보다 **파라미터 효율적(parameter-efficient)** 인 접근을 채택한다.  

즉, 작업별 파라미터 변화량(increment) $\Delta \Phi = \Delta \Phi(\Theta)$ 를  
훨씬 더 작은 크기의 파라미터 집합 $\Theta$ 로 표현한다.  

여기서 $\mid \Theta \mid \ll \mid \Phi_0 \mid$ 이다.  

따라서 $\Delta \Phi$ 를 찾는 문제는  
이제 $\Theta$ 위에서 최적화하는 문제로 바뀐다.  

$$
\max_{\Theta} 
\sum_{(x,y) \in Z} 
\sum_{t=1}^{|y|} 
\log \big(P_{\Phi_0 + \Delta \Phi(\Theta)}(y_t \mid x, y_{<t})\big)
\tag{2}
$$  

---

> **(블로그 추가 설명) $\Theta$의 의미와 식 (2)의 해석**  
> - 식 (2)에서 등장하는 $\Theta$는  
>   **LoRA가 새로 도입한 학습 가능한 파라미터들의 집합(set of trainable parameters)** 이다.  
>   이 파라미터들은 기존의 모델 가중치 $\Phi_0$와는 별도로,  
>   **가중치 변화량(weight update)** $\Delta \Phi$를 저차원 공간에서 표현하기 위해 사용된다.  
> 
> - 구체적으로, LoRA는 기존의 거대한 가중치 행렬 $W \in \mathbb{R}^{d \times d}$ 를  
>   그대로 학습하지 않고,  
>   그 변화량을 두 개의 **저랭크 행렬(low-rank matrices)** $A \in \mathbb{R}^{r \times d}$,  
>   $B \in \mathbb{R}^{d \times r}$ 의 곱으로 근사한다.  
>   $$
>   \Delta W = B A
>   $$  
>   이때 $\Theta$는 바로 이 행렬들 $A$와 $B$ — 즉,  
>   **LoRA 모듈 내에서 새롭게 학습되는 모든 파라미터들의 집합**을 의미한다.  
> 
> - 따라서 $\Delta \Phi(\Theta)$는 “$\Theta$로부터 계산된 전체 가중치 변화량”이다.  
>   다시 말해, $\Theta$를 학습하면 그에 따라 $\Delta \Phi$가 결정된다.  
>   학습의 초점은 이제 $\Phi$ 전체가 아니라,  
>   이 작은 저차원 파라미터 공간 $\Theta$ 위로 옮겨지는 것이다.  
> 
> - 식 (2)는 이러한 과정을 수식으로 표현한 것으로,  
>   모델은 여전히 조건부 확률  
>   $P(y_t \mid x, y_{<t})$을 최대화하지만,  
>   이때의 파라미터는 **사전학습된 가중치 $\Phi_0$** 와  
>   **$\Theta$로부터 유도된 변화량 $\Delta \Phi(\Theta)$** 를 합친 형태로 쓰인다.  
> 
> - 요약하면,  
>   **$\Theta$ = LoRA에서 새로 추가된 학습 가능한 저랭크 행렬 파라미터의 집합**이며,  
>   이 $\Theta$를 학습함으로써  
>   거대한 모델 전체의 가중치를 직접 업데이트하지 않고도  
>   새로운 작업에 적응할 수 있게 되는 것이다.

---

이후의 섹션에서는  
$\Delta \Phi$ 를 계산 및 메모리 효율적으로 표현하기 위한  
**저랭크 표현(low-rank representation)** 방식을 제안한다.  

특히, 사전학습 모델이 **GPT-3 (175B)** 인 경우,  
학습 가능한 파라미터 수 $|\Theta|$ 는  
$|\Phi_0|$ 의 **0.01% 수준까지 줄일 수 있다.**

---

## 3. 기존 방법들은 충분하지 않은가? (Aren’t Existing Solutions Good Enough?)  

우리가 해결하려는 문제는 결코 새로운 것이 아니다.  
**전이 학습(transfer learning)** 이 등장한 이후,  
수많은 연구들이 모델 적응(model adaptation)을  
더 **파라미터 효율적(parameter-efficient)** 이고  
**계산 효율적(compute-efficient)** 으로 만드는 방법을 제시해 왔다.  
이와 관련된 잘 알려진 연구들에 대해서는 **6장(Section 6)** 에서 요약한다.  

**언어 모델링(language modeling)** 을 예로 들면,  
효율적인 적응을 위한 대표적인 두 가지 전략이 존재한다.  

첫 번째는 **어댑터 층(adapter layers)** 을 추가하는 방식이고,  
(Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2021; Rücklé et al., 2020).  

두 번째는 **입력층 활성화(input layer activations)** 의  
일부 형태를 최적화(optimization)하는 접근이다.  
(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021).  

그러나 두 전략 모두 한계를 가진다.  
특히 **대규모 모델 환경(large-scale setting)** 이나  
**지연(latency)에 민감한 실제 서비스(production scenario)** 에서는  
이러한 기존 방법들이 충분히 효과적이지 않다.

### **어댑터 층(Adapter Layers)은 추론 지연(Inference Latency)을 초래한다**  

어댑터(adapter)에는 여러 가지 변형(variants)이 존재한다.  
우리는 그중 **두 가지 대표적인 설계(design)** 에 초점을 맞춘다.  

첫째, **Houlsby et al. (2019)** 이 제안한 방식으로,  
각 **트랜스포머 블록(Transformer block)** 에 **두 개의 어댑터 층(adapter layers)** 을 포함한다.  

둘째, **Lin et al. (2020)** 의 보다 최근 설계로,  
각 블록에 **하나의 어댑터 층만 포함하지만**,  
**추가적인 LayerNorm (Ba et al., 2016)** 을 사용한다.

전체 지연(latency)을 줄이기 위해  
일부 층을 **가지치기(pruning)** 하거나  
**멀티태스크 환경(multi-task setting)** 을 활용할 수도 있지만  
(Rücklé et al., 2020; Pfeiffer et al., 2021),  
어댑터 층에서 발생하는 **추가 연산량(extra compute)** 자체를  
근본적으로 제거할 방법은 없다.  

처음에는 이것이 큰 문제가 아닌 것처럼 보일 수 있다.  
왜냐하면 어댑터 층은 일반적으로  
**좁은 병목 차원(bottleneck dimension)** 을 사용하여  
**원래 모델의 1% 미만의 파라미터**만 추가하기 때문이다.  
이로 인해 추가되는 연산량(FLOPs)은 제한적이다.  

그러나 **대규모 신경망(large neural networks)** 은  
**하드웨어 병렬 처리(hardware parallelism)** 를 통해  
낮은 지연(latency)을 유지한다.  
문제는 어댑터 층이 **순차적으로(sequentially)** 처리되어야 한다는 점이다.  

따라서 **온라인 추론(online inference)** 환경처럼  
**배치 크기(batch size)** 가 1에 가까운 상황에서는  
이 순차적 처리로 인해 지연이 뚜렷하게 나타난다.  

예를 들어, 모델 병렬화(model parallelism)가 적용되지 않은 일반적인 환경에서,  
단일 GPU 위에서 **GPT-2 (Radford et al.) medium 모델**을  
어댑터와 함께 실행하면,  
병목 차원이 매우 작더라도(표 1 참조)  
**추론 지연(latency)이 눈에 띄게 증가**함을 확인할 수 있다.  

---

**표 1**

GPT-2 medium에서 **단일 순전파(single forward pass)** 시의  
**추론 지연 시간(inference latency)** 을 밀리초 단위로 측정한 결과이다.  
100회의 실험을 평균하여 산출했으며,  
**NVIDIA Quadro RTX8000** GPU를 사용하였다.  

기호 “$|\Theta|$”는 **어댑터 층(adapter layers)** 에서의  
**학습 가능한 파라미터 수(trainable parameters)** 를 나타낸다.  

**Adapter**<sup>L</sup>과 **Adapter**<sup>H</sup>는  
두 가지 **어댑터 튜닝(adapter tuning)** 변형이며,  
이에 대한 설명은 **5.1절(Section 5.1)** 에 제시되어 있다.  

어댑터 층으로 인해 발생하는 추론 지연은  
**온라인 환경(online setting)** 이나  
**짧은 시퀀스 길이(short-sequence-length)** 의 경우 특히 두드러질 수 있다.  
자세한 실험 결과는 **부록 B(Appendix B)** 를 참조하라.

<img src="/assets/img/paper/lora/image_2.png" alt="image" width="720px"> 

---

이 문제는 Shoeybi et al. (2020), Lepikhin et al. (2020)에서처럼  
모델을 여러 GPU에 **샤딩(sharding)** 해야 할 때 더 심각해진다.  

추가된 네트워크 깊이로 인해  
**AllReduce** 및 **Broadcast**와 같은  
**동기식 GPU 연산(synchronous GPU operations)** 이 더 많이 필요하기 때문이다.  

이는 우리가 어댑터 파라미터를 여러 번 **중복 저장(redundantly store)** 하지 않는 한  
피할 수 없는 문제이다.

---

> **(블로그 추가 설명) AllReduce와 Broadcast 연산의 의미**  
> - **AllReduce** 와 **Broadcast** 는  
>   다중 GPU 환경에서 모델 학습이나 추론 시 **데이터를 동기화(synchronize)** 하기 위해 사용되는  
>   대표적인 **통신 연산(communication operations)** 이다.  
> 
> - **AllReduce 연산**은  
>   여러 GPU가 각각 계산한 값을 서로 교환(exchange)하고,  
>   이를 더하거나(예: gradient 합산), 평균내는 등의 연산을 통해  
>   모든 GPU가 동일한 결과를 갖도록 만드는 과정이다.  
>   예를 들어, 모델 병렬 학습 시 각 GPU가 계산한 그래디언트를  
>   전체적으로 합산하여 모든 GPU에 동일하게 반영하는 데 사용된다.  
>   $$
>   \text{AllReduce: } x_i \rightarrow \sum_i x_i \quad \text{(모든 GPU 간 합산 후 동기화)}
>   $$  
> 
> - **Broadcast 연산**은  
>   한 GPU(일반적으로 “마스터 GPU”)에서 계산된 값을  
>   다른 모든 GPU로 복사(copy)하여 전달하는 과정이다.  
>   이는 모델 파라미터나 중간 결과를 여러 장치에 동일하게 배포(distribute)할 때 사용된다.  
>   $$
>   \text{Broadcast: } x_1 \rightarrow (x_1, x_1, x_1, \dots)
>   $$  
> 
> - 이러한 연산들은 모두 **동기식(synchronous)** 으로 수행되기 때문에,  
>   하나의 GPU라도 작업이 지연되면 전체 연산이 멈추게 된다.  
>   따라서 네트워크 깊이가 깊어질수록,  
>   또는 어댑터 층이 추가될수록  
>   **GPU 간 통신 부담(communication overhead)** 이 커지고  
>   결과적으로 **추론 지연(latency)** 도 증가하게 된다.

---

### **프롬프트 직접 최적화의 어려움 (Directly Optimizing the Prompt is Hard)**  

다른 방향의 접근법인 **프리픽스 튜닝(prefix tuning)** (Li & Liang, 2021)은  
다른 형태의 어려움을 겪는다.  

우리는 프리픽스 튜닝이 **최적화(optimization)** 하기 어렵고,  
**학습 가능한 파라미터 수(trainable parameters)** 에 따라  
그 성능이 **단조적으로(non-monotonically)** 변하지 않는다는 것을 관찰했다.  
이는 원 논문(original paper)에서도 보고된 바와 동일한 현상이다.  

보다 근본적으로,  
**시퀀스 길이(sequence length)** 의 일부를  
적응(adaptation)을 위해 예약(reserve)해야 한다는 점이 문제이다.  

이로 인해 전이 학습(downstream task)을 처리할 수 있는  
**실제 시퀀스 길이(available sequence length)** 가 줄어들게 된다.  

우리는 이러한 제약이  
프롬프트 튜닝(prompt tuning)의 성능을  
다른 방법들에 비해 떨어뜨리는 요인 중 하나라고 추정한다.  

작업별 성능(task performance)에 대한 자세한 연구는  
**5절(Section 5)** 에서 다룬다.

---

> **(블로그 추가 설명) 시퀀스 길이 예약이 의미하는 것**  
> - 프리픽스 튜닝(prefix tuning)에서는  
>   모델이 새로운 작업(task)에 적응할 수 있도록  
>   입력 시퀀스의 맨 앞부분에 **학습 가능한 토큰들(trainable tokens)**,  
>   즉 **프리픽스(prefix)** 를 추가한다.  
> 
> - 이 프리픽스는 모델의 파라미터를 직접 수정하지 않고도  
>   입력에 “작업별 힌트(task-specific context)”를 주는 역할을 한다.  
>   하지만 이 추가된 프리픽스 토큰들이  
>   전체 시퀀스 길이 안에 포함되기 때문에,  
>   모델이 실제 입력 데이터(예: 문장, 문단 등)를 처리할 수 있는  
>   **토큰 수(token capacity)** 가 줄어들게 된다.  
> 
> - 예를 들어, 모델의 최대 시퀀스 길이가 512라면  
>   프리픽스로 20개의 토큰을 사용했을 때  
>   실제 작업 입력에는 492개의 토큰만 사용할 수 있다.  
>   즉, **적응을 위한 공간이 늘어날수록  
>   입력 데이터에 할당할 수 있는 공간이 줄어드는 구조적 제약**이 생긴다.  
> 
> - 이러한 이유로 프리픽스 튜닝은  
>   특히 긴 문맥(long-context)이나 복잡한 입력을 처리해야 하는 작업에서  
>   성능이 저하될 가능성이 있다.

---

## 4. 우리의 방법 (Our Method)  

이 절에서는 **LoRA의 간단한 설계(simple design)** 와  
그 **실용적 장점(practical benefits)** 에 대해 설명한다.  

여기서 제시하는 원칙(principles)은  
딥러닝 모델의 모든 **밀집층(dense layers)** 에 적용 가능하지만,  
본 연구에서는 **트랜스포머(Transformer) 기반 언어 모델**의  
특정 가중치(weights)에만 초점을 맞춘다.  
이는 LoRA의 주요 동기 사례(motivating use case)이기 때문이다.

### **4.1 저랭크 기반의 파라미터화된 갱신 행렬 (Low-Rank-Parametrized Update Matrices)**  

신경망(neural network)은 여러 개의 **밀집층(dense layer)** 으로 구성되어 있으며,  
각 층은 **행렬 곱(matrix multiplication)** 연산을 수행한다.  
이 층들의 가중치 행렬(weight matrix)은 일반적으로 **풀 랭크(full-rank)** 구조를 가진다.  

---

> **(블로그 추가 설명) 풀 랭크(Full-Rank)란 무엇인가?**  
> - **풀 랭크(full-rank)** 행렬이란,  
>   행렬이 가질 수 있는 **최대 차수(rank)** 를 모두 가진 경우를 의미한다.  
>   즉, $d \times k$ 크기의 행렬이라면  
>   그 랭크(rank)가 $\min(d, k)$ 인 상태를 말한다.  
> 
> - 행렬의 **랭크(rank)** 는  
>   행 또는 열이 서로 **선형적으로 독립(linearly independent)** 한 정도를 나타내며,  
>   이는 곧 행렬이 표현할 수 있는 **정보의 다양성(information diversity)** 을 의미한다.  
> 
> - 예를 들어,  
>   어떤 가중치 행렬이 풀 랭크라면  
>   모든 출력 방향(output direction)을  
>   입력 공간(input space)의 선형 조합(linear combination)으로 표현할 수 있다.  
>   즉, **정보 손실 없이 입력을 변환할 수 있는 완전한 표현 능력(full representational capacity)** 을 가진다.  
> 
> - 반대로, 랭크가 낮은 행렬(저랭크 행렬, low-rank matrix)은  
>   일부 방향으로의 변환 능력이 제한되어 있으며,  
>   이는 계산량을 줄이는 대신 **표현력(expressiveness)** 을 희생한다.  
> 
> - LoRA는 이러한 특성을 이용하여,  
>   기존의 풀 랭크 행렬을 직접 학습하는 대신  
>   **저랭크 근사(low-rank approximation)** 를 통해  
>   훨씬 적은 파라미터로 유사한 변환을 수행하도록 설계된 것이다.

---

특정 작업(task)에 적응(adaptation)할 때,  
**Aghajanyan et al. (2020)** 은  
사전학습된 언어 모델(pre-trained language model)이  
**낮은 “내재 차원(intrinsic dimension)”** 을 가지고 있으며,  
무작위 투영(random projection)을 통해  
더 작은 부분공간(subspace)으로 축소되더라도  
효율적으로 학습할 수 있음을 보였다.  

이 결과에 영감을 받아,  
우리는 **적응 과정에서의 가중치 변화(weight update)** 역시  
낮은 **내재 랭크(intrinsic rank)** 를 가진다고 가정(hypothesize)한다.  

사전학습된 가중치 행렬이 $W_0 \in \mathbb{R}^{d \times k}$ 일 때,  
그 갱신(update)을 **저랭크 분해(low-rank decomposition)** 형태로 제한한다.  

$$
W_0 + \Delta W = W_0 + BA
$$  

여기서 $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$이며,  
랭크 $r \ll \min(d, k)$ 이다.  

---

> **(블로그 추가 설명) $d$와 $k$의 의미**  
> - 여기서 $d$와 $k$는 각각 **행렬의 차원(dimension)** 을 나타낸다.  
>   구체적으로, $W_0 \in \mathbb{R}^{d \times k}$ 는  
>   **입력 벡터(input vector)** 를 **출력 벡터(output vector)** 로 변환하는  
>   **가중치 행렬(weight matrix)** 이다.  
> 
> - $k$는 **입력 차원(input dimension)**,  
>   즉 한 데이터 샘플(토큰 등)을 표현하는 벡터의 크기이다.  
>   반면 $d$는 **출력 차원(output dimension)** 으로,  
>   변환 후 벡터가 표현되는 공간의 크기를 의미한다.  
> 
> - 예를 들어,  
>   트랜스포머의 셀프-어텐션(self-attention) 구조에서  
>   쿼리(query)나 밸류(value) 투영 행렬이  
>   $W_q, W_v \in \mathbb{R}^{d \times k}$ 라면,  
>   입력 임베딩의 차원이 $k$이고,  
>   어텐션 공간의 출력 차원이 $d$가 된다.  
> 
> - LoRA에서는 이 가중치 행렬의 업데이트를  
>   $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$ 로 분해하여  
>   **저랭크 근사(low-rank approximation)** 를 수행한다.  
>   여기서 $r$은 $d$와 $k$보다 훨씬 작은 값으로 설정되어,  
>   계산량과 파라미터 수를 크게 줄이면서도  
>   원래의 표현력을 최대한 보존할 수 있도록 한다.

---

학습 과정 동안 $W_0$는 **고정(frozen)** 되어  
그래디언트 업데이트를 받지 않으며,  
$A$와 $B$만이 **학습 가능한 파라미터(trainable parameters)** 를 포함한다.  

$W_0$와 $\Delta W = BA$ 는  
모두 동일한 입력(input)을 곱하고,  
그 결과(output) 벡터는 **좌표 단위로 합산(coordinate-wise summation)** 된다.  

$h = W_0 x$ 라고 할 때, 수정된 순전파(modified forward pass)는 다음과 같다:

$$
h = W_0 x + \Delta W x = W_0 x + BAx
\tag{3}
$$

이 재매개변수화(reparametrization)는 **그림 1(Figure 1)** 에 시각화되어 있다.  

초기화 시, $A$는 **정규분포(random Gaussian)** 로 $B$는 **0(zero)** 으로 설정하여,  
학습 초기에 $\Delta W = BA = 0$ 이 되도록 한다.  

그 후, $\Delta W x$를 $\dfrac{\alpha}{r}$로 스케일링(scale)한다.  
여기서 $\alpha$는 랭크 $r$에 따라 설정되는 상수(constant)이다.  

**Adam 옵티마이저**를 사용할 때, $\alpha$ 값을 조정하는 것은  
초기화 스케일을 적절히 설정했을 경우 **학습률(learning rate)** 을 조정하는 것과 유사하다.  
따라서 우리는 시도하는 첫 번째 랭크 $r$에 대해 $\alpha$를 설정하고 이후 추가 조정은 하지 않는다.  

이러한 스케일링은 $r$ 값을 변화시킬 때마다 하이퍼파라미터를 다시 조정해야 하는 필요성을 줄여준다  
(Yang & Hu, 2021).

---

> **(블로그 추가 설명) $\alpha$를 고정해도 되는 이유**  
> - LoRA에서는 학습 중 가중치 갱신항 $\Delta W x = BAx$에  
>   $\dfrac{\alpha}{r}$이라는 스케일링 계수를 곱해 사용한다.  
>   따라서 전체 식은 다음과 같이 표현된다.  
>
>   $$
>   h = W_0x + \frac{\alpha}{r}BAx
>   $$  
> 
> - 이렇게 하면 **랭크 $r$이 달라져도**  
>   업데이트의 크기(scale)가 자동으로 보정된다.  
>   예를 들어 $r$이 커지면 자유도가 늘어나고 $\Delta W$의 크기가 커지지만,  
>   동시에 $\dfrac{1}{r}$로 나누어 주기 때문에 변화폭이 일정하게 유지된다.  
> 
> - 논문에서 “$\alpha$를 처음 시도하는 $r$ 값으로 고정한다”고 말하는 이유는  
>   바로 이 자동 보정 덕분이다.  
>   $\alpha$를 한 번 정하면 이후 다른 $r$ 값을 실험하더라도  
>   $\dfrac{\alpha}{r}$ 항이 알아서 균형을 맞춰주므로  
>   **$\alpha$나 학습률을 다시 튜닝할 필요가 없다.**  
> 
> - 즉, $\alpha$는 고정된 상수이지만  
>   실제 적용은 $\dfrac{\alpha}{r}$ 형태로 이루어지기 때문에  
>   **랭크가 달라져도 학습 안정성(stability)** 을 유지할 수 있는 것이다.

---

#### **전체 미세조정의 일반화 (A Generalization of Full Fine-Tuning)**  

보다 일반적인 형태의 **미세조정(fine-tuning)** 은  
사전학습된 파라미터 중 일부만을 학습 대상으로 삼는 방식을 허용한다.  

**LoRA**는 이보다 한 단계 더 나아가,  
적응(adaptation) 과정에서 가중치 행렬(weight matrix)에 적용되는  
**누적 그래디언트 업데이트(accumulated gradient update)** 가  
**풀 랭크(full-rank)** 일 필요가 없다고 가정한다.  

이는 곧 LoRA를 모든 가중치 행렬에 적용하고  
모든 **바이어스(bias)**<sup>2</sup> 를 함께 학습시킬 때,  
**사전학습된 가중치 행렬의 랭크(rank)** 와  
**LoRA의 랭크 $r$** 를 동일하게 설정하면  
**전체 미세조정(full fine-tuning)** 과 거의 동일한 수준의  
**표현력(expressiveness)** 을 복원할 수 있음을 의미한다.  

---

>**주석**  
>
><sup>2</sup> 바이어스는 가중치에 비해 차지하는 파라미터 수가 매우 적다.  

---

즉, 학습 가능한 파라미터의 수를 점진적으로 늘려감에 따라,  
**LoRA의 학습은 원래 모델의 학습과 점점 수렴(converge)** 하게 된다.  

반면,  
**어댑터(adapter) 기반 방법**은  
이론적으로 **MLP(다층 퍼셉트론)** 에 수렴하며,  
**프리픽스(prefix)** 기반 방법은  
**긴 입력 시퀀스(long input sequences)** 를 처리하지 못하는 모델로 수렴한다.<sup>3</sup>

---

>**주석**  
>
><sup>3</sup> 이는 복잡한(hard) 작업에 적응할 때 불가피한 현상이다.

---

#### **추가적인 추론 지연이 없음 (No Additional Inference Latency)**  

프로덕션 환경(production environment)에서 모델을 배포할 때,  
다음과 같이 명시적으로 계산하여 저장할 수 있다.  

$$
W = W_0 + BA
$$  

그 후 **일반적인 방식으로(in the usual way)** 추론(inference)을 수행하면 된다.  
여기서 $W_0$와 $BA$는 모두 $\mathbb{R}^{d \times k}$ 공간에 속한다.  

다른 전이 학습(downstream) 작업으로 전환해야 하는 경우에는,  
$BA$를 빼서 원래의 $W_0$를 복원한 뒤,  
새로운 작업에 해당하는 $B'A'$를 더하면 된다.  
이 연산은 매우 빠르며 **메모리 오버헤드(memory overhead)** 도 거의 없다.  

가장 중요한 것은, 이러한 설계에 의해  
**LoRA는 추론 중(inference) 완전 미세조정(fine-tuned) 모델과 비교했을 때  
어떠한 추가적인 지연(latency)도 유발하지 않는다**는 점이다.


### **4.2 트랜스포머에 LoRA 적용하기 (Applying LoRA to Transformer)**  

이론적으로, **LoRA**는  
신경망(neural network) 내의 **임의의 가중치 행렬(weight matrix) 부분집합**에  
적용하여 학습 가능한 파라미터 수를 줄일 수 있다.  

**트랜스포머(Transformer)** 아키텍처에서는  
**셀프-어텐션(self-attention)** 모듈에  
4개의 가중치 행렬 $W_q, W_k, W_v, W_o$ 가 있고,  
**MLP 모듈**에는 2개의 가중치 행렬이 존재한다.  

우리는 $W_q$ (또는 $W_k$, $W_v$)를  
출력 차원이 일반적으로 **어텐션 헤드(attention heads)** 로 분할되어 있더라도,  
**단일 행렬(single matrix)** $d_{\text{model}} \times d_{\text{model}}$ 크기의 행렬로 간주한다.  

본 연구에서는 단순성과 파라미터 효율성(parameter-efficiency)을 위해  
**전이 학습(downstream task)** 시  
**어텐션 가중치(attention weights)** 만 적응(adapt)시키고,  
**MLP 모듈은 고정(freeze)** 시켜 학습하지 않는다.  

또한, **7.1절(Section 7.1)** 에서  
트랜스포머 내 서로 다른 유형의 어텐션 가중치 행렬에  
LoRA를 적용할 때의 효과를 추가로 분석한다.  

한편, **MLP 층, LayerNorm 층, 바이어스(biases)** 에  
LoRA를 적용하는 것에 대한 실증적 연구(empirical investigation)는  
**향후 연구(future work)** 로 남겨둔다.

#### **실용적 이점과 한계 (Practical Benefits and Limitations)**  

가장 큰 이점은 **메모리(memory)** 와 **저장 공간(storage)** 사용량의 감소이다.  

대규모 트랜스포머(Transformer)를 **Adam 옵티마이저**로 학습할 때,  
$r \ll d_{\text{model}}$ 인 경우  
고정된 파라미터(frozen parameters)에 대한 옵티마이저 상태(optimizer states)를  
저장할 필요가 없으므로 **VRAM 사용량을 최대 2/3까지 줄일 수 있다.**  

예를 들어, **GPT-3 (175B)** 모델의 경우  
학습 중 VRAM 소비량이 **1.2TB에서 350GB로 감소**한다.  

$r = 4$이고 **쿼리(query)** 및 **밸류(value)** 투영 행렬만을 적응시킬 때,  
체크포인트(checkpoint) 크기는 약 **10,000배 감소**한다  
(350GB → 35MB)<sup>4</sup>.  

---

>**주석**  
>
><sup>4</sup> 배포(deployment) 시에는 여전히 **350GB 크기의 기본 모델**이 필요하다.  
>그러나 100개의 적응된 모델(adapted models)을 저장하더라도  
>필요한 용량은 **350GB + (35MB × 100) ≈ 354GB**에 불과하다.  
>이는 각각의 모델을 별도로 저장했을 때 필요한 **100 × 350GB ≈ 35TB**에 비해  
>극적으로 작다.  

---

이로 인해 훨씬 적은 수의 GPU로 학습이 가능해지고,  
**입출력 병목(I/O bottleneck)** 문제를 피할 수 있다.  

또한, LoRA 가중치만 교체하면  
모든 파라미터를 교체할 필요 없이  
**배포 중에도 낮은 비용으로 작업 간 전환(task switching)** 이 가능하다.  

이 덕분에 VRAM에 사전학습된 가중치를 저장한 채로  
다양한 **맞춤형(customized) 모델**을  
**실시간(on the fly)** 으로 교체하여 사용할 수 있다.  

마지막으로, **GPT-3 (175B)** 에서 전체 미세조정(full fine-tuning)과 비교했을 때  
**약 25%의 학습 속도 향상(speedup)** 이 관찰되었다.<sup>5</sup>  

이는 전체 파라미터 중 대다수에 대해  
그래디언트를 계산할 필요가 없기 때문이다.

---

>**주석**  
>
><sup>5</sup> **GPT-3 (175B)** 의 경우,  
>**전체 미세조정(full fine-tuning)** 의 학습 처리 속도(training throughput)는  
>**V100 GPU당 32.5 tokens/s** 이다.  
>동일한 모델 병렬화(model parallelism) 분할 수(weight shards)를 사용했을 때,  
>**LoRA** 의 처리 속도는 **V100 GPU당 43.1 tokens/s** 로 향상된다.

---

LoRA에도 몇 가지 한계점(limitations)이 존재한다.  

예를 들어, 추가적인 추론 지연(inference latency)을 제거하기 위해  
$A$와 $B$를 $W$에 흡수(absorb)하기로 선택한 경우,  
서로 다른 작업(tasks)에 대해 서로 다른 $A$와 $B$를 가진 입력들을  
**하나의 순전파(single forward pass)** 에서 배치(batch) 처리하는 것은 간단하지 않다.  

그러나 지연(latency)이 중요하지 않은 시나리오에서는,  
가중치(weights)를 병합하지 않고  
배치 내의 각 샘플(sample)에 사용할 **LoRA 모듈**을  
**동적으로 선택(dynamically choose)** 하는 것이 가능하다.

---

> **(블로그 추가 설명) 왜 서로 다른 작업을 한 번에 배치 처리하기 어려운가?**  
> - LoRA에서는 각 작업(task)마다 고유한 $A$와 $B$ 행렬을 가진다.  
>   즉, “작업 A용 LoRA”와 “작업 B용 LoRA”는 서로 다른 파라미터 집합을 사용한다.  
> 
> - 그런데 만약 $A$와 $B$를 **기본 가중치 $W$에 미리 흡수(absorb)** 해버리면  
>   LoRA가 원래의 모델 구조에 완전히 통합되어 버린다.  
>   이렇게 되면 한 번의 순전파(forward pass)에서  
>   여러 작업이 서로 다른 $A$와 $B$를 동시에 사용하는 것이 불가능해진다.  
>   (즉, 한 모델이 한 가지 LoRA 설정만 사용할 수 있는 셈이다.)  
> 
> - 반면, **지연(latency)** 이 중요하지 않은 상황이라면  
>   $A$와 $B$를 미리 합치지 않고 그대로 분리해 둔 채로  
>   **배치 내 각 샘플(sample)** 이 서로 다른 LoRA 모듈을  
>   **동적으로 선택(dynamically select)** 하여 사용할 수 있다.  
>   예를 들어, 같은 배치에 “요약 작업용 LoRA”와 “번역 작업용 LoRA”가 섞여 있어도  
>   각 입력마다 대응되는 LoRA 모듈을 지정해 처리할 수 있다는 뜻이다.  
> 
> - 그러나 이 방식은 LoRA 모듈을 매번 선택하고 적용해야 하므로  
>   연산 효율이 떨어지고, 실제 서비스 환경(online inference)에서는  
>   속도 저하(latency increase)가 발생할 수 있다.

---

## 5. 실증 실험 (Empirical Experiments)  

우리는 **RoBERTa** (Liu et al., 2019), **DeBERTa** (He et al., 2021),  
그리고 **GPT-2** (Radford et al., b)에서 **LoRA**의 전이 학습(downstream) 성능을 평가하고,  
그 후 이를 **GPT-3 (175B)** (Brown et al., 2020)로 확장한다.  

본 실험은 **자연어 이해(NLU, Natural Language Understanding)** 부터  
**자연어 생성(NLG, Natural Language Generation)** 까지  
넓은 범위의 작업(tasks)을 포함한다.  

구체적으로,  
**RoBERTa**와 **DeBERTa**는 **GLUE 벤치마크(GLUE benchmark)** (Wang et al., 2019)를 기준으로 평가한다.  

**GPT-2**의 경우, **Li & Liang (2021)** 의 설정을 그대로 따라  
직접적인 비교(direct comparison)를 수행하며,  
대규모 실험(large-scale experiments)을 위해  
**GPT-3**에는 다음 두 작업을 추가한다:  
- **WikiSQL** (Zhong et al., 2017): 자연어 질의(NL)를 SQL 쿼리로 변환  
- **SAMSum** (Gliwa et al., 2019): 대화 요약(conversation summarization)  

사용된 데이터셋에 대한 더 자세한 내용은 **부록 C(Appendix C)** 를 참고한다.  

모든 실험은 **NVIDIA Tesla V100 GPU** 환경에서 수행하였다.
