---
layout: post
title: "[논문] LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS"
date: 2025-10-04 15:00:00 +0900
categories:
  - "논문"
tags: []
---

> **논문 출처**  
> Hu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W.  
> *LoRA: Low-Rank Adaptation of Large Language Models*.  
> arXiv preprint (arXiv:2106.09685), Version 2.  
> <a href="https://arxiv.org/abs/2106.09685" target="_blank">🔗 원문 링크 (arXiv:2106.09685)</a>

**저자**  
- Edward Hu (Microsoft Corporation) – edwardhu@microsoft.com  
- Yelong Shen (Microsoft Corporation) – yeshe@microsoft.com  
- Phillip Wallis (Microsoft Corporation) – phwallis@microsoft.com  
- Zeyuan Allen-Zhu (Microsoft Corporation) – zeyuana@microsoft.com  
- Yuanzhi Li (Carnegie Mellon University) – yuanzhil@andrew.cmu.edu  
- Shean Wang (Microsoft Corporation) – swang@microsoft.com  
- Lu Wang (Microsoft Corporation) – luw@microsoft.com  
- Weizhu Chen (Microsoft Corporation) – wzchen@microsoft.com  

---

**주석**  

∗ 공동 기여(Equal contribution).

---

(Version 2)

---

**주석**  

V1과 비교했을 때, 이번 초안에는 더 나은 **베이스라인(baselines)**,  
**GLUE 실험**, 그리고 **어댑터 지연(adapter latency)** 에 대한  
추가 내용이 포함되어 있다.

---

## 초록 (Abstract)  

자연어 처리(NLP)의 주요 패러다임 중 하나는  
**일반 도메인 데이터에 대한 대규모 사전학습(pre-training)** 과  
**특정 작업이나 도메인에 대한 적응(adaptation)** 으로 구성된다.  

하지만 모델이 커질수록,  
모든 파라미터를 다시 학습시키는 **전체 미세조정(full fine-tuning)** 은  
점점 더 비현실적이 된다.  

예를 들어, **GPT-3 (175B)** 모델의 경우,  
각각 1,750억 개의 파라미터를 가진 미세조정된 모델 인스턴스를  
독립적으로 배포하는 것은 **막대한 비용**이 든다.  

이를 해결하기 위해 우리는 **LoRA (Low-Rank Adaptation)** 를 제안한다.  

LoRA는 **사전학습된 모델의 가중치를 고정(freeze)** 한 채,  
**트랜스포머(Transformer) 아키텍처의 각 층(layer)** 에  
**학습 가능한 저랭크 분해 행렬(rank decomposition matrices)** 을 삽입함으로써,  
**전이 학습(transfer learning)** 시 필요한 **학습 가능한 파라미터 수를 크게 줄인다.**  

---

> **(블로그 추가 설명) 전이 학습(Transfer Learning)이란?**  
> - **전이 학습(Transfer Learning)** 은  
>   하나의 작업(task)이나 도메인(domain)에서 학습된 지식을  
>   **다른 작업으로 이전(transfer)** 하여 활용하는 학습 방법이다.  
> 
> - 일반적으로 **대규모 데이터로 사전학습(pre-training)** 된 모델은  
>   언어, 문맥, 구조 등에 대한 **일반적인 표현(representation)** 을 이미 학습하고 있다.  
>   이러한 모델을 새로운 응용 문제(예: 감정 분석, 질의응답, 번역 등)에 적용할 때는  
>   모델의 **일부 파라미터만 조정**하거나,  
>   **특정 층(layer)만 미세조정(fine-tuning)** 하는 방식으로 수행된다.  
> 
> - 따라서 처음부터 모델을 다시 학습시키지 않고,  
>   **이미 학습된 지식(knowledge)** 을 바탕으로  
>   **새로운 작업에 빠르게 적응(adapt)** 하는 것이  
>   전이 학습의 핵심 개념이다.  
> 
> - LoRA 역시 이러한 전이 학습의 한 형태로,  
>   전체 모델을 다시 학습하지 않고  
>   **필요한 부분만 효율적으로 조정**하여 성능을 유지한다.

---

GPT-3 (175B)를 Adam 옵티마이저로 미세조정했을 때와 비교하면,  
LoRA는 **학습 가능한 파라미터 수를 최대 10,000배까지 줄이고**,  
**GPU 메모리 요구량을 약 3배 절감**할 수 있다.  

그럼에도 불구하고, LoRA는  
RoBERTa, DeBERTa, GPT-2, GPT-3 등의 모델에서  
**미세조정(full fine-tuning)과 동등하거나 더 우수한 성능**을 보인다.  

또한, 학습 가능한 파라미터가 더 적고,  
**학습 처리량(training throughput)** 이 더 높으며,  
**어댑터(adapters)** 방식과 달리  
**추론(inference) 시 지연(latency)** 도 추가되지 않는다.  

---

> **(블로그 추가 설명) 어댑터(Adapter) 방식이란?**  
> - **어댑터(Adapter)** 는 대형 사전학습 모델(pre-trained model)의  
>   모든 파라미터를 다시 학습시키지 않고,  
>   **각 층(layer)** 사이에 **작은 학습 가능한 모듈(small trainable modules)** 을 추가하여  
>   새로운 작업에 맞게 모델을 미세조정(fine-tuning)하는 방법이다.  
> 
> - 이 방식은 원래의 모델 파라미터를 거의 그대로 유지한 채,  
>   **적은 추가 파라미터(parameter-efficient)** 만 학습할 수 있다는 장점이 있다.  
>   따라서 전체 모델을 다시 학습시키는 것보다 훨씬 효율적이며,  
>   서로 다른 작업 간에 **공유(reuse)** 도 용이하다.  
> 
> - 그러나 어댑터는 **추론(inference)** 단계에서  
>   각 층에 추가된 모듈을 통과해야 하므로,  
>   **추가적인 지연(latency)** 이 발생할 수 있다는 단점이 있다.  
>
> 어댑터 방식은 “모델 전체를 다시 학습하지 않고 필요한 부분만 추가 학습하는”  
> **파라미터 효율적 미세조정(Parameter-Efficient Fine-Tuning, PEFT)** 기법이다.  
> 하지만 **LoRA는 어댑터와 달리 추론 시 지연이 거의 없고**,  
> 동일한 효율성을 유지하면서 더 단순한 구조를 제공한다.

---

아울러 우리는  
**언어 모델 적응에서의 랭크 결핍(rank-deficiency)** 현상에 대한  
실증적 분석을 제공하여,  
**LoRA의 효율성에 대한 통찰(insight)** 을 제시한다.  

---

> **(블로그 추가 설명) 왜 ‘랭크 결핍(Rank-Deficiency)’이라고 부를까?**  
> - “랭크 결핍”은 원래 선형대수학에서 **행렬의 랭크(rank)** 가  
>   최대치보다 낮을 때, 즉 **선형 독립 성분이 부족할 때**를 의미하는 **부정적 용어**이다.  
>   예를 들어, $n \times n$ 행렬의 랭크가 $n$보다 작다면  
>   그 행렬은 **정보 손실(loss of information)** 이 있거나,  
>   **역행렬이 존재하지 않는(singular)** 상태로 본다.  
> 
> - 하지만 언어 모델의 관점에서는 이 “결핍”이 반드시 나쁜 것이 아니다.  
>   오히려 모델이 실제로는 **낮은 차원(low-rank)** 의 표현만으로도  
>   충분히 의미 있는 정보를 학습하고,  
>   복잡한 패턴을 표현할 수 있음을 보여주는 **구조적 효율성(structural efficiency)** 의 신호이기도 하다.  
> 
> - LoRA는 바로 이 특성을 적극적으로 이용한다.  
>   즉, 언어 모델이 본질적으로 **랭크 결핍 구조를 가진다**는 점을 활용하여,  
>   전체 파라미터를 모두 학습하지 않고도  
>   **필요한 저차원 방향만 학습함으로써 동일한 효과를 얻는다.**  
> 
> - 따라서 “랭크 결핍”이라는 단어는 수학적으로는 제약(constraint)을 뜻하지만,  
>   LoRA의 맥락에서는 **“낮은 차원 구조로도 충분히 잘 작동하는 현상”** 이라는  
>   **긍정적인 의미로 재해석**된다.

---

마지막으로, 우리는  
**PyTorch 모델에 LoRA를 통합할 수 있는 패키지**를 공개하였으며,  
RoBERTa, DeBERTa, GPT-2용 **구현 및 모델 체크포인트**를  
다음 링크에서 제공한다.  

<a href="https://github.com/microsoft/LoRA" target="_blank">🔗 https://github.com/microsoft/LoRA</a>


## 1. 서론 (Introduction)  

자연어 처리(NLP)의 많은 응용들은  
**하나의 대규모 사전학습 언어 모델(pre-trained language model)** 을  
여러 개의 **전이(또는 하위) 응용 프로그램(downstream applications)** 에  
적응시키는 것에 의존한다.  

이러한 적응(adaptation)은 일반적으로  
**모든 사전학습된 모델의 파라미터를 업데이트하는 미세조정(fine-tuning)** 을 통해 수행된다.  

그러나 미세조정의 주요 단점은,  
**새로운 모델이 원래 모델과 동일한 수의 파라미터를 갖게 된다는 점**이다.  

모델 규모가 몇 달마다 더 커지는 현 상황에서,  
이 문제는 단순히 GPT-2 (Radford et al.)나 RoBERTa large (Liu et al., 2019) 수준에서는  
“약간의 불편함”에 불과할 수 있었지만,  
**1,750억 개의 학습 가능한 파라미터를 가진 GPT-3 (Brown et al., 2020)** 의 경우에는  
**중대한 배포(deployment)상의 도전 과제**로 변하게 되었다.  

---

**주석**  

GPT-3 (175B)는 **퓨샷 학습(few-shot learning)** 만으로도  
일정 수준 이상의 성능을 달성하지만,  
**부록 A(Appendix A)** 에서 보여지듯이  
**미세조정(fine-tuning)** 을 통해 그 성능이 크게 향상된다.

---

많은 연구자들이 이 문제를 완화하기 위해  
**일부 파라미터만 적응(adapt)** 시키거나,  
**새로운 작업(task)** 을 위해 **외부 모듈(external modules)** 을 학습하는 방법을 시도해왔다.  

이러한 방식에서는 각 작업마다  
사전학습된 모델에 더해 **작업별 파라미터(task-specific parameters)** 만  
저장하고 불러오면 되므로,  
배포(deployment) 시 **운영 효율성(operational efficiency)** 이 크게 향상된다.  

그러나 기존 기법들은 종종  
**모델의 깊이(model depth)** 를 확장함으로써 **추론 지연(inference latency)** 을 유발하거나  
(Loulsby et al., 2019; Rebuffi et al., 2017),  
**모델이 처리할 수 있는 시퀀스 길이(sequence length)** 를 줄이는 문제를 발생시킨다  
(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021) (3장 참조).  

더 중요한 점은, 이러한 방법들이 종종  
**미세조정(fine-tuning) 기반의 베이스라인(baseline)** 수준의 성능을 달성하지 못해,  
**효율성과 모델 품질 간의 트레이드오프(trade-off)** 를 초래한다는 것이다.

우리는 Li et al. (2018a) 및 Aghajanyan et al. (2020)에서 제시된 결과로부터 영감을 얻었다.  
이들 연구는 **과도하게 매개변수화된(over-parameterized)** 모델이 실제로는  
**낮은 고유 차원(low intrinsic dimension)** 상에 존재함을 보여준다.  

이에 기반하여 우리는,  
모델 적응(model adaptation) 과정에서 발생하는 **가중치 변화(weight change)** 역시  
낮은 **내재적 랭크(intrinsic rank)** 를 가진다고 가정(hypothesis)한다.  
이 가정으로부터 우리가 제안한 **LoRA (Low-Rank Adaptation)** 방법이 도출된다.  

LoRA는 사전학습된 가중치를 그대로 **고정(freeze)** 시킨 채,  
**적응 과정에서의 밀집층(dense layer) 변화량**을  
**랭크 분해 행렬(rank decomposition matrices)** 형태로 학습하도록 설계된다.  
즉, 밀집층 자체를 직접 학습하는 대신,  
그 변화(change)를 저랭크 행렬 형태로 간접적으로 최적화한다는 것이다  
(그림 1 참고).  

---

그림 1: 우리의 **재매개변수화(reparametrization)** 방식.  
우리는 **A와 B만 학습한다.**

<img src="/assets/img/paper/lora/image_1.png" alt="image" width="360px"> 

---

> **(블로그 추가 설명) 재매개변수화(Reparametrization)와 랭크 분해 행렬 학습이란?**  
> - **재매개변수화(Reparametrization)** 는  
>   기존의 가중치 행렬 $W$ 를 직접 학습하지 않고,  
>   그것을 **다른 형태의 파라미터(예: $A$, $B$)** 로 다시 표현하여 학습하는 방법이다.  
>   즉, $W$ 대신 **$W + \Delta W$** 형태로 모델을 표현하고,  
>   이때 **$\Delta W = B A$** 로 두어 $A$, $B$ 두 행렬만 학습한다.  
> 
> - 이렇게 하면, 원래의 거대한 가중치 행렬 $W \in \mathbb{R}^{d \times d}$ 을  
>   직접 업데이트할 필요가 없다.  
>   대신, **저랭크(rank $r$)** 제약을 가진 두 행렬 $A \in \mathbb{R}^{r \times d}$,  
>   $B \in \mathbb{R}^{d \times r}$ 을 통해  
>   $W$ 의 변화를 근사(approximation)하게 된다.  
> 
> - 그림에서처럼, $A$ 는 초기값을 **정규분포 $\mathcal{N}(0, \sigma^2)$** 로 설정하고,  
>   $B$ 는 **0으로 초기화**된다.  
>   학습 과정에서는 $A$와 $B$만 최적화되고,  
>   원래의 사전학습 가중치 $W$ 는 **고정(freeze)** 되어 유지된다.  
> 
> - 이러한 방식은 모델의 전체 구조를 바꾸지 않으면서도  
>   **적은 수의 학습 파라미터로 동일한 효과를 얻을 수 있게 해주며**,  
>   이를 통해 **저장 공간과 계산량을 크게 절약**할 수 있다.  
> 
> - 요약하자면, LoRA의 재매개변수화는  
>   거대한 파라미터 행렬을 저차원(rank-$r$) 공간으로 분해해  
>   효율적으로 학습하는 **저랭크 근사(low-rank approximation)** 기반의 접근법이다.

---

GPT-3 (175B)를 예로 들면,  
전체 랭크($d$)가 12,288에 달하더라도  
**매우 낮은 랭크(예: $r=1$ 또는 $r=2$)** 만으로 충분함을 보여준다.  
이는 LoRA가 **저장(storage)** 및 **계산(compute)** 양 측면에서  
매우 효율적인 방법임을 의미한다.

LoRA는 여러 가지 핵심적인 이점을 가진다.  

- **사전학습된 모델(pre-trained model)** 은 여러 다른 작업(tasks)을 위한  
  작은 LoRA 모듈들을 구축하는 데 공유되고 사용될 수 있다.  
  우리는 공유된 모델을 고정(freeze)한 상태로 유지하고,  
  그림 1의 행렬 A와 B를 교체함으로써 작업을 효율적으로 전환할 수 있다.  
  이는 저장 공간 요구량(storage requirement)과  
  작업 전환 시 오버헤드(task-switching overhead)를 크게 줄여준다.  

- LoRA는 학습을 더욱 효율적으로 만들고,  
  적응형 옵티마이저(adaptive optimizer)를 사용할 때  
  하드웨어 진입 장벽(hardware barrier to entry)을 최대 3배까지 낮춘다.  
  이는 대부분의 파라미터에 대해 그래디언트를 계산하거나  
  옵티마이저 상태(optimizer state)를 유지할 필요가 없기 때문이다.  
  대신, 우리는 삽입된 **훨씬 작은 저랭크 행렬(low-rank matrices)** 만 최적화한다.  

- 우리의 단순한 선형(linear) 설계는,  
  배포(deployment) 시 학습 가능한 행렬(trainable matrices)을  
  고정된 가중치(frozen weights)와 병합(merge)할 수 있게 하며,  
  완전히 미세조정된 모델(fully fine-tuned model)과 비교했을 때  
  **추론 지연(inference latency)** 을 전혀 초래하지 않는다.  

- LoRA는 많은 기존 기법들과 **직교적(orthogonal)** 이며,  
  **프리픽스 튜닝(prefix-tuning)** 과 같은  
  여러 방법들과 결합될 수 있다.  
  이에 대한 예시는 **부록 E(Appendix E)** 에서 제시한다.

---

> **(블로그 추가 설명) ‘직교적(Orthogonal)’이라는 표현의 의미**  
> - 여기서 **직교적(orthogonal)** 이라는 표현은  
>   수학적 의미의 “서로 독립적(independent)”이라는 개념에서 유래한다.  
>   즉, **LoRA가 다른 방법들과 간섭하지 않고 동시에 사용할 수 있다** 는 뜻이다.  
> 
> - 어떤 기법이 직교적이라는 것은  
>   그것이 다른 학습 기법의 구조나 원리에 의존하지 않으며,  
>   **별도로 적용하거나 함께 결합(combine)** 해도  
>   서로의 작동 방식에 영향을 주지 않는다는 의미이다.  
> 
> - 따라서 LoRA는 **프리픽스 튜닝(prefix-tuning)**, **프로프트 튜닝(prompt tuning)**,  
>   **어댑터(adapter)** 등과 같은 다른 **파라미터 효율적 학습 기법(PEFT)** 들과  
>   함께 사용될 수 있으며, 이러한 결합은  
>   **추가적인 성능 향상이나 효율성 개선**을 가능하게 한다.

---

### **용어 및 표기 규칙 (Terminologies and Conventions)**  

우리는 **트랜스포머(Transformer) 아키텍처**를 자주 참조하며,  
그 구조에서 일반적으로 사용되는 용어와 표기 규칙(conventions)을 따른다.  

트랜스포머 층의 **입력 및 출력 차원 크기**를 $$d_{\text{model}}$$이라고 부른다.  

$$W_q, \; W_k, \; W_v, \; W_o$$는 각각 **셀프-어텐션(self-attention)** 모듈 내의  
**쿼리(query)**, **키(key)**, **밸류(value)**, **출력(output)** 투영 행렬을 의미한다.  

$$W \; \text{또는} \; W_0$$는 **사전학습된(pre-trained)** 가중치 행렬을,  
$$\Delta W$$는 **적응(adaptation)** 중 누적된 **그래디언트 업데이트(accumulated gradient update)** 를 나타낸다.  

$$r$$은 **LoRA 모듈의 랭크(rank)** 를 의미한다.  

우리는 **Vaswani et al. (2017)** 및 **Brown et al. (2020)** 의 표준 규칙을 따르며,  
모델 최적화(model optimization)에는 **Adam 옵티마이저**  
(**Loshchilov & Hutter, 2019; Kingma & Ba, 2017**)를 사용한다.  

또한, 트랜스포머의 **MLP 피드포워드(feedforward) 차원**은  
$$d_{\text{ffn}} = 4 \times d_{\text{model}}$$로 설정한다.

