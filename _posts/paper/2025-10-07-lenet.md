---
layout: post
title: "[논문] Gradient-Based Learning Applied to Document Recognition"
date: 2025-10-07 16:00:00 +0900
categories:
  - "논문"
tags: []
---

> 논문 출처  
> LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P.  
> *Gradient-Based Learning Applied to Document Recognition.*  
> *Proceedings of the IEEE*, 86(11), 2278–2324 (1998).  
> <a href="https://doi.org/10.1109/5.726791" target="_blank">🔗 원문 링크 (DOI: 10.1109/5.726791)</a>

저자  
- Yann LeCun (AT&T Labs–Research, Holmdel, NJ, USA) – yann@cs.nyu.edu  
- Léon Bottou (AT&T Labs–Research, Holmdel, NJ, USA) – leonb@research.att.com  
- Yoshua Bengio (Université de Montréal, Canada) – yoshua.bengio@umontreal.ca  
- Patrick Haffner (AT&T Labs–Research, Holmdel, NJ, USA) – haffner@research.att.com

---

>저자들은 모두 AT&T 연구소(AT&T Labs-Research) 내  
>음성 및 영상 처리 연구소(Speech and Image Processing Services Research Laboratory) 에 소속되어 있다.  
>주소는 다음과 같다.  
>
>AT&T Labs-Research  
>100 Schulz Drive, Red Bank, NJ 07701, USA  
>E-mail: {yann, leonb, yoshua, haffner}@research.att.com  
>
>또한 Yoshua Bengio 교수는  
>몬트리올 대학교(Université de Montréal)  
>정보학 및 운영연구학과(Département d’Informatique et de Recherche Opérationelle) 에도 소속되어 있다.  
>
>주소:  
>C.P. 6128 Succ. Centre-Ville,  
>2920 Chemin de la Tour,  
>Montréal, Québec, Canada H3C 3J7

---

## 초록 (Abstract)  

역전파 알고리즘으로 학습된 다층 신경망(Multilayer Neural Networks)은  
성공적인 그래디언트 기반 학습(Gradient-Based Learning) 기법의 가장 대표적인 예를 이룬다.  

적절한 네트워크 구조가 주어졌을 때,  
그래디언트 기반 학습 알고리즘은 복잡한 결정 경계(complex decision surface)를 구성할 수 있으며,  
이를 통해 손으로 쓴 문자(handwritten characters)와 같은  
고차원 패턴을 최소한의 전처리로 분류할 수 있다.  

이 논문은 필기 문자 인식(handwritten character recognition)에 적용된 여러 기존 방법들을 검토하고,  
이를 표준 필기 숫자 인식(handwritten digit recognition)과제에서 서로 비교한다.  

특히, 2차원 형태의 변동성(variability of 2D shapes)에  
대처하도록 특별히 설계된 합성곱 신경망(Convolutional Neural Networks)은  
다른 모든 기법을 능가하는 성능을 보인다.  

실제 문서 인식(document recognition) 시스템은  
필드 추출(field extraction), 분할(segmentation), 인식(recognition),  
언어 모델링(language modeling) 등 여러 모듈들로 구성되어 있다.  

본 논문에서는 그래프 변환기 네트워크(Graph Transformer Networks, GTN) 라는  
새로운 학습 패러다임을 제안한다.  

이 접근법은 여러 모듈로 구성된 시스템이  
그래디언트 기반 방법(Gradient-Based methods)을 통해  
전역적으로(global) 학습되도록 하여,  
전체 성능 척도(overall performance measure)를 최소화하도록 설계된다.  

이 논문에서는 온라인 필기 인식(on-line handwriting recognition)을 위한  
두 가지 시스템을 설명한다.  

실험 결과는 전역 학습(global training) 의 장점과  
그래프 변환기 네트워크(Graph Transformer Networks) 의 유연성을 보여준다.  

또한, 은행 수표(bank check) 판독용 그래프 변환기 네트워크 도 제안한다.  

이 시스템은 합성곱 신경망(CNN) 기반의 문자 인식기(character recognizer)와  
전역 학습 기법(global training techniques)을 결합하여  
업무용 및 개인용 수표에 대해 기록적인 인식 정확도를 달성한다.  

이 시스템은 상용화되어 있으며, 하루에 수백만 건의 수표를 처리한다.  

---

키워드(Keywords) —  
Neural Networks, OCR, Document Recognition,  
Machine Learning, Gradient-Based Learning,  
Convolutional Neural Networks, Graph Transformer Networks,  
Finite State Transducers.

## 용어 정리 (Nomenclature)  

- GT — 그래프 변환기 (Graph Transformer)  
- GTN — 그래프 변환기 네트워크 (Graph Transformer Network)  
- HMM — 은닉 마르코프 모델 (Hidden Markov Model)  
- HOS — 휴리스틱 과분할 (Heuristic Oversegmentation)  
- K-NN — K-최근접 이웃 (K-Nearest Neighbor)  
- NN — 신경망 (Neural Network)  
- OCR — 광학 문자 인식 (Optical Character Recognition)  
- PCA — 주성분 분석 (Principal Component Analysis)  
- RBF — 방사형 기저 함수 (Radial Basis Function)  
- RS-SVM — 축소 집합 서포트 벡터 방법 (Reduced-Set Support Vector Method)  
- SDNN — 공간 변위 신경망 (Space Displacement Neural Network)  
- SVM — 서포트 벡터 방법 (Support Vector Method)  
- TDNN — 시간 지연 신경망 (Time Delay Neural Network)  
- V-SVM — 가상 서포트 벡터 방법 (Virtual Support Vector Method)

## 1. 서론 (Introduction)  

지난 몇 년 동안, 특히 신경망(Neural Networks)에 적용된 머신러닝(Machine Learning) 기법은  
패턴 인식 시스템(pattern recognition systems) 설계에서 점점 더 중요한 역할을 담당하게 되었다.  

실제로, 학습 기법의 발전은  
최근 연속 음성 인식(continuous speech recognition) 및  
필기 인식(handwriting recognition)과 같은  
패턴 인식 응용 분야의 성공에 결정적인 요인으로 작용했다고 볼 수 있다.  

이 논문의 핵심 메시지는 다음과 같다.  

보다 우수한 패턴 인식 시스템은  
수작업으로 설계된 휴리스틱(heuristics)에 덜 의존하고,  
자동 학습(automatic learning)에 더 많이 의존함으로써 구축될 수 있다.  

이러한 접근이 가능해진 이유는 최근의 머신러닝 및 컴퓨터 기술의 발전 덕분이다.  

문자 인식(character recognition)을 사례로 하여,  
본 논문은 수작업으로 설계된 특징 추출(feature extraction)을 대신해,  
픽셀 이미지(pixel images)에서 직접 작동하는  
정교하게 설계된 학습 머신(learning machines)으로 대체할 수 있음을 보인다.  

또한, 문서 이해(document understanding)를 사례로 하여,  
전통적으로 개별적으로 설계된 모듈들을 수동으로 통합하던 인식 시스템 구축 방식을  
하나의 통합적이고 원리 기반의 설계 패러다임으로 대체할 수 있음을 보인다.  

그 패러다임이 바로 그래프 변환기 네트워크(Graph Transformer Networks)이며,  
이는 모든 모듈들을 전역적인(global) 성능 기준을 최적화하도록 동시에 학습시킬 수 있게 한다.

패턴 인식의 초창기 시절부터,  
자연 데이터(natural data)의 다양성과 풍부함—  
즉 음성(speech), 문자 기호(glyphs), 혹은 다른 형태의 패턴들—으로 인해  
정확한 인식 시스템을 완전히 수작업으로 구축하는 것이  
거의 불가능하다는 사실이 알려져 왔다.  

그 결과, 대부분의 패턴 인식 시스템은  
자동 학습 기법(automatic learning techniques)과  
수작업으로 제작된 알고리즘(hand-crafted algorithms)의 결합으로 구성된다.  

일반적인 개별 패턴 인식 방법은 시스템을 두 개의 주요 모듈로 나누는 것으로 이루어진다  
(그림 1 참조).  

---

**그림 1.**

전통적인 패턴 인식은 두 개의 모듈로 수행된다:  
고정된 특징 추출기(fixed feature extractor)와  
학습 가능한 분류기(trainable classifier).

<img src="/assets/img/paper/lenet/image_1.png" alt="image" width="450px"> 

---

첫 번째 모듈은 특징 추출기(feature extractor)로 불리며,  
입력 패턴을 변환하여 저차원 벡터나 짧은 기호 문자열로 표현할 수 있도록 한다.  

이때 다음 두 가지 성질을 만족해야 한다.  

(a) 쉽게 비교하거나 매칭될 수 있어야 하고,  
(b) 입력 패턴의 본질을 바꾸지 않는 변환이나 왜곡에 대해  
상대적으로 불변(invariant)이어야 한다.  

특징 추출기(feature extractor)는  
대부분의 사전 지식(prior knowledge)을 담고 있으며  
대개 특정 작업(task)에 특화되어 있다.  

또한 설계 노력의 대부분이 여기에 집중되며,  
이는 보통 전적으로 수작업으로 만들어지기 때문이다.  

반면 분류기(classifier)는  
일반 목적(general-purpose)이며 학습 가능(trainable)한 경우가 많다.  

이 접근법의 주요 문제 중 하나는  
인식 정확도(recognition accuracy)가  
설계자가 적절한 특징(feature) 집합을  
얼마나 잘 찾아내는가에 의해 크게 좌우된다는 점이다.  

이것은 매우 어려운 과제이며,  
불행히도 새로운 문제마다 다시 수행되어야 한다.  

패턴 인식 연구의 상당 부분은  
특정 작업(task)에 대해 서로 다른 특징 집합(feature sets)의  
상대적 장점(merits)을 기술하고 비교하는 데 할애되어 있다.

---

역사적으로, 적절한 특징 추출기가 필요했던 이유는  
분류기가 사용하는 학습 기법들이  
저차원의 공간에 한정되어 있었고  
그 클래스들이 쉽게 분리 가능했기 때문이었다 [1].  

세 가지 요소의 결합이 지난 10년 동안 이러한 관점을 변화시켜 왔다.  

첫째, 빠른 산술 연산 장치를 갖춘 저비용 기계의 이용 가능성은  
알고리즘적 세련화보다  
무차별 대입식 “수치적” 방법에 더 많이 의존할 수 있게 해주었다.  

둘째, 필기체 인식과 같이 큰 시장성과 폭넓은 관심을 지닌 문제들을 위한  
대규모 데이터베이스의 이용 가능성은  
설계자들이 수작업 기반의 특징 추출에 덜 의존하고  
실제 데이터에 더 많이 의존하여  
인식 시스템을 구축할 수 있게 해주었다.  

셋째이자 매우 중요한 요소는  
고차원 입력을 처리할 수 있고 이러한 대규모 데이터셋이 주어졌을 때  
복잡한 결정 함수를 생성할 수 있는 강력한 기계 학습 기법들의 이용 가능성이다.  

또한, 음성 및 필기체 인식 시스템의 정확도 향상은  
최근 들어 학습 기법과 대규모 훈련 데이터에 더 많이 의존하게 된 데에서  
상당 부분 비롯된 것이라고 볼 수 있다.  

이 사실의 증거로서, 많은 최신 상업용 OCR 시스템들은  
역전파로 학습된 다층 신경망(multi-layer Neural Network)의 어떤 형태를 사용한다.

---

이 연구에서 우리는 손글씨 문자 인식 과제(섹션 I과 II)를 다루고,  
손글씨 숫자 인식을 위한 벤치마크 데이터셋(섹션 III)에서  
여러 학습 기법들의 성능을 비교한다.  

보다 자동화된 학습이 유리하긴 하지만,  
어떤 학습 기법도 해당 과제에 대한 최소한의 사전 지식 없이 성공할 수는 없다.  

다층 신경망의 경우,  
지식을 반영하는 좋은 방법은 과제에 맞게 신경망의 구조를 조정하는 것이다.  

섹션 II에서 소개된 합성곱 신경망(CNN) [2]은  
2D 형태의 불변성(invariances)을 활용하기 위해  
국소적 연결 패턴을 사용하고  
가중치에 제약을 가하는 방식으로  
지식을 구조 안에 통합한 특수한 신경망 구조의 예이다.  

섹션 III에서는 독립된 손글씨 숫자 인식을 위한 여러 방법들의 비교가 제시된다.  

개별 문자 인식에서 문서 내 단어나 문장 인식으로 확장하기 위해,  
전체 오류를 줄이도록 학습된 여러 모듈을 결합하는 개념이  
섹션 IV에서 소개된다.  

손글씨 단어처럼 길이가 가변적인 객체를 인식하는 데에는  
여러 모듈 기반의 시스템이 효과적이며,  
특히 그 모듈들이 방향 그래프를 조작할 수 있을 때 가장 좋다.  

이것은 학습 가능한  
Graph Transformer Network(GTN) 개념으로 이어지며,  
이 역시 섹션 IV에서 소개된다.  

섹션 V는 단어 또는 다른 문자 스트링을 인식하기 위해  
지금은 고전적인 방법이 된 휴리스틱 기반 과분절(over-segmentation) 기법을 설명한다.  

> **휴리스틱 기반 과분절(over-segmentation) 기법이란?**  
>  
> - 글자를 정확히 어디에서 끊어야 하는지 알 수 없을 때,  
>   가능한 모든 분절 후보 지점을 **과하게 많이** 생성해 두는 방식이다.  
> - 이렇게 만들어진 여러 분절 후보들 중에서  
>   이후 단계의 모델(예: HMM, 신경망 등)이  
>   가장 일관된 경로를 선택하여 실제 문자를 결정한다.  
> - 즉, “분할을 미리 최소화하지 않고,  
>   가능한 분할을 넓게 열어 둔 뒤  
>   후처리 단계에서 최적 경로를 찾는” 접근이다.

섹션 VI에서는 수동 분절 및 레이블링 없이  
단어 단위에서 인식기(recognizer)를 학습시키는  
식별적(discriminative)·비식별적(non-discriminative) 경사 기반 기법들이 제시된다.  

섹션 VII은 입력의 모든 가능한 위치를 스캔함으로써  
분절 휴리스틱의 필요성을 제거하는 유망한 공간-변위 신경망  
(Space-Displacement Neural Network) 접근법을 다룬다.  

섹션 VIII에서는 학습 가능한 GTN이 일반적인 그래프 구성 알고리즘에 기반한  
다중 일반화 변환(multiple generalized transductions)으로  
정식화될 수 있음을 보인다.  

또한, 음성 인식에서 흔히 사용되는  
은닉 마르코프 모델(HMM)과 GTN 간의 연결성도 다루어진다.  

섹션 IX는 펜 컴퓨터에서 입력되는 손글씨를 인식하기 위해  
글로벌하게 학습된 GTN 시스템을 설명한다.  

이 문제는 사용자가 글을 쓰는 즉시 머신이 피드백을 제공해야 하므로  
‘온라인(online) 손글씨 인식’으로 알려져 있다.  

이 시스템의 핵심은 합성곱 신경망(Convolutional Neural Network)이다.  

결과는 단어 단위에서 인식기를 학습시키는 것이  
사전에 분절된 개별 문자에 대해 학습시키는 것보다  
명확한 장점을 가진다는 것을 보여 준다.  

섹션 X는 손글씨와 기계 인쇄된 은행 수표를 읽기 위한  
완전한 GTN 기반 시스템을 설명한다.  

이 시스템의 핵심은 섹션 II에서 설명된 LeNet-5라 불리는 합성곱 신경망이다.  

이 시스템은 NCR Corporation의  
은행권 체크 인식 시스템에서 상업적으로 사용되고 있으며,

미국 전역 여러 은행에서 매달 수백만 건의 수표를 읽고 있다.

---

### A. Learning from Data

자동 기계 학습에는 여러 접근법이 존재하지만,  
최근 몇 년 동안 신경망 커뮤니티에 의해 대중화된 가장 성공적인 접근법 중 하나는  
“수치적(numerical)” 또는 경사 기반 학습(gradient-based learning)이라 불릴 수 있다.  

학습 기계는 입력 패턴 $Z^p$ 와 시스템 안의 조절 가능한 파라미터 $W$에 대해  
$Y^p = F(Z^p, W)$ 라는 함수를 계산한다.  

패턴 인식 환경에서, 출력 $Y^p$ 는 패턴 $Z^p$ 의 인식된 클래스 레이블로,  
혹은 각 클래스에 연관된 점수 또는 확률로 해석될 수 있다.  

손실 함수 $E^p = D(D^p, F(W, Z^p))$는 패턴 $Z^p$에 대해  
“정답” 또는 원하는 출력 $D^p$와 시스템이 생성한 출력 사이의 불일치를 측정한다.  

평균 손실 함수 $E_{\text{train}}(W)$는 라벨이 붙은 예제들의 집합인  
훈련 세트 $\{(Z^1, D^1), \ldots, (Z^P, D^P)\}$에 걸친 오차 $E^p$ 들의 평균이다.  

가장 단순한 설정에서, 학습 문제는  
$E_{\text{train}}(W)$를 최소화하는 $W$ 의 값을 찾는 것이다.  

실제로는, 훈련 세트에서의 시스템 성능은 그다지 관심 대상이 아니다.  

더 관련 있는 측정값은 실제 환경에서 사용될 때의 시스템 오류율이며,  
이는 훈련 세트와 분리된 샘플 집합인 테스트 세트에서의 정확도를 측정함으로써 추정된다.  

많은 이론적 및 실험적 연구 [3], [4], [5] 는  
테스트 세트에서의 기대 오류율 $E_{\text{test}}$ 과  
훈련 세트에서의 오류율 $E_{\text{train}}$ 사이의 간격이  
훈련 샘플 수가 증가함에 따라 대략 다음과 같이 감소한다는 것을 보여주었다:

$$
E_{\text{test}} - E_{\text{train}}
= k \left( \frac{h}{P} \right)^{\alpha}
\tag{1}
$$

여기서 $P$ 는 훈련 샘플의 수이고,  
$h$ 는 머신의 “유효 용량” 또는 복잡도의 척도이며 [6], [7],  
$\alpha$ 는 0.5 와 1.0 사이의 수이고, $k$ 는 상수이다.  

이 간격은 훈련 샘플 수가 증가할 때 항상 감소한다.  

더 나아가, 용량 $h$가 증가함에 따라 $E_{\text{train}}$은 감소한다.  

따라서, 용량 $h$ 를 증가시킬 때에는  
$E_{\text{train}}$의 감소와 간격의 증가 사이에 상충 관계(trade-off)가 존재하며,  
최소의 일반화 오류 $E_{\text{test}}$를 달성하는 최적의 용량 값 $h$가 존재한다.  

대부분의 학습 알고리즘들은 $E_{\text{train}}$뿐 아니라  
그 간격에 관한 추정치도 함께 최소화하려고 한다.  

이것의 형식적 버전은 구조적 위험 최소화(structural risk minimization)[6], [7] 라 불리며,  
증가하는 용량을 갖는 학습 기계들의 일련의 순서를 정의하는 데 기반한다.  

이것은 각 부분집합이 이전 부분집합의 상위 집합이 되도록 구성된  
파라미터 공간의 부분집합들의 일련의 순서에 대응한다.  

실용적인 측면에서, 구조적 위험 최소화는 $E_{\text{train}} + \beta H(W)$를 최소화함으로써 구현되는데,  
여기서 함수 $H(W)$는 정규화 함수라 불리고, $\beta$는 상수이다.  

$H(W)$는 파라미터 공간의 고용량 부분집합에 속하는 $W$ 값들에 대해 큰 값을 갖도록 선택된다.  

$H(W)$ 를 최소화하는 것은 실제로 접근 가능한 파라미터 공간의 용량을 제한하는 효과를 가지며,  
그 결과 훈련 오류를 최소화하는 일과 훈련·테스트 오류 간의 기대 간격을  
최소화하는 일 사이의 상충 관계를 제어하게 된다.
