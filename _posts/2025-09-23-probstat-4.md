---
layout: post
title: "[4주차] 확률과 통계"
date: 2025-09-23 16:40:00 +0900
categories: ["확률과 통계"]
tags: []
---

## p2. 피셔 정보  

**정의: 피셔 정보(Fisher information)**  

한 관측치에 대한 로그우도를 다음과 같이 정의한다:  
$$
\ell(\theta) = \log L_{1}(X, \theta), \quad \theta \in \Theta \subset \mathbb{R}^{d}
$$  

$\ell$이 거의 모든 경우(a.s.)에서 두 번 미분 가능하다고 가정하자.  
일정한 정규성 조건하에서 (Under some regularity conditions), 통계모델의 피셔 정보는 다음과 같이 정의된다:  
$$
I(\theta) 
$$
$$
= \mathbb{E}\!\big[\nabla \ell(\theta)\nabla \ell(\theta)^{\top}\big] 
- \mathbb{E}[\nabla \ell(\theta)]\mathbb{E}[\nabla \ell(\theta)]^{\top} 
$$
$$
= -\mathbb{E}\!\big[\nabla^{2}\ell(\theta)\big].
$$  

만약 $\Theta \subset \mathbb{R}$이면,  
$$
I(\theta) = \mathrm{var}\!\big[\ell'(\theta)\big] = -\mathbb{E}[\ell''(\theta)].
$$  

---

>**보충설명**
>
>1. **왜 a.s.라는 표현이 붙는가?**  
>   로그우도 함수 $\ell(\theta)$가 모든 경우에 항상 두 번 미분 가능하다고 단정할 수는 없다.  
>   예외적으로 미분이 불가능한 표본이 있을 수 있지만, 그런 경우는 확률적으로 0에 해당한다.  
>   그래서 “거의 확실히(almost surely, a.s.) 두 번 미분 가능하다”라는 표현을 써서,  
>   확률 1의 집합 위에서는 문제없이 미분할 수 있음을 강조한다.  
>
>2. **일정한 정규성 조건의 의미**  
>   수식 전개 과정에서는 미분과 기댓값을 교환하는 과정이 필요하다.  
>   예를 들어  
>   $$
>   \mathbb{E}\!\left[\frac{\partial}{\partial \theta}\ell(\theta)\right]
   = \frac{\partial}{\partial \theta}\mathbb{E}[\ell(\theta)]
>   $$
>   와 같은 식이 자연스럽게 성립해야 한다.  
>   이를 보장하는 가정을 “정규성 조건”이라고 부른다.  
>   즉, 수학적으로 까다로운 예외 상황을 배제하고, 우리가 원하는 성질이 성립하도록 하는 기술적 조건이다.  
>
>---
>
>3. **수식 전개 과정**  
>
>- **스칼라에서의 분산**  
>  스칼라 확률변수 $X$의 분산 정의는  
>  $$
>  \mathrm{Var}(X) = \mathbb{E}\big[(X - \mathbb{E}[X])^{2}\big].
>  $$  
>  즉, 편차의 제곱의 평균을 의미한다.  
>
>- **벡터에서의 분산 → 공분산 행렬**  
>  벡터 $U = (U_{1}, U_{2}, \dots, U_{d})^{\top}$에 대해서는 각 성분쌍의 공분산을 모두 담아야 하므로  
>  분산을 공분산 **행렬**로 정의한다:  
>  $$
>  \mathrm{Var}(U) = \mathbb{E}\!\left[(U - \mathbb{E}[U])\,(U - \mathbb{E}[U])^{\top}\right].
>  $$  
>
>- **왜 외적(outer product)인가?**  
>  $(U-\mathbb{E}[U])$는 $d\times 1$ 열벡터, $(U-\mathbb{E}[U])^{\top}$는 $1\times d$ 행벡터이므로  
  곱은 $d\times d$ 행렬이 된다. 그 행렬의 $(i,j)$ 성분은  
>  $$
>  \big(\mathrm{Var}(U)\big)_{ij}
  = \mathbb{E}\!\big[(U_{i}-\mathbb{E}[U_{i}])\,(U_{j}-\mathbb{E}[U_{j}])\big],
>  $$  
>  즉 $U_i$와 $U_j$의 공분산이 된다.  
>
>- **점수 함수와 피셔 정보**  
>  점수 함수(score function)는  
>  $$
>  U(\theta)=\nabla \ell(\theta)=\frac{\partial}{\partial \theta}\log L_{1}(X,\theta)
>  $$  
>  로 두고, 피셔 정보는 점수 함수의 분산으로 정의한다:  
>  $$
>  I(\theta)=\mathrm{Var}\big[U(\theta)\big].
>  $$  
>
>- **분산 전개 (행렬/벡터 연산을 모두 풀어 씀)**  
>  편의상 $m:=\mathbb{E}[U(\theta)]$로 둔다. 그러면
>  $$
>  \mathrm{Var}\big[U(\theta)\big]
  = \mathbb{E}\!\left[(U(\theta)-m)\,(U(\theta)-m)^{\top}\right].
>  $$  
>
>  1) **곱(외적) 전개**  
>  $$
>  (U-m)(U-m)^{\top}
  \;=\; U U^{\top} \;-\; U m^{\top} \;-\; m U^{\top} \;+\; m m^{\top}.
>  $$  
>
>  2) **기대값의 선형성 적용**  
>  $$
>  \begin{aligned}
  \mathrm{Var}[U]
  &= \mathbb{E}[U U^{\top}]
   \;-\; \mathbb{E}[U m^{\top}]
   \;-\; \mathbb{E}[m U^{\top}]
   \;+\; \mathbb{E}[m m^{\top}] \\
  &= \mathbb{E}[U U^{\top}]
   \;-\; \mathbb{E}[U]\, m^{\top}
   \;-\; m\, \mathbb{E}[U]^{\top}
   \;+\; m m^{\top}.
  \end{aligned}
 > $$  
 > 여기서 $m=\mathbb{E}[U]$는 상수 벡터이므로 기대값 바깥으로 뺄 수 있고,  
>  $\mathbb{E}[U]=m$, $\mathbb{E}[U]^{\top}=m^{\top}$를 대입하면  
>  $$
>  \mathrm{Var}[U]
  = \mathbb{E}[U U^{\top}] \;-\; m m^{\top} \;-\; m m^{\top} \;+\; m m^{\top}
>  $$
>  $$
>  = \mathbb{E}[U U^{\top}] \;-\; m m^{\top}.
>  $$
>
>  따라서
>  $$
>  \boxed{\,\mathrm{Var}[U(\theta)]
  = \mathbb{E}\!\big[U(\theta)U(\theta)^{\top}\big]
  - \mathbb{E}[U(\theta)]\,\mathbb{E}[U(\theta)]^{\top}\, }.
>  $$
>
>- **정규성 조건 적용**  
>  정규성 조건하에서는 $\mathbb{E}[U(\theta)]=0$임을 보일 수 있다.  
>
>  실제로,
>  $$
>  U(\theta) = \frac{\partial}{\partial \theta} \log p_\theta(X)
  = \frac{1}{p_\theta(X)} \frac{\partial}{\partial \theta} p_\theta(X),
>  $$
>  이므로
>  $$
>  \mathbb{E}[U(\theta)]
  = \int U(\theta) p_\theta(x)\,dx
>  $$
>  $$
>  = \int \frac{1}{p_\theta(x)} \frac{\partial}{\partial \theta} p_\theta(x)\, p_\theta(x)\,dx
  = \int \frac{\partial}{\partial \theta} p_\theta(x)\, dx.
>  $$  
>  정규성 조건(미분과 적분 교환 가능) 하에서는  
>  $$
>  \mathbb{E}[U(\theta)]
  = \frac{\partial}{\partial \theta} \int p_\theta(x)\, dx
  = \frac{\partial}{\partial \theta} (1) = 0.
>  $$  
>
>  따라서  
>  $$
  I(\theta)=\mathrm{Var}[U(\theta)]
  = \mathbb{E}\!\big[U(\theta)U(\theta)^{\top}\big].
>  $$  
>
>--- 
>
>4. **피셔 정보의 두 표현이 같아지는 이유 (수식 전개 과정 (2))**  
>
>- **로그우도 함수의 정의**  
>  확률밀도(또는 질량) 함수 $p_\theta(x)$에 대해  
>  $$
>  \ell(\theta) = \log p_\theta(X).
>  $$  
>
>- **점수 함수(score function)**  
>  점수 함수는 로그우도의 기울기이므로  
>  $$
>  U(\theta) = \nabla_\theta \ell(\theta) 
  = \nabla_\theta \log p_\theta(X).
>  $$  
>  로그 미분 성질에 의해  
>  $$
>  U(\theta) = \frac{\nabla_\theta p_\theta(X)}{p_\theta(X)}.
>  $$  
>
>- **피셔 정보의 정의**  
>  피셔 정보는 점수 함수의 분산으로 정의된다. 정규성 조건 때문에 $\mathbb{E}[U(\theta)]=0$이므로  
>  $$
>  I(\theta) = \mathbb{E}[U(\theta)U(\theta)^{\top}].
>  $$  
>
>- **로그우도의 2차 도함수 전개 (단계별)**  
>  $\ell(\theta) = \log p_\theta(X)$를 두 번 미분하면 다음과 같이 된다.  
>
>  1) **1차 미분**  
>  $$
>  \frac{\partial}{\partial \theta_i}\ell(\theta)
  = \frac{1}{p_\theta(X)} \frac{\partial}{\partial \theta_i} p_\theta(X).
>  $$  
>
>  2) **2차 미분 준비**  
>  $$
>  \frac{\partial^2}{\partial \theta_i \partial \theta_j}\ell(\theta)
  = \frac{\partial}{\partial \theta_j}\left(\frac{1}{p_\theta(X)} \frac{\partial}{\partial \theta_i} p_\theta(X)\right).
>  $$  
>
>  3) **곱의 미분 적용**  
>  $$
>  = \frac{\partial}{\partial \theta_j}\!\left(\frac{1}{p_\theta(X)}\right)\cdot \frac{\partial}{\partial \theta_i}p_\theta(X)
  + \frac{1}{p_\theta(X)} \cdot \frac{\partial^2}{\partial \theta_i \partial \theta_j}p_\theta(X).
>  $$  
>
>  4) **각 항 계산**  
>  $$
>  \frac{\partial}{\partial \theta_j}\!\left(\frac{1}{p_\theta(X)}\right)
  = -\frac{1}{p_\theta(X)^2}\frac{\partial}{\partial \theta_j}p_\theta(X).
>  $$  
>
>  따라서  
>  $$
>  \frac{\partial^2}{\partial \theta_i \partial \theta_j}\ell(\theta)
>  $$
>  $$
>  = \frac{1}{p_\theta(X)} \frac{\partial^2}{\partial \theta_i \partial \theta_j}p_\theta(X)
  - \frac{1}{p_\theta(X)^2}\frac{\partial}{\partial \theta_i}p_\theta(X)\frac{\partial}{\partial \theta_j}p_\theta(X).
>  $$  
>
>- **기댓값 취하기**  
>  양변에 $-1$을 곱하고 기댓값을 취하면  
>  $$
>  -\,\mathbb{E}\!\left[\frac{\partial^2}{\partial \theta_i \partial \theta_j}\ell(\theta)\right]
>  $$
>  $$
>  = -\int \left[\frac{1}{p_\theta(x)} \frac{\partial^2}{\partial \theta_i \partial \theta_j}p_\theta(x)
  - \frac{1}{p_\theta(x)^2}\frac{\partial}{\partial \theta_i}p_\theta(x)\frac{\partial}{\partial \theta_j}p_\theta(x)\right] p_\theta(x)\, dx.
>  $$  
>
>  전개하면  
>  $$
>  = -\int \frac{\partial^2}{\partial \theta_i \partial \theta_j}p_\theta(x)\, dx
  + \int \frac{1}{p_\theta(x)}\frac{\partial}{\partial \theta_i}p_\theta(x)\frac{\partial}{\partial \theta_j}p_\theta(x)\, dx.
>  $$  
>
>- **첫 번째 항 소거**  
>  $$
>  \int \frac{\partial^2}{\partial \theta_i \partial \theta_j}p_\theta(x)\, dx
  = \frac{\partial^2}{\partial \theta_i \partial \theta_j}\int p_\theta(x)\,dx
  = \frac{\partial^2}{\partial \theta_i \partial \theta_j}(1)=0.
>  $$  
>
>  따라서 남는 것은  
>  $$
>  -\,\mathbb{E}\!\left[\frac{\partial^2}{\partial \theta_i \partial \theta_j}\ell(\theta)\right]
  = \int \frac{1}{p_\theta(x)}\frac{\partial}{\partial \theta_i}p_\theta(x)\frac{\partial}{\partial \theta_j}p_\theta(x)\, dx.
>  $$  
>
>- **점수 함수와 연결**  
>  점수 함수의 성분은  
>  $$
>  U_i(\theta) = \frac{1}{p_\theta(X)} \frac{\partial}{\partial \theta_i}p_\theta(X).
>  $$  
>  따라서  
>  $$
>  U_i(\theta)U_j(\theta) 
  = \frac{1}{p_\theta(X)^2}\frac{\partial}{\partial \theta_i}p_\theta(X)\frac{\partial}{\partial \theta_j}p_\theta(X).
>  $$  
>
>  기댓값을 취할 때는 분포 $p_\theta(x)$에 대해 적분하므로 분모의 $p_\theta(x)^2$ 중 하나가 분자의 $p_\theta(x)$와 소거된다:  
>  $$
>  \mathbb{E}[U_i(\theta)U_j(\theta)] 
  = \int \frac{1}{p_\theta(x)^2}\frac{\partial}{\partial \theta_i}p_\theta(x)\frac{\partial}{\partial \theta_j}p_\theta(x)\, p_\theta(x)\, dx,
>  $$  
>  $$
>  = \int \frac{1}{p_\theta(x)}\frac{\partial}{\partial \theta_i}p_\theta(x)\frac{\partial}{\partial \theta_j}p_\theta(x)\, dx.
>  $$  
>
>- **결론**  
>  따라서  
>  $$
>  \mathbb{E}[U_i(\theta)U_j(\theta)]
  = -\,\mathbb{E}\!\left[\frac{\partial^2}{\partial \theta_i \partial \theta_j}\ell(\theta)\right].
>  $$  
>
>  행렬 형태로 쓰면  
>  $$
>  \boxed{\,I(\theta) = \mathbb{E}[U(\theta)U(\theta)^{\top}] 
  = -\,\mathbb{E}[\nabla^{2}\ell(\theta)]\, }.
>  $$
>
>---
>
>5. **스칼라 모수의 경우**  
>
>- 만약 모수 공간이 $\Theta \subset \mathbb{R}$인 경우, 즉 $\theta$가 스칼라(한 개의 값)라면 피셔 정보는 다음과 같이 단순화된다.  
>
>- 점수 함수는  
>  $$
>  U(\theta) = \ell'(\theta),
>  $$  
>  이고, 따라서 피셔 정보는 점수 함수의 분산이다:  
>  $$
>  I(\theta) = \mathrm{Var}[\ell'(\theta)].
>  $$  
>
>- 한편, 로그우도의 2차 도함수를 이용하면  
>  $$
>  I(\theta) = -\,\mathbb{E}[\ell''(\theta)].
>  $$  
>
>- 즉, 스칼라 모수의 경우에는 벡터/행렬 형태가 아니라 단일 값으로 표현되며, 점수 함수의 분산과 로그우도의 음의 기댓값이 서로 같은 피셔 정보의 두 가지 표현을 이룬다.

---

## p3. 최대우도추정량(MLE)의 성질  

**정리 (Theorem)**  

$\theta^{\ast} \in \Theta$ (참 모수, true parameter)라 하자. 다음을 가정한다:  

1. 모델이 식별 가능하다 (The model is identified).  
2. 모든 $\theta \in \Theta$에 대해, 분포 $P_\theta$의 지지집합(support)은 $\theta$에 의존하지 않는다.  
3. $\theta^{\ast}$는 $\Theta$의 경계(boundary)에 있지 않다.  
4. $I(\theta)$는 $\theta^{\ast}$ 근방에서 가역적(invertible)이다.  
5. 그 외 몇 가지 기술적인 조건들이 성립한다.  

---

**따라서 $\hat{\theta}_n^{MLE}$는 다음을 만족한다:**  

- 일치성(Consistency):  
  $$
  \hat{\theta}_n^{MLE} \;\;\xrightarrow{P}\;\; \theta^{\ast}, 
  \quad n \to \infty, 
  \quad \text{w.r.t. } \mathbb{P}_{\theta^{\ast}}.
  $$  

- 점근정규성(Asymptotic Normality):  
  $$
  \sqrt{n}\,(\hat{\theta}_n^{MLE} - \theta^{\ast})
  \;\;\xrightarrow{d}\;\;
  \mathcal{N}\!\big(0,\, I(\theta^{\ast})^{-1}\big),
  \quad n \to \infty,
  \quad \text{w.r.t. } \mathbb{P}_{\theta^{\ast}}.
  $$

---

>**보충설명**
>
>1. **모델이 식별 가능하다는 의미**  
>   서로 다른 모수 $\theta_1 \neq \theta_2$가 항상 서로 다른 분포 $P_{\theta_1} \neq P_{\theta_2}$를 유도한다는 뜻이다.  
>   즉, 데이터를 통해 모수를 유일하게 식별할 수 있어야 한다.  
>   만약 서로 다른 $\theta$가 같은 분포를 만들면, 추정량이 어떤 값을 수렴해야 하는지 정의되지 않는다.  
>
>2. **지지집합(support)과 $\theta$에 의존하지 않는다는 의미**  
>   확률분포 $P_\theta$의 지지집합이란, 확률밀도(또는 질량) $p_\theta(x) > 0$인 $x$의 집합이다.  
>   “$\theta$에 의존하지 않는다”는 것은, 모수 값이 달라져도 확률변수가 가질 수 있는 값의 범위는 변하지 않는다는 뜻이다.  
>   예를 들어 정규분포 $N(\mu,\sigma^2)$의 경우 지지집합은 항상 $\mathbb{R}$이므로 $\mu, \sigma$에 의존하지 않는다.  
>
>3. **$\theta^{\ast}$가 경계(boundary)에 있지 않다는 의미**  
>   참 모수 $\theta^{\ast}$는 모수공간 $\Theta$의 내부에 존재해야 한다는 조건이다.  
>   만약 경계에 있으면, 예를 들어 분산이 0에 가까운 경우처럼, 미분 가능성이 깨지거나 점근정규성이 무너지기 쉽다.  
>
>4. **$I(\theta)$가 가역적(invertible)이라는 의미**  
피셔 정보행렬 $I(\theta)$가 **양의 정부호(positive definite)**라는 것은, 행렬이 항상 양수 방향으로만 값을 가지는 안정적인 성질을 갖는다는 뜻이다.
>이 성질 덕분에 역행렬을 구할 수 있다.
>  
>  만약 $I(\theta)$의 역행렬이 존재하지 않는다면, 추정량의 분산을 계산할 수 없게 된다.
>그 결과 MLE의 점근적 분포가 정상적인 정규분포 형태를 띠지 못하고 **특이(singular)**한 모양이 되어,
>신뢰구간을 구하거나 가설검정을 하는 등의 통계적 해석이 불가능해진다.
>
>5. **그 외 기술적인 조건들**  
>   - 로그우도의 적분과 미분을 교환할 수 있도록 보장하는 정규성 조건  
>   - 모수 공간의 적당한 연속성, 매끄러움(smoothness)  
>   - 확률적 수렴에 필요한 제한 조건(예: 균등적분 가능성)  
>   이들은 주로 미분과 기댓값의 교환, 중심극한정리 적용 등을 정당화하기 위해 필요하다.  
>
>6. **결론 (첫 번째 항목: 일치성)**  
>   $$
>   \hat{\theta}_n^{MLE} \;\xrightarrow{P}\; \theta^{\ast}
>   $$  
>   이 조건은 표본의 크기가 무한히 커지면, 최대우도추정량이 참 모수에 확률적으로 수렴한다는 뜻이다.  
>   즉, MLE는 “일치추정량(consistent estimator)”이다.  
>
>7. **결론 (두 번째 항목: 점근정규성)**  
>   $$
>   \sqrt{n}\,(\hat{\theta}_n^{MLE} - \theta^{\ast})
   \;\xrightarrow{d}\; \mathcal{N}\!\big(0, I(\theta^{\ast})^{-1}\big)
>   $$  
>   이는 MLE가 대수적으로는 참 모수에 가까워질 뿐 아니라, 그 주변에서 정규분포를 따른다는 사실이다.  
>   따라서 $n$이 클수록 추정량의 분포가 평균 $\theta^{\ast}$, 공분산 $I(\theta^{\ast})^{-1}$인 정규분포로 근사된다.  
>   이 성질을 활용해 신뢰구간을 만들고, 가설검정을 수행할 수 있다.  